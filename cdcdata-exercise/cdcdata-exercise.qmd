---
title: "Data Exercise"
author: "Kevin Kosewick"
editor: visual
---

For this exercise, I'll be using a dataset I got from the [CDC website](https://data.cdc.gov/Health-Consequences-and-Costs/Smoking-Attributable-Mortality-Morbidity-and-Econo/ezab-8sq5/about_data) about smoking attributable expenses in the U.S. The data includes expenses per state for different categories such as hospital bills, ambulances, and prescriptions as well as overall expenses for the U.S. from 2005-2009. We'll do an exploratory data analysis and some data processing for this exercise.

```{r}
#load the dataset
library(here)
smoke_expense<- read.csv(here("cdcdata-exercise", "SAE.csv"))
#make sure data fully loaded by checking number of rows and columns. We should have 19 variables and 1560 observations
nrow(smoke_expense)
ncol(smoke_expense)
#check the structure and summary of the data. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #
summary(smoke_expense)
#structure(smoke_expense)
```

We can see that there's one observation for each of our 6 expense types per year. We can also see from the structure and summary that there are many columns that we don't need for an analysis. We can go ahead and pick out the ones that are of interest to us: year, location, variable (which refers to the type of expense), and value (which is the cost in millions of dollars for each expense type). All of the other columns seem to be for record keeping purposes.

```{r}
#make a new data frame containing only our four columns of interest: Year, LocationAbbr, Variable, and Data_Value.
smoke_expense_2 <- smoke_expense[, c("Year", "LocationAbbr", "Variable", "Data_Value")]
#check the new object to make sure it has everything we want. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #
#structure(smoke_expense_2)
summary(smoke_expense_2)
```

This new object is much more condensed and easier to work with. We'll check now to make sure there are no NA values and then proceed with some EDA.

```{r}
#check for NA values
na_check<- is.na(smoke_expense_2)
print(sum(na_check))
```

Since there aren't any NA values and looking at the structure indicates no missing values, we can begin to check the mean and standard deviation of each expense type across the 4 years in the data set. Rather than looking at all 50 states, let's focus on 5 to make this a bit easier. We'll use GA, TN, MS, CA, and FL.

Using Microsoft Copilot with GPT-4 in "Precise Mode", I entered the following prompt to get the code I'm about to use: "For my exploratory analysis I want to summarize each variable in a way that can be described by a distribution. For instance, I want to be able to determine the mean and standard deviation of each expense type for 5 different states over the 4 year period recorded in the dataset. What is the best approach for this and could you provide some example code?"

```{r}
# Load necessary libraries
library(dplyr)

# Filter for the 5 states you are interested in
states <- c("TN", "MS", "GA", "FL", "CA")
smoke_filtered <- smoke_expense_2 %>% filter(LocationAbbr %in% states)

# Calculate mean and standard deviation
smoke_summary <- smoke_filtered %>%
  group_by(LocationAbbr, Year, Variable) %>%
  summarise(
    Mean = mean(Data_Value, na.rm = TRUE),
    SD = sd(Data_Value, na.rm = TRUE)
  )

# Print the summary statistics
print(smoke_summary)

```

This is good information, but since there's only one observation for each expense category per year, we don't learn much from a mean or standard deviation (SD) calculation. We can group each year together to get the mean and SD to allow for easier creation of synthetic data for the second part of this exercise.

```{r}
# Calculate mean and standard deviation for all 5 states combined
smoke_summary <- smoke_filtered %>%
  group_by(LocationAbbr, Variable) %>%
  summarise(
    Mean = mean(Data_Value, na.rm = TRUE),
    SD = sd(Data_Value, na.rm = TRUE)
  )

# Print the summary statistics
print(smoke_summary)

```

Now we can create some visualizations of this information for easier interpretation of the data. I entered the following prompt into Microsoft Copilot with GPT-4 in "Precise mode": "I want to make plots now to determine if the distribution of this data is relatively normal. How would I go about that?"

I received this code string from it, with the only modifications I made being the proper column names and adjusting the binwidth to the square root of all of the observations:

```{r}
# Load necessary libraries
library(ggplot2)

# Create a histogram
ggplot(smoke_filtered, aes(x = Data_Value)) + 
  geom_histogram(binwidth = 12.25, fill = "blue", color = "black") +
  facet_wrap(~ Variable) +
  theme_minimal() +
  labs(title = "Histogram of Expenses", x = "Expense", y = "Frequency")

# Create a density plot
ggplot(smoke_filtered, aes(x = Data_Value)) + 
  geom_density(fill = "blue") +
  facet_wrap(~ Variable) +
  theme_minimal() +
  labs(title = "Density Plot of Expenses", x = "Expense", y = "Density") 

# Create a Q-Q plot
library(car)
qqPlot(smoke_filtered$Data_Value, distribution = "norm", main = "Q-Q Plot of Expenses")

```

The histogram isn't very useful for much besides visualizing the numbers, but the Q-Q plot shows that the distribution for the data as a whole seems normal but contains some outliers to the left and the right. The density plots point out that "Other", "Nursing Home", and "Prescription Drugs", and "Ambulatory" expenses are pretty skewed, but the other two are pretty normally distributed. Hopefully this is enough information to create a good synthetic data set that mimics the trends seen in this one!

### This section was contributed by Emma Hardin-Parker 

```{r}
# Load required packages
library(here)
library(dplyr)
library(ggplot2)
library(skimr)
library(gtsummary)
```

```{r}
# Set a seed for reproducibility
set.seed(207)
# Define the number of observations to generate
n_obs <- 150
```

I am now going to get a feel for the data ussing skimr and gtsummary() functions. 

```{r}
skimr::skim(smoke_filtered)
```
```{r}
gtsummary::tbl_summary(smoke_filtered, statistic = list(
  all_continuous() ~ "{mean}/{median}/{min}/{max}/{sd}",
  all_categorical() ~ "{n} / {N} ({p}%)"
),)
```

Now I am going to create a synthetic data set based off the actual data. 

```{r}
syn_smoke <- data.frame(
  Year = integer(n_obs),
  LocationAbbr = character(n_obs),
  Variable = character(n_obs),
  Data_Value = numeric(n_obs)
)


#Variable1
syn_smoke$Year <- sample(c("2005", "2006", "2007", "2008", "2009"),
                         n_obs, replace = TRUE,
                         prob = as.integer(table(smoke_filtered$Year)))
                   
#Variable2 
syn_smoke$LocationAbbr <- sample(c("GA", "MS", "TN", "FL", "CA"),
                                 n_obs, replace = TRUE,
                                 prob =  as.numeric(table(smoke_filtered$LocationAbbr)/100))

#Variable3
syn_smoke$Variable <- sample(c("Ambulatory", "Hospital", "Nursing Home", "Other", "Prescription Drugs", "Total"),
                             n_obs,
                             replace = TRUE,
                             prob = as.numeric(table(smoke_filtered$Variable)/100))
#Variable4
syn_smoke$Data_Value <- round(runif(n_obs,
                                    min = min(smoke_filtered$Data_Value),
                                    max = max(smoke_filtered$Data_Value)), 1)
```

To make sure the synthetic data set was created properly, I am going to use the head(), glimpse(), and summary() functions to see if the first five rows look okay for further analyses.

```{r}
head(syn_smoke)
summary(syn_smoke)
glimpse(syn_smoke)
```
Everything looks as it should, so it's time to make some exploratory figures and tables. 

```{r}
# Calculate mean and standard deviation for all 5 states combined
syn_smoke_summary <- syn_smoke %>%
  group_by(LocationAbbr, Variable) %>%
  summarise(
    Mean = mean(Data_Value, na.rm = TRUE),
    SD = sd(Data_Value, na.rm = TRUE)
  )
```
```{r}
# Create a histogram
syn_hist <- ggplot(syn_smoke, aes(x = Data_Value)) + 
  geom_histogram(binwidth = 12.25, fill = "blue", color = "black") +
  facet_wrap(~ Variable) +
  theme_minimal() +
  labs(title = "Histogram of Expenses", x = "Expense", y = "Frequency")
print(syn_hist)


# Create a density plot
library(scales)
syn_dens <- ggplot(syn_smoke, aes(x = Data_Value)) + 
  geom_density(fill = "blue") +
  facet_wrap(~ Variable) +
  theme_minimal() +
  labs(title = "Density Plot of Expenses", x = "Expense", y = "Density") +
  scale_x_continuous(labels = label_number())
print(syn_dens)


# Create a Q-Q plot
library(car)
syn_qq <- qqPlot(syn_smoke$Data_Value, distribution = "norm", main = "Q-Q Plot of Expenses")
```
The histograms produced between the original data and the synthetic data were quite different. Most of the individual histograms per Variable created with the original data were skewed to the right, while the histograms produced with the synthetic data were evenly distributed throughout. 

The density plots also look different between the original data and the synthetic data, however, I struggled to code this plot. For some reason, the density value on the y-axis used scientific notation and I had to add an additional line of code to remove it. Even with that transformation, the densities on average are higher in the synthetic plot than the original plot. 

The q-q plot was undoubtedly the most interesting plot to compare to the original data. The synthetic q-q plot is significantly more normally distributed that the original plot. Though the histograms differed drastically, it is much easier to compare normality between the data sets using a q-q plot, so I am happy that I was able to successfully create this one. 

