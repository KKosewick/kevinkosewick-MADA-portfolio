[
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise, I’ll be using a dataset I got from the CDC website about smoking attributable expenses in the U.S. The data includes expenses per state for different categories such as hospital bills, ambulances, and prescriptions as well as overall expenses for the U.S. from 2005-2009. We’ll do an exploratory data analysis and some data processing for this exercise.\n\n#load the dataset\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\nsmoke_expense&lt;- read.csv(here(\"cdcdata-exercise\", \"SAE.csv\"))\n#make sure data fully loaded by checking number of rows and columns. We should have 19 variables and 1560 observations\nnrow(smoke_expense)\n\n[1] 1560\n\nncol(smoke_expense)\n\n[1] 19\n\n#check the structure and summary of the data. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #\nsummary(smoke_expense)\n\n      Year      LocationAbbr       LocationDesc        DataSource       \n Min.   :2005   Length:1560        Length:1560        Length:1560       \n 1st Qu.:2006   Class :character   Class :character   Class :character  \n Median :2007   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2007                                                           \n 3rd Qu.:2008                                                           \n Max.   :2009                                                           \n  TopicType          TopicDesc         MeasureDesc          Variable        \n Length:1560        Length:1560        Length:1560        Length:1560       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value      \n Length:1560        Length:1560        Min.   :     8.4  \n Class :character   Class :character   1st Qu.:   105.7  \n Mode  :character   Mode  :character   Median :   306.1  \n                                       Mean   :  1545.2  \n                                       3rd Qu.:   842.3  \n                                       Max.   :132459.8  \n Data_Value_Footnote_Symbol Data_Value_Footnote GeoLocation       \n Length:1560                Length:1560         Length:1560       \n Class :character           Class :character    Class :character  \n Mode  :character           Mode  :character    Mode  :character  \n                                                                  \n                                                                  \n                                                                  \n Topic.Type.ID        Topic.ID          Measure.ID        SubMeasureID      \n Length:1560        Length:1560        Length:1560        Length:1560       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder\n Min.   :1.0  \n 1st Qu.:2.0  \n Median :3.5  \n Mean   :3.5  \n 3rd Qu.:5.0  \n Max.   :6.0  \n\n#structure(smoke_expense)\n\nWe can see that there’s one observation for each of our 6 expense types per year. We can also see from the structure and summary that there are many columns that we don’t need for an analysis. We can go ahead and pick out the ones that are of interest to us: year, location, variable (which refers to the type of expense), and value (which is the cost in millions of dollars for each expense type). All of the other columns seem to be for record keeping purposes.\n\n#make a new data frame containing only our four columns of interest: Year, LocationAbbr, Variable, and Data_Value.\nsmoke_expense_2 &lt;- smoke_expense[, c(\"Year\", \"LocationAbbr\", \"Variable\", \"Data_Value\")]\n#check the new object to make sure it has everything we want. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #\n#structure(smoke_expense_2)\nsummary(smoke_expense_2)\n\n      Year      LocationAbbr         Variable           Data_Value      \n Min.   :2005   Length:1560        Length:1560        Min.   :     8.4  \n 1st Qu.:2006   Class :character   Class :character   1st Qu.:   105.7  \n Median :2007   Mode  :character   Mode  :character   Median :   306.1  \n Mean   :2007                                         Mean   :  1545.2  \n 3rd Qu.:2008                                         3rd Qu.:   842.3  \n Max.   :2009                                         Max.   :132459.8  \n\n\nThis new object is much more condensed and easier to work with. We’ll check now to make sure there are no NA values and then proceed with some EDA.\n\n#check for NA values\nna_check&lt;- is.na(smoke_expense_2)\nprint(sum(na_check))\n\n[1] 0\n\n\nSince there aren’t any NA values and looking at the structure indicates no missing values, we can begin to check the mean and standard deviation of each expense type across the 4 years in the data set. Rather than looking at all 50 states, let’s focus on 5 to make this a bit easier. We’ll use GA, TN, MS, CA, and FL.\nUsing Microsoft Copilot with GPT-4 in “Precise Mode”, I entered the following prompt to get the code I’m about to use: “For my exploratory analysis I want to summarize each variable in a way that can be described by a distribution. For instance, I want to be able to determine the mean and standard deviation of each expense type for 5 different states over the 4 year period recorded in the dataset. What is the best approach for this and could you provide some example code?”\n\n# Load necessary libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Filter for the 5 states you are interested in\nstates &lt;- c(\"TN\", \"MS\", \"GA\", \"FL\", \"CA\")\nsmoke_filtered &lt;- smoke_expense_2 %&gt;% filter(LocationAbbr %in% states)\n\n# Calculate mean and standard deviation\nsmoke_summary &lt;- smoke_filtered %&gt;%\n  group_by(LocationAbbr, Year, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr', 'Year'. You can override\nusing the `.groups` argument.\n\n# Print the summary statistics\nprint(smoke_summary)\n\n# A tibble: 150 × 5\n# Groups:   LocationAbbr, Year [25]\n   LocationAbbr  Year Variable             Mean    SD\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n 1 CA            2005 Ambulatory          2057.    NA\n 2 CA            2005 Hospital            5309.    NA\n 3 CA            2005 Nursing Home         592.    NA\n 4 CA            2005 Other                620.    NA\n 5 CA            2005 Prescription Drugs  1883.    NA\n 6 CA            2005 Total              10460.    NA\n 7 CA            2006 Ambulatory          2122.    NA\n 8 CA            2006 Hospital            5652.    NA\n 9 CA            2006 Nursing Home         641.    NA\n10 CA            2006 Other                677.    NA\n# ℹ 140 more rows\n\n\nThis is good information, but since there’s only one observation for each expense category per year, we don’t learn much from a mean or standard deviation (SD) calculation. We can group each year together to get the mean and SD to allow for easier creation of synthetic data for the second part of this exercise.\n\n# Calculate mean and standard deviation for all 5 states combined\nsmoke_summary &lt;- smoke_filtered %&gt;%\n  group_by(LocationAbbr, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr'. You can override using the\n`.groups` argument.\n\n# Print the summary statistics\nprint(smoke_summary)\n\n# A tibble: 30 × 4\n# Groups:   LocationAbbr [5]\n   LocationAbbr Variable             Mean     SD\n   &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n 1 CA           Ambulatory          2294.  208. \n 2 CA           Hospital            6006.  571. \n 3 CA           Nursing Home         699.   87.9\n 4 CA           Other                725.   76.8\n 5 CA           Prescription Drugs  2172.  197. \n 6 CA           Total              11896. 1131. \n 7 FL           Ambulatory          1419.  105. \n 8 FL           Hospital            3532.  270. \n 9 FL           Nursing Home         595.   58.9\n10 FL           Other                514.   53.3\n# ℹ 20 more rows\n\n\nNow we can create some visualizations of this information for easier interpretation of the data. I entered the following prompt into Microsoft Copilot with GPT-4 in “Precise mode”: “I want to make plots now to determine if the distribution of this data is relatively normal. How would I go about that?”\nI received this code string from it, with the only modifications I made being the proper column names and adjusting the binwidth to the square root of all of the observations:\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a histogram\nggplot(smoke_filtered, aes(x = Data_Value)) + \n  geom_histogram(binwidth = 12.25, fill = \"blue\", color = \"black\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Histogram of Expenses\", x = \"Expense\", y = \"Frequency\")\n\n\n\n\n\n\n\n# Create a density plot\nggplot(smoke_filtered, aes(x = Data_Value)) + \n  geom_density(fill = \"blue\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Density Plot of Expenses\", x = \"Expense\", y = \"Density\")\n\n\n\n\n\n\n\n# Create a Q-Q plot\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nqqPlot(smoke_filtered$Data_Value, distribution = \"norm\", main = \"Q-Q Plot of Expenses\")\n\n\n\n\n\n\n\n\n[1] 131  45\n\n\nThe histogram isn’t very useful for much besides visualizing the numbers, but the Q-Q plot shows that the distribution for the data as a whole seems normal but contains some outliers to the left and the right. The density plots point out that “Other”, “Nursing Home”, and “Prescription Drugs”, and “Ambulatory” expenses are pretty skewed, but the other two are pretty normally distributed. Hopefully this is enough information to create a good synthetic data set that mimics the trends seen in this one!"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "EMMA HARDIN-PARKER contributed to this exercise.\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nWarning: package 'here' was built under R version 4.3.2"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nHair Color\n0\n1\n1\n2\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nStrands of Hair\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5592.11111\n3533.51456\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\nFigure 2 shows a box plot giving the distributions of “Height” and “Hair Color” made from R. Black haired individuals tend to average about 173 cm. with a max of 180 cm. and a low of 162. Brown haired individuals average about 166 cm. with a max of 176 and low of 154. Red haired individuals average about 166 cm. with a max of 171 and low of 151. There is no distribution for purple and white due to there being only one individual for each.\n\n\n\n\n\n\n\n\nFigure 2: A box plot of Height (cm) and Hair Color.\n\n\n\n\n\nFigure 3 shows a scatter plot showing the correlation between “Weight” and “Strands of Hair” made from R. It turns out that there is no correlation between these two variables.\n\n\n\n\n\n\n\n\nFigure 3: A scatter plot showing Weight (lbs) and Strands of Hair."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nExample Table 3 shows a silly linear model fit of “Hair Color” and “Strands of Hair” as predictors for “Height”. The small sample size makes drawing any accurate conclusions difficult.\n\n\n\n\nTable 3: Linear model fit table of Hair Color and Strands of Hair as predictors for Height.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n176.7015914\n11.9719368\n14.7596495\n0.0006747\n\n\nHair ColorBR\n-3.4262889\n14.0614015\n-0.2436662\n0.8232017\n\n\nHair ColorP\n13.5369154\n17.6933112\n0.7650866\n0.4999000\n\n\nHair ColorR\n-8.9391056\n13.5302875\n-0.6606737\n0.5560240\n\n\nHair ColorW\n-39.1558092\n16.2731172\n-2.4061653\n0.0953447\n\n\nStrands of Hair\n-0.0009048\n0.0018297\n-0.4945259\n0.6548825"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   1   2     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean     sd  p0  p25  p50  p75 p100\n1 Height                  0             1  166.    16.0 133  156  166  178  183\n2 Weight                  0             1   70.1   21.2  45   55   70   80  110\n3 Strands of Hair         0             1 5592.  3534.    1 5000 6000 8178 9122\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▃▁▃▂▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nScatterplot\n\np5 &lt;- mydata %&gt;% ggplot(aes(x = Weight, y = `Strands of Hair`)) +\n  geom_point()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-strands-scatter.png\")\nggsave(filename = figure_file, plot = p5)\n\nSaving 7 x 5 in image\n\n\nBoxplot\n\np6 &lt;- mydata %&gt;% ggplot(aes(x = `Hair Color`, y = Height)) +\n  geom_boxplot()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-color-box.png\")\nggsave(filename = figure_file, plot = p6)\n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   3   6     0        6          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean     sd  p0  p25  p50  p75 p100\n1 Height                  0             1  166.    16.0 133  156  166  178  183\n2 Weight                  0             1   70.1   21.2  45   55   70   80  110\n3 Strands of Hair         0             1 5592.  3534.    1 5000 6000 8178 9122\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▃▁▃▂▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Load the dslabs package. Install if you haven’t. Then, inspect the gapminder dataset.\n\n#load dslabs package and tidyverse\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.2\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#look at help file for gapminder data\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data \nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nCreate a new object that contains only the African countries. Then, check the structure and summary of the new object.\n\n#create the object with only African countries\nafrican_countries &lt;- gapminder[gapminder$continent == \"Africa\", ]\n#check the structure and summary\nstr(african_countries)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(african_countries)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nNow, using the new African countries object, create two new objects. One should only contain “infant_mortality” and “life_expectancy” and the other should only hold “population” and “life_expectancy”.\n\n#create the object with only population and life expectancy data\nafrican_countries_pop_life &lt;- african_countries[, c(\"population\", \"life_expectancy\")]\n#create the object with only infant mortality and life expectancy data\nafrican_countries_infant_life &lt;- african_countries[, c(\"infant_mortality\", \"life_expectancy\")]\n\nNow that we’ve created two new objects that look at these specific variables, we can inspect them and get a better idea of the data.\n\n#look at the structure and summary of the first object\nstr(african_countries_pop_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(african_countries_pop_life)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n#do the same for the second object\nstr(african_countries_infant_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(african_countries_infant_life)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\nUsing the two new objects we can now create plots to characterize the relationship between life expectancy, population, and infant mortality. We’ll create two plots; one that analyzes life expectancy vs infant mortality and one that analyzes life expectancy vs population size. The latter will have a log scale to make the data easier to visualize.\n\n#load ggplot2 to create better plots\nlibrary(ggplot2)\n\n# Plot 1: Life expectancy vs. Infant mortality. lab() creates titles for the graph.\nggplot(african_countries_infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality\")\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot 2: Life expectancy vs. Population size.\n#scale_x_log10 puts the x axis (population) on a log scale.\nggplot(african_countries_pop_life, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size (log scale)\")\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn Plot 1, we can see a negative correlation. As life expectancy decreases, the number of infants dying increases; this makes sense since more developed countries with better healthcare have higher life expectancies and lower infant mortality rates. In Plot 2, population size and life expectancy are positively correlated. This is logical, as longer lives allow for greater population growth and more infrequent deaths. The “streaks” in the data can be attributed to the presence of different years for individual countries in the dataset.\nKnowing this, we can begin to narrow in on certain years and see which ones would be easiest to analyze given our dataset. We’ll figure out which years have missing data for infant mortality.\n\n#find which years have missing data for infant mortality. \n#is.na() identifies which rows have na as their value\n#select() shows us the years that are associated with these rows.\nafrican_countries %&gt;%\n  filter(is.na(infant_mortality)) %&gt;%\n  select(year)\n\n    year\n1   1960\n2   1960\n3   1960\n4   1960\n5   1960\n6   1960\n7   1960\n8   1960\n9   1960\n10  1960\n11  1961\n12  1961\n13  1961\n14  1961\n15  1961\n16  1961\n17  1961\n18  1961\n19  1961\n20  1961\n21  1961\n22  1961\n23  1961\n24  1961\n25  1961\n26  1961\n27  1961\n28  1962\n29  1962\n30  1962\n31  1962\n32  1962\n33  1962\n34  1962\n35  1962\n36  1962\n37  1962\n38  1962\n39  1962\n40  1962\n41  1962\n42  1962\n43  1962\n44  1963\n45  1963\n46  1963\n47  1963\n48  1963\n49  1963\n50  1963\n51  1963\n52  1963\n53  1963\n54  1963\n55  1963\n56  1963\n57  1963\n58  1963\n59  1963\n60  1964\n61  1964\n62  1964\n63  1964\n64  1964\n65  1964\n66  1964\n67  1964\n68  1964\n69  1964\n70  1964\n71  1964\n72  1964\n73  1964\n74  1964\n75  1965\n76  1965\n77  1965\n78  1965\n79  1965\n80  1965\n81  1965\n82  1965\n83  1965\n84  1965\n85  1965\n86  1965\n87  1965\n88  1965\n89  1966\n90  1966\n91  1966\n92  1966\n93  1966\n94  1966\n95  1966\n96  1966\n97  1966\n98  1966\n99  1966\n100 1966\n101 1966\n102 1967\n103 1967\n104 1967\n105 1967\n106 1967\n107 1967\n108 1967\n109 1967\n110 1967\n111 1967\n112 1967\n113 1968\n114 1968\n115 1968\n116 1968\n117 1968\n118 1968\n119 1968\n120 1968\n121 1968\n122 1968\n123 1968\n124 1969\n125 1969\n126 1969\n127 1969\n128 1969\n129 1969\n130 1969\n131 1970\n132 1970\n133 1970\n134 1970\n135 1970\n136 1971\n137 1971\n138 1971\n139 1971\n140 1971\n141 1971\n142 1972\n143 1972\n144 1972\n145 1972\n146 1972\n147 1972\n148 1973\n149 1973\n150 1973\n151 1973\n152 1973\n153 1973\n154 1974\n155 1974\n156 1974\n157 1974\n158 1974\n159 1975\n160 1975\n161 1975\n162 1975\n163 1975\n164 1976\n165 1976\n166 1976\n167 1977\n168 1977\n169 1977\n170 1978\n171 1978\n172 1979\n173 1979\n174 1980\n175 1981\n176 2016\n177 2016\n178 2016\n179 2016\n180 2016\n181 2016\n182 2016\n183 2016\n184 2016\n185 2016\n186 2016\n187 2016\n188 2016\n189 2016\n190 2016\n191 2016\n192 2016\n193 2016\n194 2016\n195 2016\n196 2016\n197 2016\n198 2016\n199 2016\n200 2016\n201 2016\n202 2016\n203 2016\n204 2016\n205 2016\n206 2016\n207 2016\n208 2016\n209 2016\n210 2016\n211 2016\n212 2016\n213 2016\n214 2016\n215 2016\n216 2016\n217 2016\n218 2016\n219 2016\n220 2016\n221 2016\n222 2016\n223 2016\n224 2016\n225 2016\n226 2016\n\n\nThere is data missing up to 1981 and then again for 2016, so we’ll select 2000. We’ll create a new object now with only observations from 2000.\n\n#create an object with only data from 2000\nafrican_countries_2000 &lt;- african_countries[african_countries$year == 2000, ]\n\nNow, we’ll make the same plots as before using only the data from 2000.\n\n# Plot 3: Life expectancy vs. Infant mortality.\nggplot(african_countries_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality\")\n\n\n\n\n\n\n\n# Plot 4: Life expectancy vs. Population size. \nggplot(african_countries_2000, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size (log scale)\")\n\n\n\n\n\n\n\n\nThere still seems to be a negative correlation in plot 3, but plot 4 shows no noticeable correlation. We can now create some linear models with this data and draw more conclusions from the year 2000.\n\n#Table 1: fit life expectancy as a function of infant mortality. \n#lm() creates a linear model for the specified variables from a given dataset.\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, african_countries_2000)\n#print results to screen\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = african_countries_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n#Table 2: fit life expectancy as a function of population size\nfit2 &lt;- lm(life_expectancy ~ population, african_countries_2000)\n#print results to screen\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = african_countries_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nBased off of the results from the fit, we can see that infant mortality is a statistically significant predictor of life expectancy for African countries in the year 2000. On the other hand, population does not seem to be a statistically significant predictor for life expectancy in 2000. These are logical conclusions given our prior knowledge of demography.\nThis section contributed by Cory Cribb\nLoading dslabs dataset “murders”. Probably a more morbid data set but interesting to observe nonetheless.\n\nlibrary(dslabs)\nhelp(murders)\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\nsummary(murders)\n\n    state               abb                      region     population      \n Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n Class :character   Class :character   South        :17   1st Qu.: 1696962  \n Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n                                       West         :13   Mean   : 6075769  \n                                                          3rd Qu.: 6636084  \n                                                          Max.   :37253956  \n     total       \n Min.   :   2.0  \n 1st Qu.:  24.5  \n Median :  97.0  \n Mean   : 184.4  \n 3rd Qu.: 268.0  \n Max.   :1257.0  \n\n\nSince I am originally from the Southern region of the US; lets explore murders in that region.\n\nSouth_Murders &lt;- murders[murders$region== \"South\", ]\nstr(South_Murders)\n\n'data.frame':   17 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Arkansas\" \"Delaware\" \"District of Columbia\" ...\n $ abb       : chr  \"AL\" \"AR\" \"DE\" \"DC\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ population: num  4779736 2915918 897934 601723 19687653 ...\n $ total     : num  135 93 38 99 669 376 116 351 293 120 ...\n\nsummary(South_Murders)\n\n    state               abb                      region     population      \n Length:17          Length:17          Northeast    : 0   Min.   :  601723  \n Class :character   Class :character   South        :17   1st Qu.: 2967297  \n Mode  :character   Mode  :character   North Central: 0   Median : 4625364  \n                                       West         : 0   Mean   : 6804378  \n                                                          3rd Qu.: 8001024  \n                                                          Max.   :25145561  \n     total      \n Min.   : 27.0  \n 1st Qu.:111.0  \n Median :207.0  \n Mean   :246.8  \n 3rd Qu.:293.0  \n Max.   :805.0  \n\n\nFrom this data set, it would appear the researchers classified 17 states as being in the Southern region. Let’s explore if population size had any relationship to gun murders.\n\nPop_and_murder &lt;- South_Murders[, c(\"total\", \"population\")]\nstr(Pop_and_murder)\n\n'data.frame':   17 obs. of  2 variables:\n $ total     : num  135 93 38 99 669 376 116 351 293 120 ...\n $ population: num  4779736 2915918 897934 601723 19687653 ...\n\nsummary(Pop_and_murder)\n\n     total         population      \n Min.   : 27.0   Min.   :  601723  \n 1st Qu.:111.0   1st Qu.: 2967297  \n Median :207.0   Median : 4625364  \n Mean   :246.8   Mean   : 6804378  \n 3rd Qu.:293.0   3rd Qu.: 8001024  \n Max.   :805.0   Max.   :25145561  \n\n\nCreate a scatter plot viewing total gun murders on the x-axis and state population on the y-axis to observe a trend. Add a best fit line to the plot to see if there is a trend.\n\nattach(South_Murders)\n\nThe following object is masked from package:tidyr:\n\n    population\n\nplot(total,population, main= \"Total gun murders vs. population size\", xlab=\"Total gun murders\", ylab=\"population\")\nabline(lm(population~total))\n\n\n\n\n\n\n\n\nFrom a quick view of the plot, we see that there appears to be a positive correlation that there are more gun murders in states with higher populations. Let’s run a linear model to see if the data gives a statistically significant observation.\n\nfit3 &lt;- lm(population~total, South_Murders)\nsummary(fit3)\n\n\nCall:\nlm(formula = population ~ total, data = South_Murders)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5332407  -680032   482183  1257898  1945758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -443125     717119  -0.618    0.546    \ntotal          29370       2229  13.178 1.19e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1898000 on 15 degrees of freedom\nMultiple R-squared:  0.9205,    Adjusted R-squared:  0.9152 \nF-statistic: 173.7 on 1 and 15 DF,  p-value: 1.189e-09\n\n\nFrom the simple linear regression, we see that the slope is statistically significant. The adjusted R-squared is 0.9152 which would indicated a Strong, Positive correlation in total gun murders per Gross Population size in the Southern Region of the United States of America."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Me holding a corn snake at the Savannah River Site Ecology Lab. They’re friendly but a bit freaky!"
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "Background",
    "text": "Background\n\nEducation\nI completed my Bachelors in Wildlife Sciences at Mississippi State University in May of 2023. I entered the ILS program at UGA the following August and have since matriculated into the EHS program as a first year in Erin Lipp’s lab.\n\n\nHometown\nI was born in Orange, CA and raised in Collierville, TN outside of Memphis. My parents and 5 pets (3 cats and 2 dogs) still live there!\n\n\nFun Fact\nDuring field research on pollinators at my undergraduate university, I was stung about 5 times by a swarm of yellowjackets that I apparently offended! I couldn’t move one of my fingers for three days."
  },
  {
    "objectID": "aboutme.html#research-interestsexperience",
    "href": "aboutme.html#research-interestsexperience",
    "title": "About me",
    "section": "Research Interests/Experience",
    "text": "Research Interests/Experience\n\nCurrent Research\nI don’t have my own project yet, but I’m interested in pursuing research on bacterial pathogens and environmental epidemiology. I would love to include a wildlife aspect!\n\n\nExperience\nI have taken some basic statistics courses and an overview of data analysis using R. I don’t know any coding languages, but I can navigate RStudio and make some basic plots and analyses.\n\n\nHopes for the class\nI hope to become proficient in making my data usable for analysis and to learn more about creating a workflow that allows for reproducibility.\n\n\nInteresting Data Analysis Facts\nThis website has some fun facts about data analysis. I found it interesting that 80% of a data scientist’s time is spent cleaning the data!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Kosewick’s website and data analysis portfolio",
    "section": "",
    "text": "Hello!\n\nAnd welcome to my website and data analysis portfolio.\nHere I’ll be compiling my projects for my MADA class in the Spring 2024 semester.\n\nFeel free to use the Menu Bar above to learn more about me and my work up to this point. Thank you for looking!\nLast updated 1/26/2024"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\n\nWarning: package 'readxl' was built under R version 4.3.2\n\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height            &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"16…\n$ Weight            &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 5…\n$ Gender            &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"…\n$ `Strands of Hair` &lt;dbl&gt; 4, 5000, 5005, 6000, 7000, 8000, 1, 9000, 7500, 7550…\n$ `Hair Color`      &lt;chr&gt; \"Blonde\", \"Red\", \"Black\", \"Brown\", \"White\", \"Purple\"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Strands of Hair\n Length:14          Min.   :  45.0   Length:14          Min.   :   1   \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:5010   \n Mode  :character   Median :  70.0   Mode  :character   Median :6525   \n                    Mean   : 602.7                      Mean   :5960   \n                    3rd Qu.:  90.0                      3rd Qu.:7888   \n                    Max.   :7000.0                      Max.   :9122   \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Strands of Hair` `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M                      4 Blonde      \n2 175        70 O                   5000 Red         \n3 sixty      60 F                   5005 Black       \n4 178        76 F                   6000 Brown       \n5 192        90 NA                  7000 White       \n6 6          55 F                   8000 Purple      \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n5959.57\n2882.38\n1\n5009.75\n6525\n7887.5\n9122\n▂▁▃▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nStrands of Hair\n0\n1\n5592.11\n3533.51\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\n\nWarning: package 'readxl' was built under R version 4.3.2\n\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                       `Allowed Values`  \n  &lt;chr&gt;           &lt;chr&gt;                                       &lt;chr&gt;             \n1 Height          height in centimeters                       numeric value &gt;0 …\n2 Weight          weight in kilograms                         numeric value &gt;0 …\n3 Gender          identified gender (male/female/other)       M/F/O/NA          \n4 Strands of Hair How many strands of hair the individual has numeric value &gt;0 …\n5 Hair Color      What color the invididual's hair is         BR/BL/R/W/P; BR=b…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height            &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"16…\n$ Weight            &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 5…\n$ Gender            &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"…\n$ `Strands of Hair` &lt;dbl&gt; 4, 5000, 5005, 6000, 7000, 8000, 1, 9000, 7500, 7550…\n$ `Hair Color`      &lt;chr&gt; \"BL\", \"R\", \"BL\", \"BR\", \"W\", \"P\", \"R\", \"BR\", \"BR\", \"B…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Strands of Hair\n Length:14          Min.   :  45.0   Length:14          Min.   :   1   \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:5010   \n Mode  :character   Median :  70.0   Mode  :character   Median :6525   \n                    Mean   : 602.7                      Mean   :5960   \n                    3rd Qu.:  90.0                      3rd Qu.:7888   \n                    Max.   :7000.0                      Max.   :9122   \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Strands of Hair` `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M                      4 BL          \n2 175        70 O                   5000 R           \n3 sixty      60 F                   5005 BL          \n4 178        76 F                   6000 BR          \n5 192        90 NA                  7000 W           \n6 6          55 F                   8000 P           \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n5959.57\n2882.38\n1\n5009.75\n6525\n7887.5\n9122\n▂▁▃▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nStrands of Hair\n0\n1\n5592.11\n3533.51\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing informatio"
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise, we will generate a synthetic data set that simulates foraging trends of the Northern Mockingbird somewhere in the United States. We will then generate some figures and statistical analyses to explore our new data set. I’ll be using Microsoft Edge AI tools to help generate code. Let’s begin by loading helpful packages for creating synthetic data sets.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ tibble  3.2.1\n✔ readr   2.1.5     ✔ tidyr   1.3.0\n✔ stringr 1.5.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nI’ve entered this prompt into Microsoft Edge copilot: “Write R code that creates data for N=100 Northern Mockingbirds. Individuals ages are randomly distributed between 1 and 14 years. Assume that individuals forage for two types of food: insects and plants. Individuals can consume a combination of either food type at an average of half an ounce daily. Food consumption is measured in ounces every other day. Assume that insects are more plentiful in warmer seasons; this should be reflected in the amount of each food type consumed in each season. The data should be collected over the course of a year.\nCreate a data frame that contains Food Type, Age, Daily Food Consumption (in ounces), Season, and Sex.\nFor function calls, specify explicitly the package from which the function comes from.\nAdd thorough documentation to the code so it is clear what each line of code does.”\nThis generated the following code:\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years\nage &lt;- sample(1:14, N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\"] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\"] * \n  rep(insect_proportion, each = N/2)[data$Season[data$Food_Type == \"insects\"]]\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                     NA spring female\n 2 insects      14                     NA spring male  \n 3 insects       3                     NA spring female\n 4 insects      10                     NA spring male  \n 5 insects       2                     NA spring female\n 6 insects       6                     NA spring male  \n 7 insects      11                     NA spring male  \n 8 insects       5                     NA spring male  \n 9 insects       4                     NA spring female\n10 insects      14                     NA spring female\n# ℹ 90 more rows\n\n\nThis is a good start, but there are some problems. First, the AI only gave values of NA to food consumption data collected in the spring and summer. Second, there are an abnormally high number of old birds in the population; in a normal wild population, we’d expect to see more mockingbirds in the 2-8 year range, as 14 is on the old end for a mockingbird. We’ll have the AI correct the data frame with the following prompt:\n“Northern Mockingbirds have an average lifespan of 8 years in the wild. Young and elderly individuals have a higher mortality rate. Given this information, please adjust the age variable to more accurately reflect age distribution in a wild population of mockingbirds. Furthermore, mockingbirds consume more food in the spring and summer due to increased energy demands that come with mating and nesting. Please replace the NA food consumption values you gave with randomized values that have an average of 0.5.”\nThe AI then adjusted our previous code string as follows:\n\n# Set the seed for reproducibility\n#set.seed(123)\n\n# Define the number of birds\n#N &lt;- 100\n\n# Generate random ages between 1 and 14 years\n#age &lt;- sample(1:14, N, replace = TRUE, prob = c(0.05, 0.05, 0.1, 0.15, 0.2, 0.15, 0.1, 0.05, 0.025, 0.025))\n\n# Define the two types of food\n#food_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\n#food_consumption &lt;- 0.5\n\n# Define the seasons\n#seasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\n#insect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\n#sex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\n#data &lt;- tibble(\n # Food_Type = rep(food_type, each = N/2),\n# Age = age,\n # Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n # Season = rep(seasons, each = N/4),\n # Sex = sex\n# )\n\n# Adjust the food consumption based on the season\n#data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] &lt;- \n # data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] * \n # rep(insect_proportion[1:2], each = sum(data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")))\n\n# Replace the NA values with randomized values that have an average of 0.5\n#data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), 0.5, 0.1)))\n\n# Print the data\n#data\n\nThis generates a new error in the way the AI created values for the age variable. I gave the AI this prompt to correct the broken lines:\n“The issue with NA values has been solved. I now need you to correct the line where you generate random ages between 1 and 14 years to resolve this error: Error in sample.int(length(x), size, replace, prob) : incorrect number of probabilities”\nThe following code was generated:\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years with a probability distribution that reflects the higher mortality rate of young and elderly individuals\nage &lt;- sample(c(1:4, 5:8, 9:12, 13:14), N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] * \n  rep(insect_proportion[1:2], each = sum(data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")))\n\nWarning in data$Daily_Food_Consumption[data$Food_Type == \"insects\" &\n(data$Season == : number of items to replace is not a multiple of replacement\nlength\n\n# Replace the NA values with randomized values that have an average of 0.5\nif (any(is.na(data$Daily_Food_Consumption))) {\n  data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), mean(Daily_Food_Consumption), 0.1)))\n}\n\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.286 spring female\n 2 insects      14                  0.384 spring male  \n 3 insects       3                  0.406 spring female\n 4 insects      10                  0.329 spring male  \n 5 insects       2                  0.284 spring female\n 6 insects       6                  0.309 spring male  \n 7 insects      11                  0.383 spring male  \n 8 insects       5                  0.289 spring male  \n 9 insects       4                  0.206 spring female\n10 insects      14                  0.284 spring female\n# ℹ 90 more rows\n\n\nWe seem to have finally generated a data set that should reflect associations we expect to see in the population. However, I’d like to make one more adjustment that reflects increased food consumption by females in the spring and summer due to reproductive demands. I’ll use a 1.75 times increase in food consumption, as data is limited on the exact amounts/averages mockingbirds consume.\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years with a probability distribution that reflects the higher mortality rate of young and elderly individuals\nage &lt;- sample(c(1:4, 5:8, 9:12, 13:14), N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season and sex\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\") & data$Sex == \"female\"] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\") & data$Sex == \"female\"] * \n  1.75\n\n# Replace the NA values with randomized values that have an average of 0.5\nif (any(is.na(data$Daily_Food_Consumption))) {\n  data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), mean(Daily_Food_Consumption), 0.1)))\n}\n\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.835 spring female\n 2 insects      14                  0.640 spring male  \n 3 insects       3                  1.18  spring female\n 4 insects      10                  0.549 spring male  \n 5 insects       2                  0.828 spring female\n 6 insects       6                  0.515 spring male  \n 7 insects      11                  0.638 spring male  \n 8 insects       5                  0.482 spring male  \n 9 insects       4                  0.601 spring female\n10 insects      14                  0.829 spring female\n# ℹ 90 more rows\n\n\nThis dataset looks a lot better. We’ll now check the structure and summary to get a better idea of what we created.\n\n#check the structure and summary\nsummary(data)\n\n  Food_Type              Age        Daily_Food_Consumption    Season         \n Length:100         Min.   : 1.00   Min.   :0.2652         Length:100        \n Class :character   1st Qu.: 5.00   1st Qu.:0.4490         Class :character  \n Mode  :character   Median : 8.50   Median :0.5660         Mode  :character  \n                    Mean   : 7.92   Mean   :0.6026                           \n                    3rd Qu.:11.00   3rd Qu.:0.6929                           \n                    Max.   :14.00   Max.   :1.2025                           \n     Sex           \n Length:100        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nstructure(data)\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.835 spring female\n 2 insects      14                  0.640 spring male  \n 3 insects       3                  1.18  spring female\n 4 insects      10                  0.549 spring male  \n 5 insects       2                  0.828 spring female\n 6 insects       6                  0.515 spring male  \n 7 insects      11                  0.638 spring male  \n 8 insects       5                  0.482 spring male  \n 9 insects       4                  0.601 spring female\n10 insects      14                  0.829 spring female\n# ℹ 90 more rows\n\n\nOur data looks good and reflects the averages we had the AI incorporate when creating our values. We’ll now create a few plots looking at some relationships in the data.\n\n#create a plot with food consumption as a function of season for all individuals (stratified by sex)\nggplot(data, aes(x = Season, y = Daily_Food_Consumption, color = Sex)) +\n  geom_boxplot() +\n  labs(title = \"Food Consumption as a Function of Season for All Individuals, Stratified by Sex\",\n       x = \"Season\",\n       y = \"Daily Food Consumption (oz)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe boxplot shows that our assumptions are reflected in the data set; females consume more in the spring and summer and our average food consumption is 0.5 ounces a day. Now we’ll see if the data accurately shows changes in the primary type of food consumed over the seasons.\n\n#create a histogram comparing food type consumed in different seasons stratified by sex\nggplot(data, aes(x = Season, fill = Food_Type)) +\n  geom_bar(position = \"dodge\", stat = \"count\") +\n  facet_grid(. ~ Sex) +\n  labs(title = \"Food Type Comparison Across Seasons, Stratified by Sex\",\n       x = \"Season\",\n       y = \"Count\",\n       fill = \"Food Type\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe plots show that insects are the predominant food source in the spring and summer while plants dominate the winter and fall. This accurately reflects booms in the insect population in spring and summer; Northern Mockingbirds consume over 85% insects then, dropping to around 15% in the colder seasons. Now that we see our data is tidy, reflects our assumptions and follows the trends we identified, we can fit the data to some linear models.\n\n#create a linear model with season and sex as predictors for food consumption\nmodel1&lt;- lm(Daily_Food_Consumption~ Season + Sex, data = data)\nsummary(model1)\n\n\nCall:\nlm(formula = Daily_Food_Consumption ~ Season + Sex, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27214 -0.11026 -0.01652  0.08739  0.35195 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.58657    0.03355  17.483  &lt; 2e-16 ***\nSeasonspring  0.26398    0.04267   6.187 1.54e-08 ***\nSeasonsummer  0.11562    0.04261   2.713  0.00791 ** \nSeasonwinter -0.03203    0.04288  -0.747  0.45690    \nSexmale      -0.16488    0.03079  -5.354 5.95e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1506 on 95 degrees of freedom\nMultiple R-squared:  0.4768,    Adjusted R-squared:  0.4548 \nF-statistic: 21.65 on 4 and 95 DF,  p-value: 1.023e-12\n\n\nIt seems like winter doesn’t have a significant impact on food consumption, but spring and summer do in the created dataset. Now we’ll move on to make a couple more models.\n\n#create a linear model with season as a predictor for food type\n# model2&lt;- lm(Food_Type ~ Season, data = data)\n# summary(model2)\n\nA linear regression didn’t work for this type of data.The above line generated an error message. After consulting with AI, a multinomial logistic regression model would work better. We’ll also include sex as a predictor in this one.\n\n#load the nnet package for this model\nlibrary(nnet)\n# Create a multinomial logistic regression model with season and sex as predictors for food type\nmodel2 &lt;- multinom(Food_Type ~ Season + Sex, data = data)\n\n# weights:  6 (5 variable)\ninitial  value 69.314718 \niter  10 value 0.021604\niter  20 value 0.012091\niter  30 value 0.000938\niter  40 value 0.000662\niter  50 value 0.000442\niter  60 value 0.000273\niter  70 value 0.000224\niter  80 value 0.000159\niter  90 value 0.000153\niter 100 value 0.000122\nfinal  value 0.000122 \nstopped after 100 iterations\n\nsummary(model2)\n\nCall:\nmultinom(formula = Food_Type ~ Season + Sex, data = data)\n\nCoefficients:\n                  Values Std. Err.\n(Intercept)   13.3268743  186.7045\nSeasonspring -26.8167895  232.8969\nSeasonsummer -26.8610564  236.0821\nSeasonwinter  16.4198276    0.0000\nSexmale        0.5251443  186.4204\n\nResidual Deviance: 0.0002439321 \nAIC: 10.00024 \n\n\nIt seems that the model agrees with our assumptions. The odds of mockingbirds choosing plants over insects are lower in the spring and summer and the opposite in winter. The residual deviance is low, indicating a good fit. We’ve created a pretty good dataset that has the associations and trends we wanted to see."
  }
]