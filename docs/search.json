[
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "ml-models-exercise",
    "section": "",
    "text": "For this exercise, we’ll be practicing some machine learning models using the data from our previous “Fitting exercise”. Much of the base code for this exercise was generated using Microsoft Copilot in Precise Mode; this was then modified to the specifics of our data and to fix errors. We’ll start by loading packages, the data, and creating a random seed for reproducibility.\n\n#Load packages\nlibrary(tidymodels)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(dplyr)\nlibrary(corrplot)\nlibrary(glmnet)\nlibrary(ranger)\nlibrary(tune)\nlibrary(dials)\n#Load the data\nmldata&lt;- readRDS(here(\"ml-models-exercise\", \"modelfitting.rds\"))\n\n#seed setting\nrngseed=1234\n\nNow we’ll do some data processing. We’ll combine the “7” and “88” categories for our RACE variable into a new category, “3”.\n\n# Change 7 and 88 to 3.\n# Convert RACE to numeric\nmldata$RACE &lt;- as.numeric(as.character(mldata$RACE))\n\n# Perform the replacement\nmldata &lt;- mldata %&gt;%\n  mutate(RACE = case_when(\n    RACE %in% c(7, 88) ~ 3,\n    TRUE ~ RACE\n  ))\n\n# Convert RACE back to factor\nmldata$RACE &lt;- as.factor(mldata$RACE)\n\n#Double check to make sure everything is the class we want it\nclass(mldata$RACE)\n\n[1] \"factor\"\n\n\nNow we’ll make a pairwise correlation plot for our continuous variables to make sure we don’t have too much collinearity between variables.\n\n# Select the variables\ncontinuous_vars &lt;- mldata[, c(\"Y\", \"DOSE\", \"AGE\", \"WT\", \"HT\")]\n\n# Compute correlation matrix\ncorrelation_matrix &lt;- cor(continuous_vars)\n\n# Create a pairwise correlation plot (using corrplot)\ncorrplot(correlation_matrix, method = \"circle\")\n\n\n\n\n\n\n\n\nNothing seems to correlated (absolute value of 0.9+), so we can continue with our modelling. We’ll first create a new variable for Body Mass Index (BMI) using the values in our HT and WT columns.\n\n# Compute BMI from our height and weight data (assumed to be in meters and kg. based off of the values)\nmldata$BMI &lt;- mldata$WT / (mldata$HT^2)\n\nWith all of our cleaning done, we can now proceed to fitting. We’ll do 3 models: a GLM with all predictors, a LASSO model, and a random forest model. We’ll use the tidymodels framework for all of these.\n\n# Set seed for reproducibility\nset.seed(rngseed)\n\n# Define the outcome and predictors\noutcome &lt;- \"Y\"\npredictors &lt;- setdiff(names(mldata), outcome)\n\n# Create a recipe\nrecipe &lt;- recipe(formula = Y ~ ., data = mldata) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_normalize(all_predictors())\n\n# Define the models\n# 1. Linear Model\nall_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# 2. LASSO Model\nlasso_model &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# 3. Random Forest Model\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n# Create workflows\nall_workflow &lt;- workflow() %&gt;%\n  add_model(all_model) %&gt;%\n  add_recipe(recipe)\n\nlasso_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model) %&gt;%\n  add_recipe(recipe)\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(recipe)\n\n# Fit the models\nall_fit &lt;- all_workflow %&gt;%\n  fit(data = mldata)\n\nlasso_fit &lt;- lasso_workflow %&gt;%\n  fit(data = mldata)\n\nrf_fit &lt;- rf_workflow %&gt;%\n  fit(data = mldata)\n\nNow that we’ve fit the models, we can begin to evaluate their performance. We’ll use RMSE as the metric and create some plots.\n\n# Make predictions\nall_preds &lt;- predict(all_fit, new_data = mldata)\nlasso_preds &lt;- predict(lasso_fit, new_data = mldata)\nrf_preds &lt;- predict(rf_fit, new_data = mldata)\n\n# Calculate RMSE\n#augment to evaluate performance metric\naug_all&lt;- augment(all_fit, mldata)\naug_all %&gt;% select(Y, .pred)\n\n# A tibble: 120 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 2691. 1666.\n 2 2639. 1951.\n 3 2150. 1896.\n 4 1789. 1548.\n 5 3126. 2369.\n 6 2337. 1921.\n 7 3007. 1510.\n 8 2796. 2156.\n 9 3866. 2658.\n10 1762. 1352.\n# ℹ 110 more rows\n\n#LASSO augment\naug_lasso&lt;- augment(lasso_fit, mldata)\naug_lasso %&gt;% select(Y, .pred)\n\n# A tibble: 120 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 2691. 1665.\n 2 2639. 1951.\n 3 2150. 1901.\n 4 1789. 1553.\n 5 3126. 2358.\n 6 2337. 1929.\n 7 3007. 1513.\n 8 2796. 2154.\n 9 3866. 2644.\n10 1762. 1345.\n# ℹ 110 more rows\n\n#Forest augment\naug_rf&lt;- augment(rf_fit, mldata)\naug_rf %&gt;% select(Y, .pred)\n\n# A tibble: 120 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 2691. 2229.\n 2 2639. 2379.\n 3 2150. 1934.\n 4 1789. 1818.\n 5 3126. 2740.\n 6 2337. 1977.\n 7 3007. 2277.\n 8 2796. 2518.\n 9 3866. 2959.\n10 1762. 1617.\n# ℹ 110 more rows\n\n#get RMSE of models\nall_rmse&lt;- aug_all %&gt;% rmse(truth = Y, .pred)\nlasso_rmse &lt;- aug_lasso %&gt;% rmse(truth = Y, .pred)\nrf_rmse &lt;- aug_rf %&gt;% rmse(truth = Y, .pred)\n\n# Print RMSE\nprint(paste(\"Linear Model RMSE: \", all_rmse))\n\n[1] \"Linear Model RMSE:  rmse\"            \n[2] \"Linear Model RMSE:  standard\"        \n[3] \"Linear Model RMSE:  571.595397430179\"\n\nprint(paste(\"LASSO Model RMSE: \", lasso_rmse))\n\n[1] \"LASSO Model RMSE:  rmse\"             \"LASSO Model RMSE:  standard\"        \n[3] \"LASSO Model RMSE:  571.650382196397\"\n\nprint(paste(\"Random Forest Model RMSE: \", rf_rmse))\n\n[1] \"Random Forest Model RMSE:  rmse\"            \n[2] \"Random Forest Model RMSE:  standard\"        \n[3] \"Random Forest Model RMSE:  381.596767379991\"\n\n# Create observed vs predicted plots\nggplot() +\n  geom_point(aes(x = mldata$Y, y = all_preds$.pred), color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  ggtitle(\"Linear Model: Observed vs Predicted\") +\n  xlab(\"Observed\") +\n  ylab(\"Predicted\")\n\n\n\n\n\n\n\nggplot() +\n  geom_point(aes(x = mldata$Y, y = lasso_preds$.pred), color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  ggtitle(\"LASSO Model: Observed vs Predicted\") +\n  xlab(\"Observed\") +\n  ylab(\"Predicted\")\n\n\n\n\n\n\n\nggplot() +\n  geom_point(aes(x = mldata$Y, y = rf_preds$.pred), color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  ggtitle(\"Random Forest Model: Observed vs Predicted\") +\n  xlab(\"Observed\") +\n  ylab(\"Predicted\")\n\n\n\n\n\n\n\n\nWe can see that the Random Forest model performs a lot better on the RMSE metric than the other two models. The prediction points are also closer to the line on the observed vs predicted plots, indicating a better fit overall. The linear and LASSO models are very similar to each other; this is likely because we set the tuning paramter so low for our LASSO, resulting in very little change from a typical linear model.\nWe’ll now practice tuning our complex models (LASSO and RF). We’ll start with LASSO. Note that this tuning is being done with the data used to train the model; this is a poor choice in a real analysis and is only being done now for practice purposes.\n\n# Define the grid of parameters\npenalty_values &lt;- 10^seq(-5, 2, length.out = 50)\npenalty_grid &lt;- tibble(penalty = penalty_values)\n\n# Update the LASSO model specification to include the penalty parameter\nlasso_model &lt;- linear_reg(penalty = tune(), mixture = 1) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Update the LASSO workflow to include the updated model specification\nlasso_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model) %&gt;%\n  add_recipe(recipe)\n\n# Create resamples using the apparent() function\nresamples &lt;- apparent(mldata)\n\n# Tune the LASSO model\ntune_results &lt;- tune_grid(\n  lasso_workflow,\n  resamples = resamples,\n  grid = penalty_grid\n)\n\n# Print the tuning results\nprint(tune_results)\n\n# Tuning results\n# Apparent sampling \n# A tibble: 1 × 4\n  splits            id       .metrics           .notes          \n  &lt;list&gt;            &lt;chr&gt;    &lt;list&gt;             &lt;list&gt;          \n1 &lt;split [120/120]&gt; Apparent &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n\n# Plot the tuning results\nautoplot(tune_results)\n\n\n\n\n\n\n\n\nWe can see that as our penalty parameter increases, the RMSE in our LASSO model also increases. Low penalty values hover at the same RMSE as our linear model; this could indicate that we have no overfitting in our model or that all predictors are relevant. However, we know that we’re tuning our model with the same data we used to fit it, so these results are misleading since we aren’t actually able to test our model with new data. Now we’ll tune our RF model.\n\n# Update the Random Forest model specification to include the mtry and min_n parameters\nrf_model &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 300) %&gt;%\n  set_engine(\"ranger\", seed = rngseed) %&gt;%\n  set_mode(\"regression\")\n\n# Update the Random Forest workflow to include the updated model specification\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(recipe)\n\n# Define the grid of parameters\nmtry_param &lt;- mtry(range = c(1, 7))\nmin_n_param &lt;- min_n(range = c(1, 21))\nrf_grid &lt;- grid_regular(mtry_param, min_n_param, levels = 7)\n\n# Tune the Random Forest model\ntune_results &lt;- tune_grid(\n  rf_workflow,\n  resamples = resamples,\n  grid = rf_grid\n)\n\n# Print the tuning results\nprint(tune_results)\n\n# Tuning results\n# Apparent sampling \n# A tibble: 1 × 4\n  splits            id       .metrics          .notes          \n  &lt;list&gt;            &lt;chr&gt;    &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [120/120]&gt; Apparent &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n# Plot the tuning results\nautoplot(tune_results)\n\n\n\n\n\n\n\n\nIt seems that a higher value of mtry and a lower value of min_n lead to the best results. We’ll move on for now to using CV then actually tuning the data to “new” observations.\n\n#set the seed again\nset.seed(rngseed)\n\n# Create real resamples using 5-fold cross-validation, 5 times repeated\nresamples &lt;- vfold_cv(mldata, v = 5, repeats = 5)\n\n# Define the grid of parameters for LASSO\npenalty_values &lt;- 10^seq(-5, 2, length.out = 50)\npenalty_grid &lt;- tibble(penalty = penalty_values)\n\n# Tune the LASSO model\ntune_results_lasso &lt;- tune_grid(\n  lasso_workflow,\n  resamples = resamples,\n  grid = penalty_grid\n)\n\n# Print the tuning results for LASSO\nprint(tune_results_lasso)\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times \n# A tibble: 25 × 5\n   splits          id      id2   .metrics           .notes          \n   &lt;list&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;list&gt;             &lt;list&gt;          \n 1 &lt;split [96/24]&gt; Repeat1 Fold1 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [96/24]&gt; Repeat1 Fold2 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [96/24]&gt; Repeat1 Fold3 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [96/24]&gt; Repeat1 Fold4 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [96/24]&gt; Repeat1 Fold5 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [96/24]&gt; Repeat2 Fold1 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [96/24]&gt; Repeat2 Fold2 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [96/24]&gt; Repeat2 Fold3 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [96/24]&gt; Repeat2 Fold4 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [96/24]&gt; Repeat2 Fold5 &lt;tibble [100 × 5]&gt; &lt;tibble [0 × 3]&gt;\n# ℹ 15 more rows\n\n# Plot the tuning results for LASSO\nautoplot(tune_results_lasso)\n\n\n\n\n\n\n\n# Define the grid of parameters for Random Forest\nrf_param_grid &lt;- grid_regular(\n  mtry(range = c(1, 7)),\n  min_n(range = c(1, 21)),\n  levels = 7\n)\n\n# Tune the Random Forest model\ntune_results_rf &lt;- tune_grid(\n  rf_workflow,\n  resamples = resamples,\n  grid = rf_param_grid\n)\n\n# Print the tuning results for Random Forest\nprint(tune_results_rf)\n\n# Tuning results\n# 5-fold cross-validation repeated 5 times \n# A tibble: 25 × 5\n   splits          id      id2   .metrics          .notes          \n   &lt;list&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n 1 &lt;split [96/24]&gt; Repeat1 Fold1 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [96/24]&gt; Repeat1 Fold2 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [96/24]&gt; Repeat1 Fold3 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [96/24]&gt; Repeat1 Fold4 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [96/24]&gt; Repeat1 Fold5 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [96/24]&gt; Repeat2 Fold1 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [96/24]&gt; Repeat2 Fold2 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [96/24]&gt; Repeat2 Fold3 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [96/24]&gt; Repeat2 Fold4 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [96/24]&gt; Repeat2 Fold5 &lt;tibble [98 × 6]&gt; &lt;tibble [0 × 3]&gt;\n# ℹ 15 more rows\n\n# Plot the tuning results for Random Forest\nautoplot(tune_results_rf)\n\n\n\n\n\n\n\n\nThe LASSO still does best for a small penalty, the RMSE for both models went up, and the LASSO now has lower RMSE compared to the RF. CV provides a more robust estimate of model performance by averaging the performance across multiple folds. This often results in a higher RMSE compared to a single model fit on the entire dataset. This could explain why the RMSE for both models went up when using CV. When the penalty is small, LASSO includes more features in the model, making it more flexible and potentially leading to lower RMSE. This could explain why LASSO still does best for a small penalty. LASSO is a linear model, which may be less complex than a Random Forest depending on the number of features selected. If the true relationship is linear/nearly linear, LASSO may outperform a Random Forest, which could explain why LASSO now has a lower RMSE compared to the Random Forest. Based off of these findings, it would seem that the LASSO model explains more variation (higher R squared value) and has a lower RMSE, indicating better performance. I would select this as the better model for our study."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise, I’ll be using a dataset I got from the CDC website about smoking attributable expenses in the U.S. The data includes expenses per state for different categories such as hospital bills, ambulances, and prescriptions as well as overall expenses for the U.S. from 2005-2009. We’ll do an exploratory data analysis and some data processing for this exercise.\n\n#load the dataset and packages\nlibrary(dplyr)\nlibrary(here)\nlibrary(ggplot2)\nlibrary(car)\nsmoke_expense&lt;- read.csv(here(\"cdcdata-exercise\", \"SAE.csv\"))\n#make sure data fully loaded by checking number of rows and columns. We should have 19 variables and 1560 observations\nnrow(smoke_expense)\n\n[1] 1560\n\nncol(smoke_expense)\n\n[1] 19\n\n#check the structure and summary of the data. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #\nsummary(smoke_expense)\n\n      Year      LocationAbbr       LocationDesc        DataSource       \n Min.   :2005   Length:1560        Length:1560        Length:1560       \n 1st Qu.:2006   Class :character   Class :character   Class :character  \n Median :2007   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2007                                                           \n 3rd Qu.:2008                                                           \n Max.   :2009                                                           \n  TopicType          TopicDesc         MeasureDesc          Variable        \n Length:1560        Length:1560        Length:1560        Length:1560       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value      \n Length:1560        Length:1560        Min.   :     8.4  \n Class :character   Class :character   1st Qu.:   105.7  \n Mode  :character   Mode  :character   Median :   306.1  \n                                       Mean   :  1545.2  \n                                       3rd Qu.:   842.3  \n                                       Max.   :132459.8  \n Data_Value_Footnote_Symbol Data_Value_Footnote GeoLocation       \n Length:1560                Length:1560         Length:1560       \n Class :character           Class :character    Class :character  \n Mode  :character           Mode  :character    Mode  :character  \n                                                                  \n                                                                  \n                                                                  \n Topic.Type.ID        Topic.ID          Measure.ID        SubMeasureID      \n Length:1560        Length:1560        Length:1560        Length:1560       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder\n Min.   :1.0  \n 1st Qu.:2.0  \n Median :3.5  \n Mean   :3.5  \n 3rd Qu.:5.0  \n Max.   :6.0  \n\n#structure(smoke_expense)\n\nWe can see that there’s one observation for each of our 6 expense types per year. We can also see from the structure and summary that there are many columns that we don’t need for an analysis. We can go ahead and pick out the ones that are of interest to us: year, location, variable (which refers to the type of expense), and value (which is the cost in millions of dollars for each expense type). All of the other columns seem to be for record keeping purposes.\n\n#make a new data frame containing only our four columns of interest: Year, LocationAbbr, Variable, and Data_Value.\nsmoke_expense_2 &lt;- smoke_expense[, c(\"Year\", \"LocationAbbr\", \"Variable\", \"Data_Value\")]\n#check the new object to make sure it has everything we want. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #\n#structure(smoke_expense_2)\nsummary(smoke_expense_2)\n\n      Year      LocationAbbr         Variable           Data_Value      \n Min.   :2005   Length:1560        Length:1560        Min.   :     8.4  \n 1st Qu.:2006   Class :character   Class :character   1st Qu.:   105.7  \n Median :2007   Mode  :character   Mode  :character   Median :   306.1  \n Mean   :2007                                         Mean   :  1545.2  \n 3rd Qu.:2008                                         3rd Qu.:   842.3  \n Max.   :2009                                         Max.   :132459.8  \n\n\nThis new object is much more condensed and easier to work with. We’ll check now to make sure there are no NA values and then proceed with some EDA.\n\n#check for NA values\nna_check&lt;- is.na(smoke_expense_2)\nprint(sum(na_check))\n\n[1] 0\n\n\nSince there aren’t any NA values and looking at the structure indicates no missing values, we can begin to check the mean and standard deviation of each expense type across the 4 years in the data set. Rather than looking at all 50 states, let’s focus on 5 to make this a bit easier. We’ll use GA, TN, MS, CA, and FL.\nUsing Microsoft Copilot with GPT-4 in “Precise Mode”, I entered the following prompt to get the code I’m about to use: “For my exploratory analysis I want to summarize each variable in a way that can be described by a distribution. For instance, I want to be able to determine the mean and standard deviation of each expense type for 5 different states over the 4 year period recorded in the dataset. What is the best approach for this and could you provide some example code?”\n\n# Filter for the 5 states you are interested in\nstates &lt;- c(\"TN\", \"MS\", \"GA\", \"FL\", \"CA\")\nsmoke_filtered &lt;- smoke_expense_2 %&gt;% filter(LocationAbbr %in% states)\n\n# Calculate mean and standard deviation\nsmoke_summary &lt;- smoke_filtered %&gt;%\n  group_by(LocationAbbr, Year, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr', 'Year'. You can override\nusing the `.groups` argument.\n\n# Print the summary statistics\nprint(smoke_summary)\n\n# A tibble: 150 × 5\n# Groups:   LocationAbbr, Year [25]\n   LocationAbbr  Year Variable             Mean    SD\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n 1 CA            2005 Ambulatory          2057.    NA\n 2 CA            2005 Hospital            5309.    NA\n 3 CA            2005 Nursing Home         592.    NA\n 4 CA            2005 Other                620.    NA\n 5 CA            2005 Prescription Drugs  1883.    NA\n 6 CA            2005 Total              10460.    NA\n 7 CA            2006 Ambulatory          2122.    NA\n 8 CA            2006 Hospital            5652.    NA\n 9 CA            2006 Nursing Home         641.    NA\n10 CA            2006 Other                677.    NA\n# ℹ 140 more rows\n\n\nThis is good information, but since there’s only one observation for each expense category per year, we don’t learn much from a mean or standard deviation (SD) calculation. We can group each year together to get the mean and SD to allow for easier creation of synthetic data for the second part of this exercise.\n\n# Calculate mean and standard deviation for all 5 states combined\nsmoke_summary &lt;- smoke_filtered %&gt;%\n  group_by(LocationAbbr, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr'. You can override using the\n`.groups` argument.\n\n# Print the summary statistics\nprint(smoke_summary)\n\n# A tibble: 30 × 4\n# Groups:   LocationAbbr [5]\n   LocationAbbr Variable             Mean     SD\n   &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n 1 CA           Ambulatory          2294.  208. \n 2 CA           Hospital            6006.  571. \n 3 CA           Nursing Home         699.   87.9\n 4 CA           Other                725.   76.8\n 5 CA           Prescription Drugs  2172.  197. \n 6 CA           Total              11896. 1131. \n 7 FL           Ambulatory          1419.  105. \n 8 FL           Hospital            3532.  270. \n 9 FL           Nursing Home         595.   58.9\n10 FL           Other                514.   53.3\n# ℹ 20 more rows\n\n\nNow we can create some visualizations of this information for easier interpretation of the data. I entered the following prompt into Microsoft Copilot with GPT-4 in “Precise mode”: “I want to make plots now to determine if the distribution of this data is relatively normal. How would I go about that?”\nI received this code string from it, with the only modifications I made being the proper column names and adjusting the binwidth to the square root of all of the observations:\n\n# Create a histogram\nggplot(smoke_filtered, aes(x = Data_Value)) + \n  geom_histogram(binwidth = 12.25, fill = \"blue\", color = \"black\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Histogram of Expenses\", x = \"Expense\", y = \"Frequency\")\n\n\n\n\n\n\n\n# Create a density plot\nggplot(smoke_filtered, aes(x = Data_Value)) + \n  geom_density(fill = \"blue\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Density Plot of Expenses\", x = \"Expense\", y = \"Density\") \n\n\n\n\n\n\n\n# Create a Q-Q plot with the car package\nqqPlot(smoke_filtered$Data_Value, distribution = \"norm\", main = \"Q-Q Plot of Expenses\")\n\n\n\n\n\n\n\n\n[1] 131  45\n\n\nThe histogram isn’t very useful for much besides visualizing the numbers, but the Q-Q plot shows that the distribution for the data as a whole seems normal but contains some outliers to the left and the right. The density plots point out that “Other”, “Nursing Home”, and “Prescription Drugs”, and “Ambulatory” expenses are pretty skewed, but the other two are pretty normally distributed. Hopefully this is enough information to create a good synthetic data set that mimics the trends seen in this one!\n\nThis section was contributed by Emma Hardin-Parker\n\n# Load required packages\nlibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(gtsummary)\n\n\n# Set a seed for reproducibility\nset.seed(207)\n# Define the number of observations to generate\nn_obs &lt;- 150\n\nI am now going to get a feel for the data ussing skimr and gtsummary() functions.\n\nskimr::skim(smoke_filtered)\n\n\nData summary\n\n\nName\nsmoke_filtered\n\n\nNumber of rows\n150\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nLocationAbbr\n0\n1\n2\n2\n0\n5\n0\n\n\nVariable\n0\n1\n5\n18\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2007.0\n1.42\n2005.0\n2006.00\n2007.00\n2008.00\n2009.0\n▇▇▇▇▇\n\n\nData_Value\n0\n1\n1747.8\n2581.55\n50.6\n225.15\n660.15\n2073.02\n13292.4\n▇▁▁▁▁\n\n\n\n\n\n\ngtsummary::tbl_summary(smoke_filtered, statistic = list(\n  all_continuous() ~ \"{mean}/{median}/{min}/{max}/{sd}\",\n  all_categorical() ~ \"{n} / {N} ({p}%)\"\n),)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1501\n\n\n\n\nYear\n\n\n\n\n    2005\n30 / 150 (20%)\n\n\n    2006\n30 / 150 (20%)\n\n\n    2007\n30 / 150 (20%)\n\n\n    2008\n30 / 150 (20%)\n\n\n    2009\n30 / 150 (20%)\n\n\nLocationAbbr\n\n\n\n\n    CA\n30 / 150 (20%)\n\n\n    FL\n30 / 150 (20%)\n\n\n    GA\n30 / 150 (20%)\n\n\n    MS\n30 / 150 (20%)\n\n\n    TN\n30 / 150 (20%)\n\n\nVariable\n\n\n\n\n    Ambulatory\n25 / 150 (17%)\n\n\n    Hospital\n25 / 150 (17%)\n\n\n    Nursing Home\n25 / 150 (17%)\n\n\n    Other\n25 / 150 (17%)\n\n\n    Prescription Drugs\n25 / 150 (17%)\n\n\n    Total\n25 / 150 (17%)\n\n\nData_Value\n1,748/660/51/13,292/2,582\n\n\n\n1 n / N (%); Mean/Median/Minimum/Maximum/SD\n\n\n\n\n\n\n\n\nNow I am going to create a synthetic data set based off the actual data.\n\nsyn_smoke &lt;- data.frame(\n  Year = integer(n_obs),\n  LocationAbbr = character(n_obs),\n  Variable = character(n_obs),\n  Data_Value = numeric(n_obs)\n)\n\n\n#Variable1\nsyn_smoke$Year &lt;- sample(c(\"2005\", \"2006\", \"2007\", \"2008\", \"2009\"),\n                         n_obs, replace = TRUE,\n                         prob = as.integer(table(smoke_filtered$Year)))\n                   \n#Variable2 \nsyn_smoke$LocationAbbr &lt;- sample(c(\"GA\", \"MS\", \"TN\", \"FL\", \"CA\"),\n                                 n_obs, replace = TRUE,\n                                 prob =  as.numeric(table(smoke_filtered$LocationAbbr)/100))\n\n#Variable3\nsyn_smoke$Variable &lt;- sample(c(\"Ambulatory\", \"Hospital\", \"Nursing Home\", \"Other\", \"Prescription Drugs\", \"Total\"),\n                             n_obs,\n                             replace = TRUE,\n                             prob = as.numeric(table(smoke_filtered$Variable)/100))\n#Variable4\nsyn_smoke$Data_Value &lt;- round(runif(n_obs,\n                                    min = min(smoke_filtered$Data_Value),\n                                    max = max(smoke_filtered$Data_Value)), 1)\n\nTo make sure the synthetic data set was created properly, I am going to use the head(), glimpse(), and summary() functions to see if the first five rows look okay for further analyses.\n\nhead(syn_smoke)\n\n  Year LocationAbbr   Variable Data_Value\n1 2009           CA   Hospital       76.5\n2 2009           TN Ambulatory     2217.7\n3 2009           FL   Hospital      411.9\n4 2005           TN   Hospital     2993.4\n5 2008           CA Ambulatory      910.9\n6 2006           FL Ambulatory     7999.9\n\nsummary(syn_smoke)\n\n     Year           LocationAbbr         Variable           Data_Value     \n Length:150         Length:150         Length:150         Min.   :   76.5  \n Class :character   Class :character   Class :character   1st Qu.: 3172.8  \n Mode  :character   Mode  :character   Mode  :character   Median : 7116.2  \n                                                          Mean   : 6566.1  \n                                                          3rd Qu.: 9791.8  \n                                                          Max.   :13236.8  \n\nglimpse(syn_smoke)\n\nRows: 150\nColumns: 4\n$ Year         &lt;chr&gt; \"2009\", \"2009\", \"2009\", \"2005\", \"2008\", \"2006\", \"2006\", \"…\n$ LocationAbbr &lt;chr&gt; \"CA\", \"TN\", \"FL\", \"TN\", \"CA\", \"FL\", \"CA\", \"GA\", \"GA\", \"GA…\n$ Variable     &lt;chr&gt; \"Hospital\", \"Ambulatory\", \"Hospital\", \"Hospital\", \"Ambula…\n$ Data_Value   &lt;dbl&gt; 76.5, 2217.7, 411.9, 2993.4, 910.9, 7999.9, 1914.5, 9803.…\n\n\nEverything looks as it should, so it’s time to make some exploratory figures and tables.\n\n# Calculate mean and standard deviation for all 5 states combined\nsyn_smoke_summary &lt;- syn_smoke %&gt;%\n  group_by(LocationAbbr, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr'. You can override using the\n`.groups` argument.\n\n\n\n# Create a histogram\nsyn_hist &lt;- ggplot(syn_smoke, aes(x = Data_Value)) + \n  geom_histogram(binwidth = 12.25, fill = \"blue\", color = \"black\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Histogram of Expenses\", x = \"Expense\", y = \"Frequency\")\nprint(syn_hist)\n\n\n\n\n\n\n\n# Create a density plot\nlibrary(scales)\nsyn_dens &lt;- ggplot(syn_smoke, aes(x = Data_Value)) + \n  geom_density(fill = \"blue\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Density Plot of Expenses\", x = \"Expense\", y = \"Density\") +\n  scale_x_continuous(labels = label_number())\nprint(syn_dens)\n\n\n\n\n\n\n\n# Create a Q-Q plot\nlibrary(car)\nsyn_qq &lt;- qqPlot(syn_smoke$Data_Value, distribution = \"norm\", main = \"Q-Q Plot of Expenses\")\n\n\n\n\n\n\n\n\nThe histograms produced between the original data and the synthetic data were quite different. Most of the individual histograms per Variable created with the original data were skewed to the right, while the histograms produced with the synthetic data were evenly distributed throughout.\nThe density plots also look different between the original data and the synthetic data, however, I struggled to code this plot. For some reason, the density value on the y-axis used scientific notation and I had to add an additional line of code to remove it. Even with that transformation, the densities on average are higher in the synthetic plot than the original plot.\nThe q-q plot was undoubtedly the most interesting plot to compare to the original data. The synthetic q-q plot is significantly more normally distributed that the original plot. Though the histograms differed drastically, it is much easier to compare normality between the data sets using a q-q plot, so I am happy that I was able to successfully create this one."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "For this week’s exercise, we’ll be taking part in Tidy Tuesday, a data analysis initiative where a community of analysts picks a dataset to analyze and discuss weekly. This week’s exercise contains information from NASA on the path of annularity and path of totality for the solar eclipse in 2023 and 2024. The annular path describes areas that will be able to see a “ring of fire” around the moon as it mostly blocks the sun. The totality path describes areas that will experience a full blocking of the sun by the moon, causing it to become dark in said region. This data includes the cities that will be in each of these paths as well as the predicted times that the eclipse will start and end for each location. We’ll load the data now using a script published in the TidyTuesday Github repository.\n\n# Load the data and libraries \nlibrary(tidytuesdayR)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidymodels)\nlibrary(rsample)\nlibrary(parsnip)\nlibrary(tune)\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 15)\n\n\n    Downloading file 1 of 4: `eclipse_annular_2023.csv`\n    Downloading file 2 of 4: `eclipse_total_2024.csv`\n    Downloading file 3 of 4: `eclipse_partial_2023.csv`\n    Downloading file 4 of 4: `eclipse_partial_2024.csv`\n\neclipse_annular_2023 &lt;- tuesdata$eclipse_annular_2023\neclipse_total_2024 &lt;- tuesdata$eclipse_total_2024\neclipse_partial_2023 &lt;- tuesdata$eclipse_partial_2023\neclipse_partial_2024 &lt;- tuesdata$eclipse_partial_2024\n\nLet’s take a look at the data.\n\n# Look at a quick header for each data frame\nhead(eclipse_annular_2023)\n\n# A tibble: 6 × 10\n  state name         lat   lon eclipse_1 eclipse_2 eclipse_3 eclipse_4 eclipse_5\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;   \n1 AZ    Chilchinb…  36.5 -110. 15:10:50  15:56:20  16:30:29  16:33:31  17:09:40 \n2 AZ    Chinle      36.2 -110. 15:11:10  15:56:50  16:31:21  16:34:06  17:10:30 \n3 AZ    Del Muerto  36.2 -109. 15:11:20  15:57:00  16:31:13  16:34:31  17:10:40 \n4 AZ    Dennehotso  36.8 -110. 15:10:50  15:56:20  16:29:50  16:34:07  17:09:40 \n5 AZ    Fort Defi…  35.7 -109. 15:11:40  15:57:40  16:32:28  16:34:35  17:11:30 \n6 AZ    Kayenta     36.7 -110. 15:10:40  15:56:00  16:29:54  16:33:21  17:09:10 \n# ℹ 1 more variable: eclipse_6 &lt;time&gt;\n\nhead(eclipse_partial_2023)\n\n# A tibble: 6 × 9\n  state name         lat   lon eclipse_1 eclipse_2 eclipse_3 eclipse_4 eclipse_5\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;   \n1 AL    Abanda      33.1 -85.5 15:41:20  16:23:30  17:11:10  18:00:00  18:45:10 \n2 AL    Abbeville   31.6 -85.3 15:42:30  16:25:50  17:13:50  18:03:10  18:49:30 \n3 AL    Adamsville  33.6 -87.0 15:38:20  16:20:50  17:07:50  17:56:30  18:42:10 \n4 AL    Addison     34.2 -87.2 15:37:50  16:19:50  17:06:50  17:55:10  18:40:30 \n5 AL    Akron       32.9 -87.7 15:37:20  16:20:40  17:07:30  17:56:00  18:42:50 \n6 AL    Alabaster   33.2 -86.8 15:38:50  16:21:30  17:08:40  17:57:20  18:43:20 \n\nhead(eclipse_partial_2024)\n\n# A tibble: 6 × 9\n  state name         lat   lon eclipse_1 eclipse_2 eclipse_3 eclipse_4 eclipse_5\n  &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;   \n1 AL    Abanda      33.1 -85.5 17:43:00  18:24:10  19:02:00  19:39:20  20:18:50 \n2 AL    Abbeville   31.6 -85.3 17:41:40  18:21:40  19:00:30  19:38:50  20:17:20 \n3 AL    Adamsville  33.6 -87.0 17:41:00  18:23:10  19:00:00  19:36:40  20:17:30 \n4 AL    Addison     34.2 -87.2 17:41:30  18:24:10  19:00:30  19:36:40  20:18:00 \n5 AL    Akron       32.9 -87.7 17:38:40  18:20:40  18:58:00  19:35:00  20:15:50 \n6 AL    Alabaster   33.2 -86.8 17:40:40  18:22:40  18:59:50  19:36:50  20:17:20 \n\nhead(eclipse_total_2024)\n\n# A tibble: 6 × 10\n  state name        lat   lon eclipse_1 eclipse_2 eclipse_3 eclipse_4 eclipse_5\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;    &lt;time&gt;   \n1 AR    Acorn      34.6 -94.2 17:30:40  18:15:50  18:47:35  18:51:37  19:23:40 \n2 AR    Adona      35.0 -92.9 17:33:20  18:18:30  18:50:08  18:54:22  19:26:10 \n3 AR    Alexander  34.6 -92.5 17:33:20  18:18:30  18:51:09  18:53:38  19:26:20 \n4 AR    Alicia     35.9 -91.1 17:37:30  18:22:40  18:54:29  18:58:05  19:29:50 \n5 AR    Alix       35.4 -93.7 17:32:50  18:17:50  18:49:54  18:53:00  19:25:20 \n6 AR    Alleene    33.8 -94.3 17:29:10  18:14:20  18:46:15  18:50:16  19:22:30 \n# ℹ 1 more variable: eclipse_6 &lt;time&gt;\n\n# Check summary of each frame\nsummary(eclipse_annular_2023)\n\n    state               name                lat             lon         \n Length:811         Length:811         Min.   :27.22   Min.   :-124.45  \n Class :character   Class :character   1st Qu.:31.30   1st Qu.:-111.98  \n Mode  :character   Mode  :character   Median :35.42   Median :-106.70  \n                                       Mean   :35.41   Mean   :-108.05  \n                                       3rd Qu.:38.42   3rd Qu.:-101.36  \n                                       Max.   :44.87   Max.   : -96.72  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:811        Length:811        Length:811        Length:811       \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5         eclipse_6       \n Length:811        Length:811       \n Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric   \n                                    \n                                    \n\nsummary(eclipse_partial_2023)\n\n    state               name                lat             lon         \n Length:31363       Length:31363       Min.   :17.96   Min.   :-176.60  \n Class :character   Class :character   1st Qu.:35.36   1st Qu.: -97.50  \n Mode  :character   Mode  :character   Median :39.56   Median : -89.26  \n                                       Mean   :38.80   Mean   : -91.97  \n                                       3rd Qu.:41.93   3rd Qu.: -81.14  \n                                       Max.   :71.25   Max.   : 174.11  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:31363      Length:31363      Length:31363      Length:31363     \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5       \n Length:31363     \n Class1:hms       \n Class2:difftime  \n Mode  :numeric   \n                  \n                  \n\nsummary(eclipse_partial_2024)\n\n    state               name                lat             lon         \n Length:28844       Length:28844       Min.   :17.96   Min.   :-176.60  \n Class :character   Class :character   1st Qu.:35.24   1st Qu.: -99.08  \n Mode  :character   Mode  :character   Median :39.52   Median : -90.30  \n                                       Mean   :38.76   Mean   : -93.00  \n                                       3rd Qu.:42.04   3rd Qu.: -81.16  \n                                       Max.   :71.25   Max.   : 174.11  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:28844      Length:28844      Length:28844      Length:28844     \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5       \n Length:28844     \n Class1:hms       \n Class2:difftime  \n Mode  :numeric   \n                  \n                  \n\nsummary(eclipse_total_2024)\n\n    state               name                lat             lon         \n Length:3330        Length:3330        Min.   :28.45   Min.   :-101.16  \n Class :character   Class :character   1st Qu.:35.42   1st Qu.: -92.41  \n Mode  :character   Mode  :character   Median :39.24   Median : -86.56  \n                                       Mean   :38.33   Mean   : -86.93  \n                                       3rd Qu.:41.22   3rd Qu.: -82.31  \n                                       Max.   :46.91   Max.   : -67.43  \n  eclipse_1         eclipse_2         eclipse_3         eclipse_4       \n Length:3330       Length:3330       Length:3330       Length:3330      \n Class1:hms        Class1:hms        Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime   Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric    Mode  :numeric    Mode  :numeric   \n                                                                        \n                                                                        \n  eclipse_5         eclipse_6       \n Length:3330       Length:3330      \n Class1:hms        Class1:hms       \n Class2:difftime   Class2:difftime  \n Mode  :numeric    Mode  :numeric   \n                                    \n                                    \n\n# Check for missing values in annular 2023\nmissing_2023 &lt;- sum(is.na(eclipse_annular_2023))\nprint(paste(\"Number of missing values in df_2023: \", missing_2023))\n\n[1] \"Number of missing values in df_2023:  0\"\n\n# Check for missing values in total 2024\nmissing_2024 &lt;- sum(is.na(eclipse_total_2024))\nprint(paste(\"Number of missing values in df_2024: \", missing_2024))\n\n[1] \"Number of missing values in df_2024:  0\"\n\n\nThe data seems to be free of missing values. For definitions of each variable, see the TidyTuesday repository and the data for 04/9/2024.\nOur data seems to have loaded in properly based off of the definitions given by the TidyTuesday group and our quick summaries of the objects we made. It may be useful to go into this with an idea of what questions we want to ask, so we can think of some ideas here. These can always be adjusted as we explore the data.\nHere are my current ideas:\nDo cities differ in their eclipse starting times (eclipse_1) between the datasets we’re given?\nBased off of the picture included in the TidyTuesday repository, San Antonio experienced both the annual and total eclipse these past two years. Are there differences between the start and end times of each eclipse in this city?\nWhich places have the longest duration of totality this year?\nWe can begin making some plots to answer these questions. We’ll start by looking at the start times for the total and annular eclipses. However, we can see that the timestamp data is given in a format that R may find difficult to understand. We’ll start by converting the time data in each dataframe to a format R can use.\n\n#Make a plot showing the different start times for the annular (2023) eclipse\n\n# Convert time column to a POSIXct object, a date-time class that R can understand\neclipse_annular_2023$eclipse_1 &lt;- as.POSIXct(eclipse_annular_2023$eclipse_1, format=\"%H:%M:%S\")\neclipse_annular_2023$eclipse_2 &lt;- as.POSIXct(eclipse_annular_2023$eclipse_2, format=\"%H:%M:%S\")\neclipse_annular_2023$eclipse_3 &lt;- as.POSIXct(eclipse_annular_2023$eclipse_3, format=\"%H:%M:%S\")\neclipse_annular_2023$eclipse_4 &lt;- as.POSIXct(eclipse_annular_2023$eclipse_4, format=\"%H:%M:%S\")\neclipse_annular_2023$eclipse_5 &lt;- as.POSIXct(eclipse_annular_2023$eclipse_5, format=\"%H:%M:%S\")\neclipse_annular_2023$eclipse_6 &lt;- as.POSIXct(eclipse_annular_2023$eclipse_6, format=\"%H:%M:%S\")\n\n# Create the plot\nEDAannular &lt;- ggplot(eclipse_annular_2023, aes(x=eclipse_3, y=name)) +\n  geom_point() +\n  labs(x=\"Time of Annularity Start\", y=\"City\", title=\"Annularity Start Times in Different Cities\") +\n  theme_minimal()\n\n# Print the plot\nprint(EDAannular)\n\n\n\n\n\n\n\n\nThat’s a bit messy.. maybe we’ll start with the states instead given that we have about 800 cities to look at here.\n\n# Create the plot\nEDAannular2 &lt;- ggplot(eclipse_annular_2023, aes(x=eclipse_3, y=state)) +\n  geom_point() +\n  labs(x=\"Time of Annularity Start\", y=\"State\", title=\"Annularity Start Times in Different States\") +\n  theme_minimal()\n\n# Print the plot\nprint(EDAannular2)\n\n\n\n\n\n\n\n\nWe can see that 8 states in the western US experienced the complete annular eclipse between 4:15 and 5 pm. Now let’s see how this compares to our total eclipse data for 2024. We’ll take the same steps for timestamps as we did previously.\n\n# Convert time column to a POSIXct object, a date-time class that R can understand\neclipse_total_2024$eclipse_1 &lt;- as.POSIXct(eclipse_total_2024$eclipse_1, format=\"%H:%M:%S\")\neclipse_total_2024$eclipse_2 &lt;- as.POSIXct(eclipse_total_2024$eclipse_2, format=\"%H:%M:%S\")\neclipse_total_2024$eclipse_3 &lt;- as.POSIXct(eclipse_total_2024$eclipse_3, format=\"%H:%M:%S\")\neclipse_total_2024$eclipse_4 &lt;- as.POSIXct(eclipse_total_2024$eclipse_4, format=\"%H:%M:%S\")\neclipse_total_2024$eclipse_5 &lt;- as.POSIXct(eclipse_total_2024$eclipse_5, format=\"%H:%M:%S\")\neclipse_total_2024$eclipse_6 &lt;- as.POSIXct(eclipse_total_2024$eclipse_6, format=\"%H:%M:%S\")\n\n# Create the plot\nEDAtotal &lt;- ggplot(eclipse_total_2024, aes(x=eclipse_3, y=state)) +\n  geom_point() +\n  labs(x=\"Time of Totality Start\", y=\"State\", title=\"Totality Start Times in Different States\") +\n  theme_minimal()\n\n# Print the plot\nprint(EDAtotal)\n\n\n\n\n\n\n\n\nWe can see that for this eclipse, there are 14 states that experienced complete totality. The start times ranged from 6:30-7:30 pm apparently. There’s a mixture of eastern and western states for this one. Now we can investigate when eclipses for each year ended.\n\n# Create the plot\nEDAannular3 &lt;- ggplot(eclipse_annular_2023, aes(x=eclipse_4, y=state)) +\n  geom_point() +\n  labs(x=\"Time of Annularity End\", y=\"State\", title=\"Annularity End Times in Different States\") +\n  theme_minimal()\n\n# Print the plot\nprint(EDAannular3)\n\n\n\n\n\n\n\n# Create the plot\nEDAtotal2 &lt;- ggplot(eclipse_total_2024, aes(x=eclipse_4, y=state)) +\n  geom_point() +\n  labs(x=\"Time of Totality End\", y=\"State\", title=\"Totality End Times in Different States\") +\n  theme_minimal()\n\n# Print the plot\nprint(EDAtotal2)\n\n\n\n\n\n\n\n\nBoth eclipses appear to have been very short in duration as the start and end times don’t differ too much; however, we can see that the annular eclipse end ranged from around 4:20 to slightly after 5 pm. The total eclipse ended between around 6:35 to shortly after 7:30. Texas had the longest eclipse duration of each state; this is likely due to how large it is.\nNow I think it would be interesting to compare the cities that experienced that total and annual eclipse and see if any of them got both. Based off of the map included in the TIdyTuesday repository, we expect to see San Antonio on this list.\n\n# Merge the two data frames by city and state\neclipse_both &lt;- inner_join(eclipse_annular_2023, eclipse_total_2024, by=c(\"name\", \"state\"), suffix=c(\"_2023\", \"_2024\"))\n\n# Check the new dataframe to see what states/cities got both eclipses\nglimpse(eclipse_both)\n\nRows: 56\nColumns: 18\n$ state          &lt;chr&gt; \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"TX\", \"…\n$ name           &lt;chr&gt; \"Balcones Heights\", \"Bandera\", \"Barksdale\", \"Batesville…\n$ lat_2023       &lt;dbl&gt; 29.49022, 29.72513, 29.73290, 28.95242, 28.56988, 29.78…\n$ lon_2023       &lt;dbl&gt; -98.54914, -99.07426, -100.03283, -99.62931, -99.57018,…\n$ eclipse_1_2023 &lt;dttm&gt; 1970-01-01 15:24:00, 1970-01-01 15:23:20, 1970-01-01 1…\n$ eclipse_2_2023 &lt;dttm&gt; 1970-01-01 16:14:30, 1970-01-01 16:13:40, 1970-01-01 1…\n$ eclipse_3_2023 &lt;dttm&gt; 1970-01-01 16:52:03, 1970-01-01 16:50:40, 1970-01-01 1…\n$ eclipse_4_2023 &lt;dttm&gt; 1970-01-01 16:56:07, 1970-01-01 16:55:15, 1970-01-01 1…\n$ eclipse_5_2023 &lt;dttm&gt; 1970-01-01 17:35:30, 1970-01-01 17:34:10, 1970-01-01 1…\n$ eclipse_6_2023 &lt;dttm&gt; 1970-01-01 18:32:40, 1970-01-01 18:31:20, 1970-01-01 1…\n$ lat_2024       &lt;dbl&gt; 29.49022, 29.72513, 29.73290, 28.95242, 28.56988, 29.78…\n$ lon_2024       &lt;dbl&gt; -98.54914, -99.07426, -100.03283, -99.62931, -99.57018,…\n$ eclipse_1_2024 &lt;dttm&gt; 1970-01-01 17:14:50, 1970-01-01 17:14:30, 1970-01-01 1…\n$ eclipse_2_2024 &lt;dttm&gt; 1970-01-01 18:00:00, 1970-01-01 17:59:30, 1970-01-01 1…\n$ eclipse_3_2024 &lt;dttm&gt; 1970-01-01 18:33:44, 1970-01-01 18:31:49, 1970-01-01 1…\n$ eclipse_4_2024 &lt;dttm&gt; 1970-01-01 18:34:46, 1970-01-01 18:35:56, 1970-01-01 1…\n$ eclipse_5_2024 &lt;dttm&gt; 1970-01-01 19:09:10, 1970-01-01 19:08:40, 1970-01-01 1…\n$ eclipse_6_2024 &lt;dttm&gt; 1970-01-01 19:55:40, 1970-01-01 19:55:10, 1970-01-01 1…\n\n# Create the plot\nboth_eclipse_cities &lt;- ggplot(eclipse_both) +\n  geom_point(aes(x=eclipse_3_2023, y=name), color=\"blue\", alpha=0.5) +\n  geom_point(aes(x=eclipse_3_2024, y=name), color=\"red\", alpha=0.5) +\n  labs(x=\"Time of Eclipse Start\", y=\"City\", title=\"Eclipse Start Times in Different Cities\") +\n  theme_minimal()\n\n# Print the plot\nprint(both_eclipse_cities)\n\n\n\n\n\n\n\n\nLooking at our new dataframe with only cities that experienced both eclipses, we can see that Texas cities are the only ones included. This means that our guesses from the TidyTuesday repository were correct; Texas is the only state with cities that experienced both eclipses. The plot shows the differences in the start times between the two eclipses based on city (red is total and blue is annular); there are a lot of cities, so it’s difficult to read the y-axis, but looking at the object we can see that 56 unique cities are included.\nWe also have data on cities that experienced a partial eclipse in either year. There are 31363 cities with partial eclipses in 2023 and 28844 for 2024. We could see if there are any recurring cities on this list now.\n\n# Convert time column to a POSIXct object, a date-time class that R can understand. 2023 first, then 2024\neclipse_partial_2023$eclipse_1 &lt;- as.POSIXct(eclipse_partial_2023$eclipse_1, format=\"%H:%M:%S\")\neclipse_partial_2023$eclipse_2 &lt;- as.POSIXct(eclipse_partial_2023$eclipse_2, format=\"%H:%M:%S\")\neclipse_partial_2023$eclipse_3 &lt;- as.POSIXct(eclipse_partial_2023$eclipse_3, format=\"%H:%M:%S\")\neclipse_partial_2023$eclipse_4 &lt;- as.POSIXct(eclipse_partial_2023$eclipse_4, format=\"%H:%M:%S\")\neclipse_partial_2023$eclipse_5 &lt;- as.POSIXct(eclipse_partial_2023$eclipse_5, format=\"%H:%M:%S\")\n\neclipse_partial_2024$eclipse_1 &lt;- as.POSIXct(eclipse_partial_2024$eclipse_1, format=\"%H:%M:%S\")\neclipse_partial_2024$eclipse_2 &lt;- as.POSIXct(eclipse_partial_2024$eclipse_2, format=\"%H:%M:%S\")\neclipse_partial_2024$eclipse_3 &lt;- as.POSIXct(eclipse_partial_2024$eclipse_3, format=\"%H:%M:%S\")\neclipse_partial_2024$eclipse_4 &lt;- as.POSIXct(eclipse_partial_2024$eclipse_4, format=\"%H:%M:%S\")\neclipse_partial_2024$eclipse_5 &lt;- as.POSIXct(eclipse_partial_2024$eclipse_5, format=\"%H:%M:%S\")\n\n# Merge the two data frames by city and state\npartial_both &lt;- inner_join(eclipse_partial_2023, eclipse_partial_2024, by=c(\"name\", \"state\"), suffix=c(\"_2023\", \"_2024\"))\n\nWarning in inner_join(eclipse_partial_2023, eclipse_partial_2024, by = c(\"name\", : Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 370 of `x` matches multiple rows in `y`.\nℹ Row 370 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n# Check the new dataframe to see what states/cities were partial both years\nglimpse(partial_both)\n\nRows: 28,509\nColumns: 16\n$ state          &lt;chr&gt; \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"AL\", \"…\n$ name           &lt;chr&gt; \"Abanda\", \"Abbeville\", \"Adamsville\", \"Addison\", \"Akron\"…\n$ lat_2023       &lt;dbl&gt; 33.09163, 31.56471, 33.60231, 34.20268, 32.87907, 33.21…\n$ lon_2023       &lt;dbl&gt; -85.52703, -85.25912, -86.97153, -87.17800, -87.74090, …\n$ eclipse_1_2023 &lt;dttm&gt; 1970-01-01 15:41:20, 1970-01-01 15:42:30, 1970-01-01 1…\n$ eclipse_2_2023 &lt;dttm&gt; 1970-01-01 16:23:30, 1970-01-01 16:25:50, 1970-01-01 1…\n$ eclipse_3_2023 &lt;dttm&gt; 1970-01-01 17:11:10, 1970-01-01 17:13:50, 1970-01-01 1…\n$ eclipse_4_2023 &lt;dttm&gt; 1970-01-01 18:00:00, 1970-01-01 18:03:10, 1970-01-01 1…\n$ eclipse_5_2023 &lt;dttm&gt; 1970-01-01 18:45:10, 1970-01-01 18:49:30, 1970-01-01 1…\n$ lat_2024       &lt;dbl&gt; 33.09163, 31.56471, 33.60231, 34.20268, 32.87907, 33.21…\n$ lon_2024       &lt;dbl&gt; -85.52703, -85.25912, -86.97153, -87.17800, -87.74090, …\n$ eclipse_1_2024 &lt;dttm&gt; 1970-01-01 17:43:00, 1970-01-01 17:41:40, 1970-01-01 1…\n$ eclipse_2_2024 &lt;dttm&gt; 1970-01-01 18:24:10, 1970-01-01 18:21:40, 1970-01-01 1…\n$ eclipse_3_2024 &lt;dttm&gt; 1970-01-01 19:02:00, 1970-01-01 19:00:30, 1970-01-01 1…\n$ eclipse_4_2024 &lt;dttm&gt; 1970-01-01 19:39:20, 1970-01-01 19:38:50, 1970-01-01 1…\n$ eclipse_5_2024 &lt;dttm&gt; 1970-01-01 20:18:50, 1970-01-01 20:17:20, 1970-01-01 2…\n\n# Create the plot\npartial_eclipse_cities &lt;- ggplot(partial_both) +\n  geom_point(aes(x=eclipse_3_2023, y=name), color=\"blue\", alpha=0.5) +\n  geom_point(aes(x=eclipse_3_2024, y=name), color=\"red\", alpha=0.5) +\n  labs(x=\"Time of Eclipse Start\", y=\"City\", title=\"Eclipse Start Times in Different Cities\") +\n  theme_minimal()\n\n# Print the plot\nprint(partial_eclipse_cities)\n\n\n\n\n\n\n\n\nWe have a huge number of cities (285009) that experienced partial eclipses in both years. This data seems difficult to deal with and is less interesting than the total/annular eclipse regions, so we’ll shift our focus away from these sets for the time being. We can now try to answer the final question I posed earlier: which cities had the longest duration of annularity and which will have the longest duration of totality? We’ll need to create two new columns in our dataframe that contain the total duration of each eclipse to do this.\n\n# Create two new columns in the dataframe with cities that experienced both eclipses\neclipse_both$duration_2023 &lt;- eclipse_both$eclipse_4_2023 - eclipse_both$eclipse_3_2023\neclipse_both$duration_2024 &lt;- eclipse_both$eclipse_4_2024 - eclipse_both$eclipse_3_2024\n\n# Plot for 2023\nggplot(eclipse_both, aes(x=name, y=duration_2023)) +\n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"City\", y = \"Duration of 2023 Eclipse\", title = \"Comparison of 2023 Eclipse Durations\")\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n# Plot for 2024\nggplot(eclipse_both, aes(x=name, y=duration_2024)) +\n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(x = \"City\", y = \"Duration of 2024 Eclipse\", title = \"Comparison of 2024 Eclipse Durations\")\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\n\n\n\n\nThe duration values on the y axis are in seconds. We can see that the total eclipse seemed to be shorter overall and that cities who had a longer annular eclipse didn’t always have a long total one. This graph seems to point to an interesting question we could ask: is there a correlation between city and eclipse duration in the data from these two years, or does it seem to be random which cities experience longer eclipses? Based off of the graphs we see here, I hypothesize that there’s little to no correlation between eclipse duration and specific cities.\nFrom our EDA so far, we can see that Texas is the only state with cities that experience both eclipses, so it will be interesting to see if a model accurately predicts eclipse duration for them. Before we fit any models, we first need to calculate the durations of the partial eclipses and merge each frame into a new object for analysis.\n\n# Create new columns in each uncombined dataframe with the duration of each eclipse\neclipse_annular_2023$duration &lt;- eclipse_annular_2023$eclipse_4 - eclipse_annular_2023$eclipse_3\neclipse_total_2024$duration &lt;- eclipse_total_2024$eclipse_4 - eclipse_total_2024$eclipse_3\neclipse_partial_2023$duration&lt;- eclipse_partial_2023$eclipse_4 - \neclipse_partial_2023$eclipse_3\neclipse_partial_2024$duration&lt;- eclipse_partial_2024$eclipse_4 - \neclipse_partial_2024$eclipse_3\n\n#Create copies of each dataframe that remove the columns we don't need for our analysis. This will make it possible to merge them into one dataframe after. We'll also add a new columns that allow us to differentiate between which set the observations came from upon merging them.\n\neclipse_total_2024_duration &lt;- eclipse_total_2024 %&gt;% select(name, state, duration) %&gt;% mutate(year = 2024, eclipse_type = \"total\")\neclipse_annular_2023_duration &lt;- eclipse_annular_2023 %&gt;% select(name, state, duration) %&gt;% mutate(year = 2023, eclipse_type = \"annular\")\neclipse_partial_2023_duration &lt;- eclipse_partial_2023 %&gt;% select(name, state, duration) %&gt;% mutate(year = 2023, eclipse_type = \"partial\")\neclipse_partial_2024_duration &lt;- eclipse_partial_2024 %&gt;% select(name, state, duration) %&gt;% mutate(year = 2024, eclipse_type = \"partial\")\n\n# Combine dataframes\nduration_combined &lt;- bind_rows(eclipse_total_2024_duration, eclipse_annular_2023_duration, eclipse_partial_2023_duration, eclipse_partial_2024_duration)\n\n#Make the duration variable numeric so it works with our model fitting functions\nduration_combined$duration &lt;- as.numeric(duration_combined$duration)\n\nNow we have a cleaned and merged dataset with new variables that make an analysis easy. Let’s pick a new question for which we can create models to answer now that we’ve done an EDA. I’m thinking that it would be interesting to see if there’s a correlation between eclipse duration and the type of eclipse. I think that we will observe similar durations for each type.\nWe can now begin to fit some models to the cleaned data. We’ll use train/test splits and CV wherever applicable. I consulted Microsoft Copilot in Precise Mode for advice and determined that a Linear Regression, Decision Tree model, and Random Forest model would work well for my data since I have a combination of numerical and categorical variables I want to analyze. I used Microsoft Copilot to generate the basic code and modified it according to my specifics.\n\n# Convert the type of eclipse to a factor\nduration_combined$eclipse_type &lt;- as.factor(duration_combined$eclipse_type)\n\n# Split the data into training and testing sets\nset.seed(123)\ndata_split &lt;- initial_split(duration_combined, prop = 0.75)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Cross-validation (5-fold)\ncv &lt;- vfold_cv(train_data, v = 5)\n\n# Model 1: Linear Regression\nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Model 2: Decision Tree\nmodel2 &lt;- decision_tree() %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"regression\")\n\n# Model 3: Random Forest\nmodel3 &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# Define the workflow\nworkflow1 &lt;- workflow() %&gt;%\n  add_model(model1) %&gt;%\n  add_formula(duration ~ eclipse_type)\n\nworkflow2 &lt;- workflow() %&gt;%\n  add_model(model2) %&gt;%\n  add_formula(duration ~ eclipse_type)\n\nworkflow3 &lt;- workflow() %&gt;%\n  add_model(model3) %&gt;%\n  add_formula(duration ~ eclipse_type)\n\n# Fit the models\nfit1 &lt;- fit(workflow1, data = train_data)\nfit2 &lt;- fit(workflow2, data = train_data)\nfit3 &lt;- fit(workflow3, data = train_data)\n\n# Print the results\nfit1\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nduration ~ eclipse_type\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n        (Intercept)  eclipse_typepartial    eclipse_typetotal  \n             203.50              2154.02               -17.72  \n\nfit2\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nduration ~ eclipse_type\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 48261 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 48261 21352180000 2218.1060  \n  2) eclipse_type=annular,total 3103    13016500  189.2185 *\n  3) eclipse_type=partial 45158  7688324000 2357.5200 *\n\nfit3\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nduration ~ eclipse_type\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      48261 \nNumber of independent variables:  1 \nMtry:                             1 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       159581.6 \nR squared (OOB):                  0.6393153 \n\n\nThe models successfuly fit; now we’ll discuss the results from each before we compare the performance of each model.\nLinear Regression: The duration of a partial eclipse is, on average, 2154.02 seconds longer than an annular eclipse. The duration of a total eclipse is, on average, 17.72 seconds shorter than an annular eclipse.\nDecision Tree: The decision tree splits the data into groups based on the eclipse type. The root node represents the entire dataset, with an average duration of 2218.1060 seconds. The tree then splits the data into two groups: one for annular and total eclipses, and another for partial eclipses. The average duration for annular and total eclipses is about 189 seconds, while the average duration for partial eclipses is about 2357 seconds.\nRandom Forest: The random forest model provides an MSE of 159581.6. The R-squared value indicates that around 63.93% of the variability in duration can be explained by the eclipse type.\nNow we’ll evaluate each model by comparing their RMSEs, residuals, and observed vs. predicted accuracy. This will help us choose the “best” model that we can then evaluate using the test data from the split we made earlier.\n\n# Use the 'fit' objects to predict on the training data\npred1 &lt;- predict(fit1, new_data = train_data) %&gt;% bind_cols(train_data)\npred2 &lt;- predict(fit2, new_data = train_data) %&gt;% bind_cols(train_data)\npred3 &lt;- predict(fit3, new_data = train_data) %&gt;% bind_cols(train_data)\n\n# Calculate residuals\npred1 &lt;- pred1 %&gt;% mutate(residuals = .pred - duration)\npred2 &lt;- pred2 %&gt;% mutate(residuals = .pred - duration)\npred3 &lt;- pred3 %&gt;% mutate(residuals = .pred - duration)\n\n# Evaluate performance using RMSE and R-squared\nperf1 &lt;- pred1 %&gt;% metrics(truth = duration, estimate = .pred)\nperf2 &lt;- pred2 %&gt;% metrics(truth = duration, estimate = .pred)\nperf3 &lt;- pred3 %&gt;% metrics(truth = duration, estimate = .pred)\n\n# Print performance metrics\nperf1\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     399.   \n2 rsq     standard       0.639\n3 mae     standard     303.   \n\nperf2\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     399.   \n2 rsq     standard       0.639\n3 mae     standard     303.   \n\nperf3\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     399.   \n2 rsq     standard       0.639\n3 mae     standard     303.   \n\n# Combine the predictions into one dataframe\npredictions_df &lt;- bind_rows(\n  pred1 %&gt;% mutate(Model = \"Linear Regression\"),\n  pred2 %&gt;% mutate(Model = \"Decision Tree\"),\n  pred3 %&gt;% mutate(Model = \"Random Forest\")\n)\n\n# Create a residuals plot using ggplot2\nggplot(predictions_df, aes(x = duration)) +\n  geom_point(aes(y =  residuals, color = Model), shape = 1) +\n  geom_abline(slope = 0, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Observed\", y = \"Residuals\") +  # Axis labels\n  theme_minimal() +  # Minimal theme\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\")) +  # Color for each model\n  guides(color = guide_legend(title = \"Model\"))  # Legend title\n\n\n\n\n\n\n\n# Create an observed vs predicted plot using ggplot2\nggplot(predictions_df, aes(x = duration, y = .pred, color = Model)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Observed\", y = \"Predicted\") +  # Axis labels\n  theme_minimal() +  # Minimal theme\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\")) +  # Color for each model\n  guides(color = guide_legend(title = \"Model\"))  # Legend title\n\n\n\n\n\n\n\n\nWe can see that the RMSEs, predictions, and residuals are all similar for each model. They seem to overlap heavily and split into two distinct lines on both graphs; we can assume that the small line is our total/annular eclipses and the big line is our partial eclipses. Since our metrics are all pretty close, I think it’s safe to pick the simplest model (linear regression) in order to make interpretation easier.\nWe’ll now evaluate the performance of our trained model using the test data we reserved at the start of our model fitting.\n\n# Use the 'fit' object to predict on the test data\npred_test &lt;- predict(fit1, new_data = test_data) %&gt;% bind_cols(test_data)\n\n# Calculate residuals\npred_test &lt;- pred_test %&gt;% mutate(residuals = .pred - duration)\n\n# Evaluate performance using RMSE\nrmse_test &lt;- pred_test %&gt;% rmse(truth = duration, estimate = .pred)\n\n# Print RMSE\nprint(rmse_test)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        391.\n\n# Plot residuals\nggplot(pred_test, aes(x = .pred, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Predicted\", y = \"Residuals\") +  # Axis labels\n  theme_minimal()  # Minimal theme\n\n\n\n\n\n\n\n# Plot observed vs predicted values\nggplot(pred_test, aes(x = duration, y = .pred)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Observed\", y = \"Predicted\") +  # Axis labels\n  theme_minimal()  # Minimal theme\n\n\n\n\n\n\n\n\nThis looks pretty solid for the total/annular eclipse observations, but the partial eclipse predictions and residuals are pretty bad. The RMSE of 390 is pretty similar to our prior models (all fell around 399), so at least we know this model can make predictions for unseen data just about as well as it did for the training data. Overall this isn’t an extremely useful analysis and the model could likely be improved, but it’s just for practice and to show that I can pilot a complete data analysis at this point in the class.\nIn summary, we conducted an EDA on the TidyTuesday eclipse dataset and did some cleaning in the process. We formulated a question based off of the data: Is there a correlation between eclipse duration and the type of eclipse? Our hypothesis was that each eclipse type would have a similar duration; however, after fitting the data to several models and evaluating the “best” model on our test data, we see that the annular and total eclipses were a couple minutes shorter on average than our partial eclipses. This could have something to do with how we calculated eclipse duration. For total/annular eclipse sets, we subtracted the start of totality/annularity from the end of the eclipses. For the partial eclipse sets, we calculated duration by subtracting the peak of each eclipse from the end of the eclipse.\nIf the definitions of a peak partial eclipse and the start of totality/annularity are different, then we could be comparing times that aren’t measuring the same thing. I don’t personally know the difference, but it seems to me that they’d be similar. Overall, as shown by the graph below, we can see that total/annular eclipses seem to last shorter on average than partial eclipses.\n\n# Create a boxplot\nggplot(duration_combined, aes(x = eclipse_type, y = duration)) +\n  geom_boxplot() +\n  labs(x = \"Type of Eclipse\", y = \"Duration\") +  # Axis labels\n  theme_minimal()  # Minimal theme"
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "EMMA HARDIN-PARKER contributed to this exercise.\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\nWarning: package 'here' was built under R version 4.3.2"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nHair Color\n0\n1\n1\n2\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nStrands of Hair\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5592.11111\n3533.51456\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\nFigure 2 shows a box plot giving the distributions of “Height” and “Hair Color” made from R. Black haired individuals tend to average about 173 cm. with a max of 180 cm. and a low of 162. Brown haired individuals average about 166 cm. with a max of 176 and low of 154. Red haired individuals average about 166 cm. with a max of 171 and low of 151. There is no distribution for purple and white due to there being only one individual for each.\n\n\n\n\n\n\n\n\nFigure 2: A box plot of Height (cm) and Hair Color.\n\n\n\n\n\nFigure 3 shows a scatter plot showing the correlation between “Weight” and “Strands of Hair” made from R. It turns out that there is no correlation between these two variables.\n\n\n\n\n\n\n\n\nFigure 3: A scatter plot showing Weight (lbs) and Strands of Hair."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nExample Table 3 shows a silly linear model fit of “Hair Color” and “Strands of Hair” as predictors for “Height”. The small sample size makes drawing any accurate conclusions difficult.\n\n\n\n\nTable 3: Linear model fit table of Hair Color and Strands of Hair as predictors for Height.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n176.7015914\n11.9719368\n14.7596495\n0.0006747\n\n\nHair ColorBR\n-3.4262889\n14.0614015\n-0.2436662\n0.8232017\n\n\nHair ColorP\n13.5369154\n17.6933112\n0.7650866\n0.4999000\n\n\nHair ColorR\n-8.9391056\n13.5302875\n-0.6606737\n0.5560240\n\n\nHair ColorW\n-39.1558092\n16.2731172\n-2.4061653\n0.0953447\n\n\nStrands of Hair\n-0.0009048\n0.0018297\n-0.4945259\n0.6548825"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   1   2     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean     sd  p0  p25  p50  p75 p100\n1 Height                  0             1  166.    16.0 133  156  166  178  183\n2 Weight                  0             1   70.1   21.2  45   55   70   80  110\n3 Strands of Hair         0             1 5592.  3534.    1 5000 6000 8178 9122\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▃▁▃▂▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nScatterplot\n\np5 &lt;- mydata %&gt;% ggplot(aes(x = Weight, y = `Strands of Hair`)) +\n  geom_point()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-strands-scatter.png\")\nggsave(filename = figure_file, plot = p5)\n\nSaving 7 x 5 in image\n\n\nBoxplot\n\np6 &lt;- mydata %&gt;% ggplot(aes(x = `Hair Color`, y = Height)) +\n  geom_boxplot()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-color-box.png\")\nggsave(filename = figure_file, plot = p6)\n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   3   6     0        6          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean     sd  p0  p25  p50  p75 p100\n1 Height                  0             1  166.    16.0 133  156  166  178  183\n2 Weight                  0             1   70.1   21.2  45   55   70   80  110\n3 Strands of Hair         0             1 5592.  3534.    1 5000 6000 8178 9122\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▃▁▃▂▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise, we will be recreating a figure found on the FiveThirtyEight website about data detailing how Congress members are older on average than ever before. We’ll use the ggplot package and some help from Microsoft Copilot’s Precise Mode to generate the base code.\nI entered this prompt to get this first output that I modified to load the actual dataset:\n“I would like to use R to recreate the figure titled”The House and Senate are older than ever before Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023” found at this link: https://fivethirtyeight.com/features/aging-congress-boomers/. Can you give me code to reproduce that exact figure? The data is open access and available here: https://data.fivethirtyeight.com/. It is under the section “Congress Today Is Older Than It’s Ever Been” from April 2, 2023.”\nI also entered this prompt for the line that adds the column “Year” to the set: “The dataset doesn’t have a column for”Year”, but instead records the time period in a column named “congress” which is defined as follows: The number of the Congress that this member’s row refers to. For example, 118 indicates the member served in the 118th Congress (2023-2025). How do I go from this raw format to the one in their figure?” As well as this prompt: “Is there a way I can modify the plot to just show the average ages for each year? The variable”age_years” in the dataset doesn’t do this; it only lists the age for each member in the set.”\nHere is the AI output (made into annotations to prevent loading/error messages) after all of these prompts:\n\n# Load necessary libraries\n# library(dplyr)\n# library(ggplot2)\n\n# Calculate average age for each year\n#avg_age &lt;- congress %&gt;%\n#  group_by(Year, chamber) %&gt;%\n#  summarise(avg_age_years = mean(age_years, na.rm = TRUE))\n\n# Plot the data\n#ggplot(avg_age, aes(x = Year, y = avg_age_years, color = chamber)) +\n # geom_line() +\n # labs(title = \"The House and Senate are older than ever before\",\n  #     subtitle = \"Average age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n  #     x = \"Year\",\n  #     y = \"Average Age\",\n  #     color = \"Chamber\") +\n # theme_bw()\n\nAnd everything past here is the manually modified code:\n\n#load packages\nlibrary(ggplot2)\nlibrary(here)\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(gt)\n\n# Replace 'path_to_file' with the path to your downloaded file\ncongress &lt;- read.csv(here(\"presentation-exercise\", \"congress-demographics\", \"data_aging_congress.csv\"))\n#Check the data\nhead(congress)\n\n  congress start_date chamber state_abbrev party_code                 bioname\n1       82 1951-01-03   House           ND        200    AANDAHL, Fred George\n2       80 1947-01-03   House           VA        100 ABBITT, Watkins Moorman\n3       81 1949-01-03   House           VA        100 ABBITT, Watkins Moorman\n4       82 1951-01-03   House           VA        100 ABBITT, Watkins Moorman\n5       83 1953-01-03   House           VA        100 ABBITT, Watkins Moorman\n6       84 1955-01-03   House           VA        100 ABBITT, Watkins Moorman\n  bioguide_id   birthday cmltv_cong cmltv_chamber age_days age_years generation\n1     A000001 1897-04-09          1             1    19626  53.73306       Lost\n2     A000002 1908-05-21          1             1    14106  38.62012   Greatest\n3     A000002 1908-05-21          2             2    14837  40.62149   Greatest\n4     A000002 1908-05-21          3             3    15567  42.62012   Greatest\n5     A000002 1908-05-21          4             4    16298  44.62149   Greatest\n6     A000002 1908-05-21          5             5    17028  46.62012   Greatest\n\n# Add a Year column to the data frame. 1787 is the first period of Congress with new ones every 2 years, so this calculation makes the \"congress\" column easier to visualize.\ncongress$Year &lt;- 1787 + 2 * congress$congress\n\n#Calculate the average age for each year in the dataset\navg_age &lt;- congress %&gt;%\n  group_by(Year, chamber) %&gt;%\n  summarise(avg_age_years = mean(age_years, na.rm = TRUE))\n\nNow we can create the plot:\n\n# Plot the data\nggplot(avg_age, aes(x = Year, y = avg_age_years, color = chamber)) +\n  geom_line() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Average age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       x = \"Year\",\n       y = \"Average Age\",\n       color = \"Chamber\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is very close to the original. We’ll now just modify the x and y axes to have the same increments in time as the original and change the colors of the lines. We’ll also change the plot to show stepwise increments instead of lines and remove the gridlines. We’ll also remove the axis labels since the original didn’t have any and bold our title. Then, we’ll adjust the legend position.\n\n#modify the axis increments and the colors of the lines. Remove gridlines, make step plot instead of line plot, and thicken lines. Also adjust the legend position.\nggplot(avg_age, aes(x = Year, y = avg_age_years, color = chamber)) +\n  geom_step(linewidth = 1.5) +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       x = \"\",\n       y = \"\",\n       color = \"\") +\n  scale_x_continuous(breaks = seq(1920, 2020, by = 20), limits = c(1920, 2020)) +\n  scale_y_continuous(breaks = seq(45, 65, by = 5), limits = c(45, 65)) +\n  scale_color_manual(values = c(\"Senate\" = \"purple\", \"House\" = \"green\")) +\n  theme_bw() +\n  theme(legend.position = \"top\", legend.justification = c(0,1))+\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.border = element_blank(),\n        plot.title = element_text(face=\"bold\"))\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_step()`).\n\n\n\n\n\n\n\n\n\nThis looks extremely close to the original. R is a very useful tool for creating and reproducing visualizations. Here’s the original figure for comparison: \nFor the next part of this exercise, we’ll create table with the same data shown in the plot. To begin, I gave Microsoft Copilot Precise Mode this prompt: “Now I would like to make a table that displays the information shown in the plot in a visually pleasing way. You can pick the R package used. What would the code look like for this?”\n\n# Create the table\nkable(avg_age, caption = \"Average age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\") %&gt;%\n  kable_styling(\"striped\", full_width = F)\n\n\nAverage age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\n\n\nYear\nchamber\navg_age_years\n\n\n\n\n1919\nHouse\n50.63485\n\n\n1919\nSenate\n56.71305\n\n\n1921\nHouse\n51.51950\n\n\n1921\nSenate\n57.19329\n\n\n1923\nHouse\n51.50112\n\n\n1923\nSenate\n57.26109\n\n\n1925\nHouse\n52.35196\n\n\n1925\nSenate\n56.95630\n\n\n1927\nHouse\n53.28969\n\n\n1927\nSenate\n56.88502\n\n\n1929\nHouse\n53.80848\n\n\n1929\nSenate\n58.03901\n\n\n1931\nHouse\n53.71394\n\n\n1931\nSenate\n57.30885\n\n\n1933\nHouse\n52.54248\n\n\n1933\nSenate\n57.46966\n\n\n1935\nHouse\n51.56923\n\n\n1935\nSenate\n56.98198\n\n\n1937\nHouse\n51.33543\n\n\n1937\nSenate\n57.57079\n\n\n1939\nHouse\n51.24558\n\n\n1939\nSenate\n57.21297\n\n\n1941\nHouse\n51.17905\n\n\n1941\nSenate\n57.60996\n\n\n1943\nHouse\n52.07287\n\n\n1943\nSenate\n57.91862\n\n\n1945\nHouse\n52.45802\n\n\n1945\nSenate\n57.70944\n\n\n1947\nHouse\n51.73979\n\n\n1947\nSenate\n56.40930\n\n\n1949\nHouse\n51.65482\n\n\n1949\nSenate\n56.99059\n\n\n1951\nHouse\n52.45117\n\n\n1951\nSenate\n56.68717\n\n\n1953\nHouse\n52.16560\n\n\n1953\nSenate\n57.30762\n\n\n1955\nHouse\n52.48055\n\n\n1955\nSenate\n57.37286\n\n\n1957\nHouse\n53.36413\n\n\n1957\nSenate\n57.79469\n\n\n1959\nHouse\n52.10761\n\n\n1959\nSenate\n57.68167\n\n\n1961\nHouse\n52.73533\n\n\n1961\nSenate\n57.16725\n\n\n1963\nHouse\n52.09492\n\n\n1963\nSenate\n56.61896\n\n\n1965\nHouse\n50.88059\n\n\n1965\nSenate\n57.85668\n\n\n1967\nHouse\n51.22730\n\n\n1967\nSenate\n57.61036\n\n\n1969\nHouse\n51.78546\n\n\n1969\nSenate\n56.83123\n\n\n1971\nHouse\n52.32744\n\n\n1971\nSenate\n56.28458\n\n\n1973\nHouse\n51.64166\n\n\n1973\nSenate\n55.60350\n\n\n1975\nHouse\n50.24058\n\n\n1975\nSenate\n55.64103\n\n\n1977\nHouse\n49.67102\n\n\n1977\nSenate\n54.33374\n\n\n1979\nHouse\n49.30329\n\n\n1979\nSenate\n52.97910\n\n\n1981\nHouse\n48.75481\n\n\n1981\nSenate\n52.82680\n\n\n1983\nHouse\n49.15492\n\n\n1983\nSenate\n53.80838\n\n\n1985\nHouse\n50.19013\n\n\n1985\nSenate\n54.78347\n\n\n1987\nHouse\n51.14722\n\n\n1987\nSenate\n54.90860\n\n\n1989\nHouse\n51.93884\n\n\n1989\nSenate\n55.91800\n\n\n1991\nHouse\n52.93039\n\n\n1991\nSenate\n57.56222\n\n\n1993\nHouse\n51.73735\n\n\n1993\nSenate\n58.04549\n\n\n1995\nHouse\n51.35202\n\n\n1995\nSenate\n58.52094\n\n\n1997\nHouse\n51.97229\n\n\n1997\nSenate\n57.96277\n\n\n1999\nHouse\n53.07509\n\n\n1999\nSenate\n58.70057\n\n\n2001\nHouse\n53.75171\n\n\n2001\nSenate\n58.81935\n\n\n2003\nHouse\n54.40583\n\n\n2003\nSenate\n59.92630\n\n\n2005\nHouse\n55.53191\n\n\n2005\nSenate\n60.85637\n\n\n2007\nHouse\n56.19800\n\n\n2007\nSenate\n62.16361\n\n\n2009\nHouse\n56.70651\n\n\n2009\nSenate\n62.20867\n\n\n2011\nHouse\n56.65312\n\n\n2011\nSenate\n61.88345\n\n\n2013\nHouse\n56.68415\n\n\n2013\nSenate\n61.09670\n\n\n2015\nHouse\n57.15462\n\n\n2015\nSenate\n61.15650\n\n\n2017\nHouse\n57.93973\n\n\n2017\nSenate\n62.33103\n\n\n2019\nHouse\n57.44181\n\n\n2019\nSenate\n62.75417\n\n\n2021\nHouse\n57.76279\n\n\n2021\nSenate\n63.53330\n\n\n2023\nHouse\n57.30675\n\n\n2023\nSenate\n63.93180\n\n\n\n\n\n\n\nThis has too many rows and isn’t very visually appealing. I’ll try a different prompt: “Is there a different type of table you suggest to display a large dataset?”\n\n# Create a new variable for the decade\navg_age$Decade &lt;- 10 * floor(avg_age$Year / 10)\n\n# Summarize the data\nsummary_data &lt;- avg_age %&gt;%\n  group_by(Decade, chamber) %&gt;%\n  summarise(\n    Mean = mean(avg_age_years),\n    Min = min(avg_age_years),\n    Max = max(avg_age_years)\n  )\n\n`summarise()` has grouped output by 'Decade'. You can override using the\n`.groups` argument.\n\n# Create the table\nknitr::kable(summary_data, caption = \"Summary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\")\n\n\nSummary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\n\n\nDecade\nchamber\nMean\nMin\nMax\n\n\n\n\n1910\nHouse\n50.63485\n50.63485\n50.63485\n\n\n1910\nSenate\n56.71305\n56.71305\n56.71305\n\n\n1920\nHouse\n52.49415\n51.50112\n53.80848\n\n\n1920\nSenate\n57.26694\n56.88502\n58.03901\n\n\n1930\nHouse\n52.08133\n51.24558\n53.71394\n\n\n1930\nSenate\n57.30885\n56.98198\n57.57079\n\n\n1940\nHouse\n51.82091\n51.17905\n52.45802\n\n\n1940\nSenate\n57.32758\n56.40930\n57.91862\n\n\n1950\nHouse\n52.51381\n52.10761\n53.36413\n\n\n1950\nSenate\n57.36880\n56.68717\n57.79469\n\n\n1960\nHouse\n51.74472\n50.88059\n52.73533\n\n\n1960\nSenate\n57.21690\n56.61896\n57.85668\n\n\n1970\nHouse\n50.63680\n49.30329\n52.32744\n\n\n1970\nSenate\n54.96839\n52.97910\n56.28458\n\n\n1980\nHouse\n50.23718\n48.75481\n51.93884\n\n\n1980\nSenate\n54.44905\n52.82680\n55.91800\n\n\n1990\nHouse\n52.21343\n51.35202\n53.07509\n\n\n1990\nSenate\n58.15840\n57.56222\n58.70057\n\n\n2000\nHouse\n55.31879\n53.75171\n56.70651\n\n\n2000\nSenate\n60.79486\n58.81935\n62.20867\n\n\n2010\nHouse\n57.17469\n56.65312\n57.93973\n\n\n2010\nSenate\n61.84437\n61.09670\n62.75417\n\n\n2020\nHouse\n57.53477\n57.30675\n57.76279\n\n\n2020\nSenate\n63.73255\n63.53330\n63.93180\n\n\n\n\n\nMuch better, but it isn’t very stylistically pleasing. I’ll ask the AI one more prompt: “Can I stylise it to look more appealing?”\n\n# Create the table\nsummary_data %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Summary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\"\n  ) %&gt;%\n  cols_label(\n    Decade = \"Decade\",\n    chamber = \"Chamber\",\n    Mean = \"Mean Age\",\n    Min = \"Minimum Age\",\n    Max = \"Maximum Age\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Mean, Min, Max),\n    decimals = 2\n  ) %&gt;%\n  tab_options(\n    table.width = px(500),\n    table.font.size = px(12)\n  )\n\n\n\n\n\n\n\nSummary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\n\n\nChamber\nMean Age\nMinimum Age\nMaximum Age\n\n\n\n\n1910\n\n\nHouse\n50.63\n50.63\n50.63\n\n\nSenate\n56.71\n56.71\n56.71\n\n\n1920\n\n\nHouse\n52.49\n51.50\n53.81\n\n\nSenate\n57.27\n56.89\n58.04\n\n\n1930\n\n\nHouse\n52.08\n51.25\n53.71\n\n\nSenate\n57.31\n56.98\n57.57\n\n\n1940\n\n\nHouse\n51.82\n51.18\n52.46\n\n\nSenate\n57.33\n56.41\n57.92\n\n\n1950\n\n\nHouse\n52.51\n52.11\n53.36\n\n\nSenate\n57.37\n56.69\n57.79\n\n\n1960\n\n\nHouse\n51.74\n50.88\n52.74\n\n\nSenate\n57.22\n56.62\n57.86\n\n\n1970\n\n\nHouse\n50.64\n49.30\n52.33\n\n\nSenate\n54.97\n52.98\n56.28\n\n\n1980\n\n\nHouse\n50.24\n48.75\n51.94\n\n\nSenate\n54.45\n52.83\n55.92\n\n\n1990\n\n\nHouse\n52.21\n51.35\n53.08\n\n\nSenate\n58.16\n57.56\n58.70\n\n\n2000\n\n\nHouse\n55.32\n53.75\n56.71\n\n\nSenate\n60.79\n58.82\n62.21\n\n\n2010\n\n\nHouse\n57.17\n56.65\n57.94\n\n\nSenate\n61.84\n61.10\n62.75\n\n\n2020\n\n\nHouse\n57.53\n57.31\n57.76\n\n\nSenate\n63.73\n63.53\n63.93\n\n\n\n\n\n\n\nThis is a pretty good table overall and much easier to digest than the first one the AI spit out. Tables don’t seem to have as many options as figures when it comes to customization, but this is a good and quick way to visualize data and it gives more information than the plot did."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Load the dslabs package. Install if you haven’t. Then, inspect the gapminder dataset.\n\n#load dslabs package and tidyverse\nlibrary(dslabs)\nlibrary(tidyverse)\n#look at help file for gapminder data\nhelp(gapminder)\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data \nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nCreate a new object that contains only the African countries. Then, check the structure and summary of the new object.\n\n#create the object with only African countries\nafrican_countries &lt;- gapminder[gapminder$continent == \"Africa\", ]\n#check the structure and summary\nstr(african_countries)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(african_countries)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nNow, using the new African countries object, create two new objects. One should only contain “infant_mortality” and “life_expectancy” and the other should only hold “population” and “life_expectancy”.\n\n#create the object with only population and life expectancy data\nafrican_countries_pop_life &lt;- african_countries[, c(\"population\", \"life_expectancy\")]\n#create the object with only infant mortality and life expectancy data\nafrican_countries_infant_life &lt;- african_countries[, c(\"infant_mortality\", \"life_expectancy\")]\n\nNow that we’ve created two new objects that look at these specific variables, we can inspect them and get a better idea of the data.\n\n#look at the structure and summary of the first object\nstr(african_countries_pop_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(african_countries_pop_life)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n#do the same for the second object\nstr(african_countries_infant_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(african_countries_infant_life)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\nUsing the two new objects we can now create plots to characterize the relationship between life expectancy, population, and infant mortality. We’ll create two plots; one that analyzes life expectancy vs infant mortality and one that analyzes life expectancy vs population size. The latter will have a log scale to make the data easier to visualize.\n\n#load ggplot2 to create better plots\nlibrary(ggplot2)\n\n# Plot 1: Life expectancy vs. Infant mortality. lab() creates titles for the graph.\nggplot(african_countries_infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality\")\n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot 2: Life expectancy vs. Population size.\n#scale_x_log10 puts the x axis (population) on a log scale.\nggplot(african_countries_pop_life, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size (log scale)\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn Plot 1, we can see a negative correlation. As life expectancy decreases, the number of infants dying increases; this makes sense since more developed countries with better healthcare have higher life expectancies and lower infant mortality rates. In Plot 2, population size and life expectancy are positively correlated. This is logical, as longer lives allow for greater population growth and more infrequent deaths. The “streaks” in the data can be attributed to the presence of different years for individual countries in the dataset.\nKnowing this, we can begin to narrow in on certain years and see which ones would be easiest to analyze given our dataset. We’ll figure out which years have missing data for infant mortality.\n\n#find which years have missing data for infant mortality. \n#is.na() identifies which rows have na as their value\n#select() shows us the years that are associated with these rows.\nafrican_countries %&gt;%\n  filter(is.na(infant_mortality)) %&gt;%\n  select(year)\n\n    year\n1   1960\n2   1960\n3   1960\n4   1960\n5   1960\n6   1960\n7   1960\n8   1960\n9   1960\n10  1960\n11  1961\n12  1961\n13  1961\n14  1961\n15  1961\n16  1961\n17  1961\n18  1961\n19  1961\n20  1961\n21  1961\n22  1961\n23  1961\n24  1961\n25  1961\n26  1961\n27  1961\n28  1962\n29  1962\n30  1962\n31  1962\n32  1962\n33  1962\n34  1962\n35  1962\n36  1962\n37  1962\n38  1962\n39  1962\n40  1962\n41  1962\n42  1962\n43  1962\n44  1963\n45  1963\n46  1963\n47  1963\n48  1963\n49  1963\n50  1963\n51  1963\n52  1963\n53  1963\n54  1963\n55  1963\n56  1963\n57  1963\n58  1963\n59  1963\n60  1964\n61  1964\n62  1964\n63  1964\n64  1964\n65  1964\n66  1964\n67  1964\n68  1964\n69  1964\n70  1964\n71  1964\n72  1964\n73  1964\n74  1964\n75  1965\n76  1965\n77  1965\n78  1965\n79  1965\n80  1965\n81  1965\n82  1965\n83  1965\n84  1965\n85  1965\n86  1965\n87  1965\n88  1965\n89  1966\n90  1966\n91  1966\n92  1966\n93  1966\n94  1966\n95  1966\n96  1966\n97  1966\n98  1966\n99  1966\n100 1966\n101 1966\n102 1967\n103 1967\n104 1967\n105 1967\n106 1967\n107 1967\n108 1967\n109 1967\n110 1967\n111 1967\n112 1967\n113 1968\n114 1968\n115 1968\n116 1968\n117 1968\n118 1968\n119 1968\n120 1968\n121 1968\n122 1968\n123 1968\n124 1969\n125 1969\n126 1969\n127 1969\n128 1969\n129 1969\n130 1969\n131 1970\n132 1970\n133 1970\n134 1970\n135 1970\n136 1971\n137 1971\n138 1971\n139 1971\n140 1971\n141 1971\n142 1972\n143 1972\n144 1972\n145 1972\n146 1972\n147 1972\n148 1973\n149 1973\n150 1973\n151 1973\n152 1973\n153 1973\n154 1974\n155 1974\n156 1974\n157 1974\n158 1974\n159 1975\n160 1975\n161 1975\n162 1975\n163 1975\n164 1976\n165 1976\n166 1976\n167 1977\n168 1977\n169 1977\n170 1978\n171 1978\n172 1979\n173 1979\n174 1980\n175 1981\n176 2016\n177 2016\n178 2016\n179 2016\n180 2016\n181 2016\n182 2016\n183 2016\n184 2016\n185 2016\n186 2016\n187 2016\n188 2016\n189 2016\n190 2016\n191 2016\n192 2016\n193 2016\n194 2016\n195 2016\n196 2016\n197 2016\n198 2016\n199 2016\n200 2016\n201 2016\n202 2016\n203 2016\n204 2016\n205 2016\n206 2016\n207 2016\n208 2016\n209 2016\n210 2016\n211 2016\n212 2016\n213 2016\n214 2016\n215 2016\n216 2016\n217 2016\n218 2016\n219 2016\n220 2016\n221 2016\n222 2016\n223 2016\n224 2016\n225 2016\n226 2016\n\n\nThere is data missing up to 1981 and then again for 2016, so we’ll select 2000. We’ll create a new object now with only observations from 2000.\n\n#create an object with only data from 2000\nafrican_countries_2000 &lt;- african_countries[african_countries$year == 2000, ]\n\nNow, we’ll make the same plots as before using only the data from 2000.\n\n# Plot 3: Life expectancy vs. Infant mortality.\nggplot(african_countries_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality\")\n\n\n\n\n\n\n\n# Plot 4: Life expectancy vs. Population size. \nggplot(african_countries_2000, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size (log scale)\")\n\n\n\n\n\n\n\n\nThere still seems to be a negative correlation in plot 3, but plot 4 shows no noticeable correlation. We can now create some linear models with this data and draw more conclusions from the year 2000.\n\n#Table 1: fit life expectancy as a function of infant mortality. \n#lm() creates a linear model for the specified variables from a given dataset.\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, african_countries_2000)\n#print results to screen\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = african_countries_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n#Table 2: fit life expectancy as a function of population size\nfit2 &lt;- lm(life_expectancy ~ population, african_countries_2000)\n#print results to screen\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = african_countries_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nBased off of the results from the fit, we can see that infant mortality is a statistically significant predictor of life expectancy for African countries in the year 2000. On the other hand, population does not seem to be a statistically significant predictor for life expectancy in 2000. These are logical conclusions given our prior knowledge of demography.\nThis section contributed by Cory Cribb\nLoading dslabs dataset “murders”. Probably a more morbid data set but interesting to observe nonetheless.\n\nlibrary(dslabs)\nhelp(murders)\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\nsummary(murders)\n\n    state               abb                      region     population      \n Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n Class :character   Class :character   South        :17   1st Qu.: 1696962  \n Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n                                       West         :13   Mean   : 6075769  \n                                                          3rd Qu.: 6636084  \n                                                          Max.   :37253956  \n     total       \n Min.   :   2.0  \n 1st Qu.:  24.5  \n Median :  97.0  \n Mean   : 184.4  \n 3rd Qu.: 268.0  \n Max.   :1257.0  \n\n\nSince I am originally from the Southern region of the US; lets explore murders in that region.\n\nSouth_Murders &lt;- murders[murders$region== \"South\", ]\nstr(South_Murders)\n\n'data.frame':   17 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Arkansas\" \"Delaware\" \"District of Columbia\" ...\n $ abb       : chr  \"AL\" \"AR\" \"DE\" \"DC\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ population: num  4779736 2915918 897934 601723 19687653 ...\n $ total     : num  135 93 38 99 669 376 116 351 293 120 ...\n\nsummary(South_Murders)\n\n    state               abb                      region     population      \n Length:17          Length:17          Northeast    : 0   Min.   :  601723  \n Class :character   Class :character   South        :17   1st Qu.: 2967297  \n Mode  :character   Mode  :character   North Central: 0   Median : 4625364  \n                                       West         : 0   Mean   : 6804378  \n                                                          3rd Qu.: 8001024  \n                                                          Max.   :25145561  \n     total      \n Min.   : 27.0  \n 1st Qu.:111.0  \n Median :207.0  \n Mean   :246.8  \n 3rd Qu.:293.0  \n Max.   :805.0  \n\n\nFrom this data set, it would appear the researchers classified 17 states as being in the Southern region. Let’s explore if population size had any relationship to gun murders.\n\nPop_and_murder &lt;- South_Murders[, c(\"total\", \"population\")]\nstr(Pop_and_murder)\n\n'data.frame':   17 obs. of  2 variables:\n $ total     : num  135 93 38 99 669 376 116 351 293 120 ...\n $ population: num  4779736 2915918 897934 601723 19687653 ...\n\nsummary(Pop_and_murder)\n\n     total         population      \n Min.   : 27.0   Min.   :  601723  \n 1st Qu.:111.0   1st Qu.: 2967297  \n Median :207.0   Median : 4625364  \n Mean   :246.8   Mean   : 6804378  \n 3rd Qu.:293.0   3rd Qu.: 8001024  \n Max.   :805.0   Max.   :25145561  \n\n\nCreate a scatter plot viewing total gun murders on the x-axis and state population on the y-axis to observe a trend. Add a best fit line to the plot to see if there is a trend.\n\nattach(South_Murders)\n\nThe following object is masked from package:tidyr:\n\n    population\n\nplot(total,population, main= \"Total gun murders vs. population size\", xlab=\"Total gun murders\", ylab=\"population\")\nabline(lm(population~total))\n\n\n\n\n\n\n\n\nFrom a quick view of the plot, we see that there appears to be a positive correlation that there are more gun murders in states with higher populations. Let’s run a linear model to see if the data gives a statistically significant observation.\n\nfit3 &lt;- lm(population~total, South_Murders)\nsummary(fit3)\n\n\nCall:\nlm(formula = population ~ total, data = South_Murders)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5332407  -680032   482183  1257898  1945758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -443125     717119  -0.618    0.546    \ntotal          29370       2229  13.178 1.19e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1898000 on 15 degrees of freedom\nMultiple R-squared:  0.9205,    Adjusted R-squared:  0.9152 \nF-statistic: 173.7 on 1 and 15 DF,  p-value: 1.189e-09\n\n\nFrom the simple linear regression, we see that the slope is statistically significant. The adjusted R-squared is 0.9152 which would indicated a Strong, Positive correlation in total gun murders per Gross Population size in the Southern Region of the United States of America."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Me holding a corn snake at the Savannah River Site Ecology Lab. They’re friendly but a bit freaky!"
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "Background",
    "text": "Background\n\nEducation\nI completed my Bachelors in Wildlife Sciences at Mississippi State University in May of 2023. I entered the ILS program at UGA the following August and have since matriculated into the EHS program as a first year in Erin Lipp’s lab.\n\n\nHometown\nI was born in Orange, CA and raised in Collierville, TN outside of Memphis. My parents and 5 pets (3 cats and 2 dogs) still live there!\n\n\nFun Fact\nDuring field research on pollinators at my undergraduate university, I was stung about 5 times by a swarm of yellowjackets that I apparently offended! I couldn’t move one of my fingers for three days."
  },
  {
    "objectID": "aboutme.html#research-interestsexperience",
    "href": "aboutme.html#research-interestsexperience",
    "title": "About me",
    "section": "Research Interests/Experience",
    "text": "Research Interests/Experience\n\nCurrent Research\nI don’t have my own project yet, but I’m interested in pursuing research on bacterial pathogens and environmental epidemiology. I would love to include a wildlife aspect!\n\n\nExperience\nI have taken some basic statistics courses and an overview of data analysis using R. I don’t know any coding languages, but I can navigate RStudio and make some basic plots and analyses.\n\n\nHopes for the class\nI hope to become proficient in making my data usable for analysis and to learn more about creating a workflow that allows for reproducibility.\n\n\nInteresting Data Analysis Facts\nThis website has some fun facts about data analysis. I found it interesting that 80% of a data scientist’s time is spent cleaning the data!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Kosewick’s Website",
    "section": "",
    "text": "Welcome to my website and data analysis portfolio.\nHere I’ve compiled my projects for the Modern Applied Data Analysis(MADA) class at the University of Georgia in the Spring semester of 2024. This course is part of my PhD curriculum in Environmental Health Sciences and was a great way to learn many new skills in the field of data analysis. Some of the topics practiced include machine learning models, data visualization, synthetic data generation, exploratory and statistical analyses, and more.\n\nFeel free to use the Menu Bar above to learn more about me and my background or navigate to my different exercises from the MADA course. Thank you for looking!\nLast updated 4/12/2024"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\n\nWarning: package 'readxl' was built under R version 4.3.2\n\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height            &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"16…\n$ Weight            &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 5…\n$ Gender            &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"…\n$ `Strands of Hair` &lt;dbl&gt; 4, 5000, 5005, 6000, 7000, 8000, 1, 9000, 7500, 7550…\n$ `Hair Color`      &lt;chr&gt; \"Blonde\", \"Red\", \"Black\", \"Brown\", \"White\", \"Purple\"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Strands of Hair\n Length:14          Min.   :  45.0   Length:14          Min.   :   1   \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:5010   \n Mode  :character   Median :  70.0   Mode  :character   Median :6525   \n                    Mean   : 602.7                      Mean   :5960   \n                    3rd Qu.:  90.0                      3rd Qu.:7888   \n                    Max.   :7000.0                      Max.   :9122   \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Strands of Hair` `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M                      4 Blonde      \n2 175        70 O                   5000 Red         \n3 sixty      60 F                   5005 Black       \n4 178        76 F                   6000 Brown       \n5 192        90 NA                  7000 White       \n6 6          55 F                   8000 Purple      \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n5959.57\n2882.38\n1\n5009.75\n6525\n7887.5\n9122\n▂▁▃▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nStrands of Hair\n0\n1\n5592.11\n3533.51\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\n\nWarning: package 'readxl' was built under R version 4.3.2\n\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.2\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.2\n\n\nhere() starts at D:/MADA/kevinkosewick-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                       `Allowed Values`  \n  &lt;chr&gt;           &lt;chr&gt;                                       &lt;chr&gt;             \n1 Height          height in centimeters                       numeric value &gt;0 …\n2 Weight          weight in kilograms                         numeric value &gt;0 …\n3 Gender          identified gender (male/female/other)       M/F/O/NA          \n4 Strands of Hair How many strands of hair the individual has numeric value &gt;0 …\n5 Hair Color      What color the invididual's hair is         BR/BL/R/W/P; BR=b…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height            &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"16…\n$ Weight            &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 5…\n$ Gender            &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"…\n$ `Strands of Hair` &lt;dbl&gt; 4, 5000, 5005, 6000, 7000, 8000, 1, 9000, 7500, 7550…\n$ `Hair Color`      &lt;chr&gt; \"BL\", \"R\", \"BL\", \"BR\", \"W\", \"P\", \"R\", \"BR\", \"BR\", \"B…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Strands of Hair\n Length:14          Min.   :  45.0   Length:14          Min.   :   1   \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:5010   \n Mode  :character   Median :  70.0   Mode  :character   Median :6525   \n                    Mean   : 602.7                      Mean   :5960   \n                    3rd Qu.:  90.0                      3rd Qu.:7888   \n                    Max.   :7000.0                      Max.   :9122   \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Strands of Hair` `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M                      4 BL          \n2 175        70 O                   5000 R           \n3 sixty      60 F                   5005 BL          \n4 178        76 F                   6000 BR          \n5 192        90 NA                  7000 W           \n6 6          55 F                   8000 P           \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n5959.57\n2882.38\n1\n5009.75\n6525\n7887.5\n9122\n▂▁▃▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nStrands of Hair\n0\n1\n5592.11\n3533.51\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing informatio"
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise, we will generate a synthetic data set that simulates foraging trends of the Northern Mockingbird somewhere in the United States. We will then generate some figures and statistical analyses to explore our new data set. I’ll be using Microsoft Edge AI tools to help generate code. Let’s begin by loading helpful packages for creating synthetic data sets.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(nnet)\n\nI’ve entered this prompt into Microsoft Edge copilot: “Write R code that creates data for N=100 Northern Mockingbirds. Individuals ages are randomly distributed between 1 and 14 years. Assume that individuals forage for two types of food: insects and plants. Individuals can consume a combination of either food type at an average of half an ounce daily. Food consumption is measured in ounces every other day. Assume that insects are more plentiful in warmer seasons; this should be reflected in the amount of each food type consumed in each season. The data should be collected over the course of a year.\nCreate a data frame that contains Food Type, Age, Daily Food Consumption (in ounces), Season, and Sex.\nFor function calls, specify explicitly the package from which the function comes from.\nAdd thorough documentation to the code so it is clear what each line of code does.”\nThis generated the following code:\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years\nage &lt;- sample(1:14, N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\"] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\"] * \n  rep(insect_proportion, each = N/2)[data$Season[data$Food_Type == \"insects\"]]\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                     NA spring female\n 2 insects      14                     NA spring male  \n 3 insects       3                     NA spring female\n 4 insects      10                     NA spring male  \n 5 insects       2                     NA spring female\n 6 insects       6                     NA spring male  \n 7 insects      11                     NA spring male  \n 8 insects       5                     NA spring male  \n 9 insects       4                     NA spring female\n10 insects      14                     NA spring female\n# ℹ 90 more rows\n\n\nThis is a good start, but there are some problems. First, the AI only gave values of NA to food consumption data collected in the spring and summer. Second, there are an abnormally high number of old birds in the population; in a normal wild population, we’d expect to see more mockingbirds in the 2-8 year range, as 14 is on the old end for a mockingbird. We’ll have the AI correct the data frame with the following prompt:\n“Northern Mockingbirds have an average lifespan of 8 years in the wild. Young and elderly individuals have a higher mortality rate. Given this information, please adjust the age variable to more accurately reflect age distribution in a wild population of mockingbirds. Furthermore, mockingbirds consume more food in the spring and summer due to increased energy demands that come with mating and nesting. Please replace the NA food consumption values you gave with randomized values that have an average of 0.5.”\nThe AI then adjusted our previous code string as follows:\n\n# Set the seed for reproducibility\n#set.seed(123)\n\n# Define the number of birds\n#N &lt;- 100\n\n# Generate random ages between 1 and 14 years\n#age &lt;- sample(1:14, N, replace = TRUE, prob = c(0.05, 0.05, 0.1, 0.15, 0.2, 0.15, 0.1, 0.05, 0.025, 0.025))\n\n# Define the two types of food\n#food_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\n#food_consumption &lt;- 0.5\n\n# Define the seasons\n#seasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\n#insect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\n#sex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\n#data &lt;- tibble(\n # Food_Type = rep(food_type, each = N/2),\n# Age = age,\n # Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n # Season = rep(seasons, each = N/4),\n # Sex = sex\n# )\n\n# Adjust the food consumption based on the season\n#data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] &lt;- \n # data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] * \n # rep(insect_proportion[1:2], each = sum(data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")))\n\n# Replace the NA values with randomized values that have an average of 0.5\n#data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), 0.5, 0.1)))\n\n# Print the data\n#data\n\nThis generates a new error in the way the AI created values for the age variable. I gave the AI this prompt to correct the broken lines:\n“The issue with NA values has been solved. I now need you to correct the line where you generate random ages between 1 and 14 years to resolve this error: Error in sample.int(length(x), size, replace, prob) : incorrect number of probabilities”\nThe following code was generated:\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years with a probability distribution that reflects the higher mortality rate of young and elderly individuals\nage &lt;- sample(c(1:4, 5:8, 9:12, 13:14), N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] * \n  rep(insect_proportion[1:2], each = sum(data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")))\n\nWarning in data$Daily_Food_Consumption[data$Food_Type == \"insects\" &\n(data$Season == : number of items to replace is not a multiple of replacement\nlength\n\n# Replace the NA values with randomized values that have an average of 0.5\nif (any(is.na(data$Daily_Food_Consumption))) {\n  data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), mean(Daily_Food_Consumption), 0.1)))\n}\n\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.286 spring female\n 2 insects      14                  0.384 spring male  \n 3 insects       3                  0.406 spring female\n 4 insects      10                  0.329 spring male  \n 5 insects       2                  0.284 spring female\n 6 insects       6                  0.309 spring male  \n 7 insects      11                  0.383 spring male  \n 8 insects       5                  0.289 spring male  \n 9 insects       4                  0.206 spring female\n10 insects      14                  0.284 spring female\n# ℹ 90 more rows\n\n\nWe seem to have finally generated a data set that should reflect associations we expect to see in the population. However, I’d like to make one more adjustment that reflects increased food consumption by females in the spring and summer due to reproductive demands. I’ll use a 1.75 times increase in food consumption, as data is limited on the exact amounts/averages mockingbirds consume.\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years with a probability distribution that reflects the higher mortality rate of young and elderly individuals\nage &lt;- sample(c(1:4, 5:8, 9:12, 13:14), N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season and sex\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\") & data$Sex == \"female\"] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\") & data$Sex == \"female\"] * \n  1.75\n\n# Replace the NA values with randomized values that have an average of 0.5\nif (any(is.na(data$Daily_Food_Consumption))) {\n  data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), mean(Daily_Food_Consumption), 0.1)))\n}\n\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.835 spring female\n 2 insects      14                  0.640 spring male  \n 3 insects       3                  1.18  spring female\n 4 insects      10                  0.549 spring male  \n 5 insects       2                  0.828 spring female\n 6 insects       6                  0.515 spring male  \n 7 insects      11                  0.638 spring male  \n 8 insects       5                  0.482 spring male  \n 9 insects       4                  0.601 spring female\n10 insects      14                  0.829 spring female\n# ℹ 90 more rows\n\n\nThis dataset looks a lot better. We’ll now check the structure and summary to get a better idea of what we created.\n\n#check the structure and summary\nsummary(data)\n\n  Food_Type              Age        Daily_Food_Consumption    Season         \n Length:100         Min.   : 1.00   Min.   :0.2652         Length:100        \n Class :character   1st Qu.: 5.00   1st Qu.:0.4490         Class :character  \n Mode  :character   Median : 8.50   Median :0.5660         Mode  :character  \n                    Mean   : 7.92   Mean   :0.6026                           \n                    3rd Qu.:11.00   3rd Qu.:0.6929                           \n                    Max.   :14.00   Max.   :1.2025                           \n     Sex           \n Length:100        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nstructure(data)\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.835 spring female\n 2 insects      14                  0.640 spring male  \n 3 insects       3                  1.18  spring female\n 4 insects      10                  0.549 spring male  \n 5 insects       2                  0.828 spring female\n 6 insects       6                  0.515 spring male  \n 7 insects      11                  0.638 spring male  \n 8 insects       5                  0.482 spring male  \n 9 insects       4                  0.601 spring female\n10 insects      14                  0.829 spring female\n# ℹ 90 more rows\n\n\nOur data looks good and reflects the averages we had the AI incorporate when creating our values. We’ll now create a few plots looking at some relationships in the data.\n\n#create a plot with food consumption as a function of season for all individuals (stratified by sex)\nggplot(data, aes(x = Season, y = Daily_Food_Consumption, color = Sex)) +\n  geom_boxplot() +\n  labs(title = \"Food Consumption as a Function of Season for All Individuals, Stratified by Sex\",\n       x = \"Season\",\n       y = \"Daily Food Consumption (oz)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe boxplot shows that our assumptions are reflected in the data set; females consume more in the spring and summer and our average food consumption is 0.5 ounces a day. Now we’ll see if the data accurately shows changes in the primary type of food consumed over the seasons.\n\n#create a histogram comparing food type consumed in different seasons stratified by sex\nggplot(data, aes(x = Season, fill = Food_Type)) +\n  geom_bar(position = \"dodge\", stat = \"count\") +\n  facet_grid(. ~ Sex) +\n  labs(title = \"Food Type Comparison Across Seasons, Stratified by Sex\",\n       x = \"Season\",\n       y = \"Count\",\n       fill = \"Food Type\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe plots show that insects are the predominant food source in the spring and summer while plants dominate the winter and fall. This accurately reflects booms in the insect population in spring and summer; Northern Mockingbirds consume over 85% insects then, dropping to around 15% in the colder seasons. Now that we see our data is tidy, reflects our assumptions and follows the trends we identified, we can fit the data to some linear models.\n\n#create a linear model with season and sex as predictors for food consumption\nmodel1&lt;- lm(Daily_Food_Consumption~ Season + Sex, data = data)\nsummary(model1)\n\n\nCall:\nlm(formula = Daily_Food_Consumption ~ Season + Sex, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27214 -0.11026 -0.01652  0.08739  0.35195 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.58657    0.03355  17.483  &lt; 2e-16 ***\nSeasonspring  0.26398    0.04267   6.187 1.54e-08 ***\nSeasonsummer  0.11562    0.04261   2.713  0.00791 ** \nSeasonwinter -0.03203    0.04288  -0.747  0.45690    \nSexmale      -0.16488    0.03079  -5.354 5.95e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1506 on 95 degrees of freedom\nMultiple R-squared:  0.4768,    Adjusted R-squared:  0.4548 \nF-statistic: 21.65 on 4 and 95 DF,  p-value: 1.023e-12\n\n\nIt seems like winter doesn’t have a significant impact on food consumption, but spring and summer do in the created dataset. Now we’ll move on to make a couple more models.\n\n#create a linear model with season as a predictor for food type\n# model2&lt;- lm(Food_Type ~ Season, data = data)\n# summary(model2)\n\nA linear regression didn’t work for this type of data.The above line generated an error message. After consulting with AI, a multinomial logistic regression model would work better. We’ll also include sex as a predictor in this one.\n\n#Using the nnet package for this model\n\n# Create a multinomial logistic regression model with season and sex as predictors for food type\nmodel2 &lt;- multinom(Food_Type ~ Season + Sex, data = data)\n\n# weights:  6 (5 variable)\ninitial  value 69.314718 \niter  10 value 0.021604\niter  20 value 0.012091\niter  30 value 0.000938\niter  40 value 0.000662\niter  50 value 0.000442\niter  60 value 0.000273\niter  70 value 0.000224\niter  80 value 0.000159\niter  90 value 0.000153\niter 100 value 0.000122\nfinal  value 0.000122 \nstopped after 100 iterations\n\nsummary(model2)\n\nCall:\nmultinom(formula = Food_Type ~ Season + Sex, data = data)\n\nCoefficients:\n                  Values Std. Err.\n(Intercept)   13.3268743  186.7045\nSeasonspring -26.8167895  232.8969\nSeasonsummer -26.8610564  236.0821\nSeasonwinter  16.4198276    0.0000\nSexmale        0.5251443  186.4204\n\nResidual Deviance: 0.0002439321 \nAIC: 10.00024 \n\n\nIt seems that the model agrees with our assumptions. The odds of mockingbirds choosing plants over insects are lower in the spring and summer and the opposite in winter. The residual deviance is low, indicating a good fit. We’ve created a pretty good dataset that has the associations and trends we wanted to see."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "We will be using the dataset found here and made by the nlmixr team for this exercise. The dataset contains pharmacokinetic observations from 120 subjects who were administered IV infusions of mavoglurant. We’ll begin by loading the necessary packages and the dataset.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(here)\nlibrary(tidymodels)\n\n# Load the data\nfittingdata &lt;- read.csv(here(\"fitting-exercise\",\"fittingdata.csv\"))\n\n# Check the data\nsummary(fittingdata)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\n# Create plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose \nggplot(fittingdata, aes(x = TIME, y = DV, color = DOSE, group = ID)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"DV\", color = \"Dose\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that the data records a time series measuring concentrations of DV (which stands for Dependent Variable, which is Mavoglurant). Looking at the summary, we can see that OCC has values greater than 1. We don’t know what these mean so we probably shouldn’t use them. We’ll remove all observations with values other than 1.\n\n# Load necessary library\nlibrary(dplyr)\n\n# Filter the data\nfittingdata2 &lt;- fittingdata %&gt;% filter(OCC == 1)\n\nWe now want to compute the sum of DV for each individual to determine the full amount of drug for each individual. I understand that according to the exercise details this is not the best approach, but this is mainly for practice anyways. I consulted Microsoft Copilot for help with this using this prompt (which is also the same as the instructions laid out in the exercise explanation): “Write code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18.”\n\n# Exclude observations with TIME = 0 and compute the sum of DV for each individual\nfittingdata_sum &lt;- fittingdata2 %&gt;%\n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV))\n\n# Create a data frame that contains only the observations where TIME == 0\nfittingdata_time0 &lt;- fittingdata2 %&gt;%\n  filter(TIME == 0)\n\n# Use the appropriate join function to combine those two data frames\nfittingdata_combined &lt;- left_join(fittingdata_time0, fittingdata_sum, by = \"ID\")\n\nWe’ve created a new data frame that contains columns that are much easier to analyze now. We’ll do some final cleaning steps by converting RACE and SEX to factors and removing some columns that we no longer need.\n\n# Convert RACE and SEX to factor variables and keep only variables specified in the exercise instructions\nfittingdata_final &lt;- fittingdata_combined %&gt;%\n  mutate(RACE = as.factor(RACE),\n         SEX = as.factor(SEX)) %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n\n#save the cleaned data\nsaveRDS(fittingdata_final, file = \"modelfitting.rds\")\n\n# Check data to make sure everything is good\nsummary(fittingdata_final)\n\n       Y               DOSE            AGE        SEX     RACE   \n Min.   : 826.4   Min.   :25.00   Min.   :18.00   1:104   1 :74  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36  \n Median :2349.1   Median :37.50   Median :31.00           7 : 2  \n Mean   :2445.4   Mean   :36.46   Mean   :33.00           88: 8  \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25                  \n Max.   :5606.6   Max.   :50.00   Max.   :50.00                  \n       WT               HT       \n Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 73.17   1st Qu.:1.700  \n Median : 82.10   Median :1.770  \n Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :115.30   Max.   :1.930  \n\nclass(fittingdata_final$RACE)\n\n[1] \"factor\"\n\n\nWe’ll begin a formal EDA now. We’re interested in how each of the variables influences our outcome variable that we created, “Y”. Again, this is the sum per individual of all of our original “DV” values. Before we begin, we should note that the documentation for this dataset is not very good. We don’t know what the values in RACE or SEX indicate, so interpreting results from the EDA will be challenging for these. According to the study this is based off of, 86% of participants were male, so we can assume that a value of 1 is male and 2 is female (based off of the frequency of these values in the dataset). We’ll generate plots for them regardless. First up is our AGE variable.\n\n# Histogram for Age\nggplot(fittingdata_final, aes(x = AGE)) +\n  geom_histogram(binwidth = 10) +\n  labs(title = \"Histogram of Age\", x = \"Age (years)\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Age\nggplot(fittingdata_final, aes(x = AGE, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Age\", x = \"Age (years)\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOur age values seem to have a relatively normal distribution with a minimum of 18 and maximum of 50. Our scatterplot shows that mavoglurant concentrations seem to remain the same on average between individuals of different ages. The plot shows no clear correlation one way or the other. Next, we’ll investigate SEX.\n\n# Boxplot for mavoglurant concentration by sex\nggplot(fittingdata_final, aes(x = SEX, y = Y)) +\n  geom_boxplot() +\n  labs(title = \"Concentration by Sex\", x = \"Sex\", y = \"Mavoglurant Concentration\")\n\n\n\n\n\n\n\n\nIf we knew what our dataset’s values meant or had clear documentation somewhere, we could interpret these results with certainty. Instead, all we can say is that if I’m right about 1 being male, they had higher concentrations on average than females. Given greatly unequal sample sizes and unclear documentation, we can’t draw many conclusions from this.\n\n# Bar plot for Race\nggplot(fittingdata_final, aes(x = RACE)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Race\", x = \"Race\", y = \"Count\")\n\n\n\n\n\n\n\n# Boxplot for mavoglurant concentration by race\nggplot(fittingdata_final, aes(x = RACE, y = Y)) +\n  geom_boxplot() +\n  labs(title = \"Concentration by Race\", x = \"Race\", y = \"Mavoglurant Concentration\")\n\n\n\n\n\n\n\n\nWe have no idea what this means since we don’t have good documentation on the variables. Next, we’ll look at our WT variable, which stands for weight (kg).\n\n# Histogram for Weight\nggplot(fittingdata_final, aes(x = WT)) +\n  geom_histogram(binwidth = 10) +\n  labs(title = \"Histogram of Weight\", x = \"Weight (kg)\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Weight\nggplot(fittingdata_final, aes(x = WT, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Weight\", x = \"Weight(kg)\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can see that there are more observations of low-mid weight than high weight individuals from our histogram. We can see from our scatterplot that there isn’t a strong correlation between weight and concentration, but it seems like higher weights have lower concentrations on average. Now we can explore HT, which is apparently our height variable. No units were given, so this will be difficult to interpret at best.\n\n# Histogram for Height\nggplot(fittingdata_final, aes(x = HT)) +\n  geom_histogram(binwidth = 0.1) +\n  labs(title = \"Histogram of Height\", x = \"Height\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Height\nggplot(fittingdata_final, aes(x = HT, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Height\", x = \"Height\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can’t tell much from the histogram since we don’t know what unit height is in, but the data seems relatively normally distributed. It is slightly skewed to the right, but not by much. The scatterplot doesn’t show a strong or clear correlation, but on average, it looks like concentration decreased as height increased. Finally, we’ll look at our dose variable, which only has values of 25, 37.5, and 50.\n\n# Bar plot for Dose\nggplot(fittingdata_final, aes(x = DOSE)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Race\", x = \"Dose\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Dose\nggplot(fittingdata_final, aes(x = DOSE, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Dose\", x = \"Dose\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: pseudoinverse used at 24.875\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: neighborhood radius 25.125\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: reciprocal condition number 2.903e-16\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: There are other near singularities as well. 631.27\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : pseudoinverse used at\n24.875\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : neighborhood radius\n25.125\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : reciprocal condition\nnumber 2.903e-16\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : There are other near\nsingularities as well. 631.27\n\n\n\n\n\n\n\n\n\nWe see that there were far fewer 37.5 doses than the others, but according to the scatterplot, there’s a clear trend of increased concentration as the dosage increases. This concludes our EDA; now, we can move into our model fitting.\nWe will now fit a linear model to Y using the main predictor of interest, DOSE. Then, we’ll fit a linear model to Y using all predictors and compare their RMSE and R-squared values. We’ll be using Microsoft Copilot in Precise mode for help with the base code again.\n\n# Split the data into training and testing sets\nfittingdata_split &lt;- initial_split(fittingdata_final, prop = 0.75)\ntrain_data &lt;- training(fittingdata_split)\ntest_data &lt;- testing(fittingdata_split)\n\n# Fit a linear model to the continuous outcome \"Y\" using the main predictor of interest, DOSE\nmodel1_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nmodel1_fit &lt;- model1_spec %&gt;% \n  fit(Y ~ DOSE, data = train_data)\n\n# Fit a linear model to the continuous outcome \"Y\" using all predictors\nmodel2_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nmodel2_fit &lt;- model2_spec %&gt;% \n  fit(Y ~ ., data = train_data)\n\n# Compute RMSE and R-squared for model1\nmodel1_metrics &lt;- model1_fit %&gt;% \n  predict(test_data) %&gt;% \n  bind_cols(test_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\ncat(\"Model 1:\\n\")\n\nModel 1:\n\ncat(\"RMSE: \", model1_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate), \"\\n\")\n\nRMSE:  765.9966 \n\ncat(\"R-squared: \", model1_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(.estimate), \"\\n\\n\")\n\nR-squared:  0.4920651 \n\n# Compute RMSE and R-squared for model2\nmodel2_metrics &lt;- model2_fit %&gt;% \n  predict(test_data) %&gt;% \n  bind_cols(test_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\ncat(\"Model 2:\\n\")\n\nModel 2:\n\ncat(\"RMSE: \", model2_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate), \"\\n\")\n\nRMSE:  720.112 \n\ncat(\"R-squared: \", model2_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(.estimate), \"\\n\")\n\nR-squared:  0.5648099 \n\nprint(model1_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     334.49        56.74  \n\nprint(model2_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ ., data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2        RACE2        RACE7  \n    3513.43        54.61        11.15      -414.49       159.18      -465.48  \n     RACE88           WT           HT  \n    -213.12       -23.94      -824.76  \n\n\nFrom our linear model that only uses DOSE as a predictor, we can see that DOSE is positively correlated with total mavoglurate concentration, which matches up with our EDA plot data. We can tell by looking at the coefficients produced by our models; positive coefficients indicate positive correlation whereas negative indicates negative.\nOur second model shows that dose is positively correlated again. Furthermore, age and race2/88 are both positively correlated too, but the size of the coefficients indicates that age may be a weaker correlation. Sex 2, our females, are strongly negatively correlated with mavoglurate concentration. Race7 and height seem to be very strongly negatively correlated. Finally, weight is negatively correlated, but due to the coefficient size, this doesn’t seem to be a strong relationship.\nOur first model, which only uses DOSE as a predictor, seems to explain a bit more of the variation in the data. The R-squared value is slightly higher (by 0.003). However, the RMSE is also higher, which means that the error of Model 1 is slightly higher than that of model 2.\nNow, we’ll look at how to do a logistic regression model on our data. We’ll use SEX as the outcome since it’s a categorical variable, even though this doesn’t make sense from a science standpoint (it’s just practice). We’ll do the same thing: 1 model for just DOSE, and another for every predictor. Then we’ll produce an ROC-AUC, which just measures performance for the classification problems at various threshold settings. We’ll use Microsoft Copilot in Precise mode for the base code again.\n\n# Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, DOSE\nmodel3_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\nmodel3_fit &lt;- model3_spec %&gt;% \n  fit(SEX ~ DOSE, data = train_data)\n\n# Fit a logistic model to SEX using all predictors\nmodel4_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\nmodel4_fit &lt;- model4_spec %&gt;% \n  fit(SEX ~ ., data = train_data)\n\n# Compute ROC-AUC for \"female\" class for model3\nmodel3_roc_auc_female &lt;- model3_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_2)\n\n# Compute ROC-AUC for \"male\" class for model3\nmodel3_roc_auc_male &lt;- model3_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\n\ncat(\"Model 3:\\n\")\n\nModel 3:\n\ncat(\"ROC-AUC for '2': \", model3_roc_auc_female$.estimate, \"\\n\")\n\nROC-AUC for '2':  0.248 \n\ncat(\"ROC-AUC for '1': \", model3_roc_auc_male$.estimate, \"\\n\\n\")\n\nROC-AUC for '1':  0.752 \n\n# Compute ROC-AUC for \"female\" class for model4\nmodel4_roc_auc_female &lt;- model4_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_2)\n\n# Compute ROC-AUC for \"male\" class for model4\nmodel4_roc_auc_male &lt;- model4_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\n\ncat(\"Model 4:\\n\")\n\nModel 4:\n\ncat(\"ROC-AUC for '2': \", model4_roc_auc_female$.estimate, \"\\n\")\n\nROC-AUC for '2':  0.04 \n\ncat(\"ROC-AUC for '1': \", model4_roc_auc_male$.estimate, \"\\n\")\n\nROC-AUC for '1':  0.96 \n\nprint(model3_fit)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ DOSE, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n   -1.66861     -0.00846  \n\nDegrees of Freedom: 89 Total (i.e. Null);  88 Residual\nNull Deviance:      66.84 \nResidual Deviance: 66.74    AIC: 70.74\n\nprint(model4_fit)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)            Y         DOSE          AGE        RACE2        RACE7  \n  91.069851    -0.002461     0.022110     0.086087    -4.002525    -0.733912  \n     RACE88           WT           HT  \n  -1.846999    -0.089081   -49.170082  \n\nDegrees of Freedom: 89 Total (i.e. Null);  81 Residual\nNull Deviance:      66.84 \nResidual Deviance: 19.39    AIC: 37.39\n\n\nThe coefficients for both models are very different than they were for our linear regression model that had Y as the outcome. We can see that DOSE appears to be negatively correlated with SEX, which in our case would indicate that higher doses mean more males. DOSE is again negatively correlated in our model using every variable as a predictor. Age, weight, Race88 and Race7 are all positively correlated, which means that as these increase we’re more likely to see females. Y, Race2, and height are negatively correlated.\nWe can see that the ROC-AUC value for Model 3 (just dose as a predictor) shows similar performance of the model when predicting both male and female values. Remember that “1” is our males and “2” is our females. Model 4, on the other hand, shows a far stronger ability to accurately predict males than females. This makes sense given that we had so many more observations of males in our data.\nThis set of models isn’t as useful in making any sort of inferences about our data, as the question we asked before creating our model doesn’t make much sense. It’s good practice regardless.\nNow, we’ll prep our data for the next exercise. We’ll set a random seed and begin splitting our data into test/train sets to fit some more models.\n\n#The Race variable is weird; we'll remove it and continue with the exercise\n\nfittingdata_ultimate &lt;- fittingdata_final\n\nfittingdata_ultimate$RACE &lt;- NULL\n\n#set a random seed\n\nset.seed(1234)\n\n# Put 3/4 of the data into the training set \ndata_split &lt;- initial_split(fittingdata_ultimate, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data2 &lt;- training(data_split)\ntest_data2  &lt;- testing(data_split)\n\nNow that we’ve split the dataframe with the RACE variable removed, we can fit two new models. One uses only DOSE as a predictor for Y and one uses all variables. I gave Microsoft Copilot in Precise Mode the following prompt to generate this code and modified it to the specifics of our frame:\n“In R, I have a data frame composed of 6 variables. I’ve split the observations with 75% in a training set and 25% in a testing set. Can you write code that uses the tidymodels framework to fit two linear models to our continuous outcome of interest (Y). The first model should only use DOSE as predictor, the second model should use all predictors. For both models, the metric to optimize should be RMSE. You should only use the training data set for fitting.”\n\n# Specify the model using only DOSE as predictor\nmodel_spec_dose &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# Create a workflow\nworkflow_dose &lt;- workflow() %&gt;% \n  add_model(model_spec_dose) %&gt;% \n  add_formula(Y ~ DOSE)\n\n# Specify the model using all predictors\nmodel_spec_all &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# Create a workflow\nworkflow_all &lt;- workflow() %&gt;% \n  add_model(model_spec_all) %&gt;% \n  add_formula(Y ~ .)\n\n#fit the DOSE model\ndose_fit &lt;- workflow_dose %&gt;% \n  fit(data = train_data2)\n\n#augment to evaluate performance metric\naug_dose&lt;- augment(dose_fit, train_data2)\naug_dose %&gt;% select(Y, .pred)\n\n# A tibble: 90 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 3004. 3207.\n 2 1347. 1871.\n 3 2772. 2539.\n 4 2028. 1871.\n 5 2353. 3207.\n 6  826. 1871.\n 7 3866. 1871.\n 8 3126. 1871.\n 9 1108. 1871.\n10 2815. 2539.\n# ℹ 80 more rows\n\n#get RMSE of DOSE model\nrmsedose&lt;- aug_dose %&gt;% rmse(truth = Y, .pred)\n\n#fit the all predictors model\nall_fit &lt;- workflow_all %&gt;% \n  fit(data = train_data2)\n\n#augment to evaluate performance metric\naug_all&lt;- augment(all_fit, train_data2)\naug_all %&gt;% select(Y, .pred)\n\n# A tibble: 90 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 3004. 3303.\n 2 1347. 1953.\n 3 2772. 2745.\n 4 2028. 2081.\n 5 2353. 2894.\n 6  826. 1265.\n 7 3866. 2429.\n 8 3126. 1976.\n 9 1108. 1561.\n10 2815. 2549.\n# ℹ 80 more rows\n\n#get RMSE of ALL model\nrmseall&lt;- aug_all %&gt;% rmse(truth = Y, .pred)\n\n\n# Print the results\nrmsedose\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        703.\n\nrmseall\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        627.\n\n\nOur second model using every predictor has a lower RMSE. We’ll now compute the RMSE of a null-model (one that would just predict the mean outcome for each observation, without using any predictor information).\n\n# Compute the mean outcome\nmean_outcome &lt;- mean(train_data2$Y, na.rm = TRUE)\n\n# Create a data frame with the predicted values for the null model\npredictions_null &lt;- data.frame(.pred = rep(mean_outcome, nrow(train_data2)))\n\n# Compute RMSE for the null model\nrmse_null &lt;- predictions_null %&gt;% \n  bind_cols(train_data2 %&gt;% select(Y)) %&gt;% \n  yardstick::rmse(Y, .pred)\n\n# Print the RMSE for the null model\nprint(paste(\"RMSE for the null model: \", rmse_null$.estimate))\n\n[1] \"RMSE for the null model:  948.352631392634\"\n\n\nAs the results show us (RMSE of 627), the best fitting model appears to be the one using all variables as predictors for Y. As to be expected, the null model that doesn’t use any predictors has the highest RMSE, indicating that it’s a poor fit. The model usng only DOSE as a predictor has a far lower RMSE (702 compared to 948) but it is higher than our all predictor model. However, we can’t be sure that this isn’t due to overfitting since we’re only using RMSE as our metric. We’ll use cross-validation (CV) as a way to see if these results could be achieved on unseen data. We’ll follow the tidymodels framework again for this code.\n\n#reset the seed\nset.seed(1234)\n\n# Create 10-fold cross-validation splits\ncv_splits &lt;- vfold_cv(train_data2, v = 10)\n\n# Perform cross-validation for the DOSE model\ncv_results_dose &lt;- workflow_dose %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Perform cross-validation for the all predictors model\ncv_results_all &lt;- workflow_all %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Print the RMSE for each model\ncv_results_dose %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    691.    10    67.5 Preprocessor1_Model1\n\ncv_results_all %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    646.    10    64.8 Preprocessor1_Model1\n\n# Run the code again with a different seed\nset.seed(456)\n\n# Create 10-fold cross-validation splits\ncv_splits &lt;- vfold_cv(train_data2, v = 10)\n\n# Perform cross-validation for the DOSE model\ncv_results_dose &lt;- workflow_dose %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Perform cross-validation for the all predictors model\ncv_results_all &lt;- workflow_all %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Print the RMSE for each model\ncv_results_dose %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    689.    10    66.6 Preprocessor1_Model1\n\ncv_results_all %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    630.    10    62.2 Preprocessor1_Model1\n\n\nWe can see that our RMSE value is 690 for our DOSE model and 645 for our all predictors model. The gap between these two is smaller than it was for our previous model evaluation without CV but the all predictors model still appears to fit better. Our second random number seed CV run gives 689 for DOSE and 630 for all predictors; this is very similar to the previous run but our all predictor model gives a slighlty stronger value.\nLooking at the standard error values, we can see that the standard error is lower in both RNG seeds for our all predictor models. These models seem more robust and than the DOSE models overall. We didn’t bother to run CV on the null model again because it doesn’t give much more information. Overall, our patterns seem the same as our initial evaluations indicated.\n\n\n\nThis section added by Xueyan Hu\n\ncreate a plot\n\n# Create a data frame with observed and predicted values from the three models\npredictions_df &lt;- data.frame(\n  Observed = train_data2$Y,  # Observed values\n   Predicted_lm1 = predict(dose_fit, new_data = train_data2),\n   Predicted_lm2 = predict(all_fit, new_data = train_data2)\n  ) \n\n# Bind the predictions from the null model to the predictions data frame\npredictions_df &lt;- bind_cols(predictions_df, predictions_null)\n\nNew names:\n• `.pred` -&gt; `.pred...2`\n• `.pred` -&gt; `.pred...4`\n\n# rename the prediction values\npredictions_df &lt;- predictions_df %&gt;%\n  rename(\"Dose_Model\" = \".pred...2\",\n         \"All_Model\" = \".pred.1\",\n         \"Null_Model\" = \".pred...4\")\n\n# Create a scatter plot using ggplot2\nggplot(predictions_df, aes(x = Observed)) +\n  geom_point(aes(y =  Dose_Model, color = \"Dose_Model\"), shape = 1) +\n  geom_point(aes(y = All_Model, color = \"All_Model\"), shape = 2) +\n  geom_point(aes(y = Null_Model, color = \"Null_Model\"), shape = 3) +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  xlim(0, 5000) + ylim(0, 5000) +  # Set axes limits\n  labs(x = \"Observed\", y = \"Predicted\") +  # Axis labels\n  theme_minimal() +  # Minimal theme\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"black\")) +  # Color for each model\n  guides(color = guide_legend(title = \"Model\"))  # Legend title\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNull model predictions only have mean, so it is a horizontal line. Since dose has 3 levels, dose model also form 3 horizontal lines. All predictor model seem more disperse than other 2.\n\n\nresidual plot\n\n# Calculate residuals for Model 2\npredictions_df$residuals &lt;- predictions_df$All_Model - predictions_df$Observed\n\n# Create a scatter plot of residuals versus predicted for Model 2\nggplot(predictions_df, aes(x = All_Model, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Predicted\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nboobstrap resampling for model 2\n\n# Set seed\nset.seed(1234)\n\n# Create 100 bootstrap samples\nboot_samples &lt;- bootstraps(train_data2, times = 100)\n\n# Function to fit model and make predictions\nfit_model &lt;- function(data) {\n  model &lt;- lm(Y ~ ., data = data)\n  return(predict(model, newdata = train_data2))\n}\n\n# Fit model to each bootstrap sample and make predictions\npred_bs &lt;- lapply(boot_samples$splits, function(split) {\n  fit_data &lt;- analysis(split)\n  fit_model(fit_data)\n})\n\n# Convert the list of predictions to a matrix or array\npred_array &lt;- do.call(cbind, pred_bs)\n\n# Compute median and confidence intervals\npreds &lt;- pred_array |&gt; apply(1, quantile,  c(0.055, 0.5, 0.945)) |&gt;  t()\n\n# Plot observed values versus point estimates and confidence intervals\nobserved &lt;- train_data2$Y\npoint_estimate &lt;- preds[, 2]\nlower_ci &lt;- preds[, 1]\nupper_ci &lt;- preds[, 3]\n\n# Plot\nplot_data &lt;- data.frame(\n  Observed = observed,\n  Point_Estimate = point_estimate,\n  Lower_CI = lower_ci,\n  Upper_CI = upper_ci\n)\n\nggplot(plot_data, aes(x = Observed)) +\n  geom_point(aes(y = Point_Estimate), color = \"blue\", shape = 19) +\n  geom_point(aes(y = Lower_CI), color = \"green\", shape = 19) +\n  geom_point(aes(y = Upper_CI), color = \"red\", shape = 19) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  xlim(0, 5000) + ylim(0, 5000) +\n  labs(x = \"Observed\", \n       y = \"Predicted\",\n       title = \"Observed and Predicted Y by bootstrap resampling with confident intervals\") +\n  theme_minimal() \n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe red line means the predictions perfectly fit the observed outcome. According to the plot, the blue dots(median predicted values) align kind of closely with the dashed red line, which means the model’s predictions are accurate on average. The green and red points (confidence intervals) are kind of narrow and symmetric around the blue points, which indicates high precision and confidence in the model’s predictions. And I don’t think there’s a consistent pattern of points deviating from the dashed red line, so maybe there is no potential issues with the model’s performance or uncertainty in predictions.\n#Exercise 10 Part 3\nBuilding off of Xueyan’s contributions, we will do one final model assessment using the test data we’ve saved for this step. I am again using Microsoft Copilot to help generate code.\n\n# Compute predictions for the test data using the all predictors model\npredictions_test &lt;- predict(all_fit, new_data = test_data2)\n\n# Create a scatter plot using ggplot2\nggplot(predictions_df, aes(x = Observed, y = All_Model)) +\n  geom_point(color = \"blue\") +\n  geom_point(data = data.frame(Observed = test_data2$Y, All_Model = predictions_test$.pred), aes(x = Observed, y = All_Model), color = \"red\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = \"dashed\") +\n  labs(x = \"Observed\", y = \"Predicted\") +\n  theme_minimal() +\n  scale_color_manual(values = c(\"blue\", \"red\"), labels = c(\"Training\", \"Test\")) +\n  guides(color = guide_legend(title = \"Dataset\"))  # Legend title\n\n\n\n\n\n\n\n\nThis seems like a good sign; the test set predictions are mixed well with the train set predictions. We appear to have avoided overfitting. Ultimately, all of our models performed better than the null model, which indicates that our predictors definitely have an influence on our outcome variable of interest. The model with only dose gives better results than the null model but it is heavily biased due to the three distinct dosage levels within the variable. It doesn’t perform as well on our metric of RMSE as the model with all predictors since we likely explain more variation with the other predictors involved. The dose model isn’t useless, as we see that dosage certainly has an impact on our Y variable, but it doesn’t give us as full of a picture as our all predictor model.\nOur all predictor model makes more sense biologically, as we would expect individuals of different age, sex, height, and weight to metabolize mavoglurant at a different rate. Dosage is certainly a huge factor when it comes to the final concentration in each individual, but we would expect to see different kinds of people metabolizing drugs at a different rate. Our uncertainty evaluations of the all predictor model were pretty positive overall with points generally falling around our ideal model. It doesn’t seem to overfit or underfit too much, but it could likely be improved still. The residual plot indicated that we had a lot of under and over predicting, but out of the models we fit this one seems to be the best option. The model definitely seems usable for ballpark predictions of mavoglurant concentration given someone’s age, sex, height, weight, and dosage level. It would likely be improved by adding more observations to the data set in order to train a model that can take more variation into account."
  }
]