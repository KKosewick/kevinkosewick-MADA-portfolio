[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   1   2     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean     sd  p0  p25  p50  p75 p100\n1 Height                  0             1  166.    16.0 133  156  166  178  183\n2 Weight                  0             1   70.1   21.2  45   55   70   80  110\n3 Strands of Hair         0             1 5592.  3534.    1 5000 6000 8178 9122\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▃▁▃▂▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nScatterplot\n\np5 &lt;- mydata %&gt;% ggplot(aes(x = Weight, y = `Strands of Hair`)) +\n  geom_point()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-strands-scatter.png\")\nggsave(filename = figure_file, plot = p5)\n\nSaving 7 x 5 in image\n\n\nBoxplot\n\np6 &lt;- mydata %&gt;% ggplot(aes(x = `Hair Color`, y = Height)) +\n  geom_boxplot()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-color-box.png\")\nggsave(filename = figure_file, plot = p6)\n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "We will be using the dataset found here and made by the nlmixr team for this exercise. The dataset contains pharmacokinetic observations from 120 subjects who were administered IV infusions of mavoglurant. We’ll begin by loading the necessary packages and the dataset.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(here)\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\n# Load the data\nfittingdata &lt;- read.csv(here(\"fitting-exercise\",\"fittingdata.csv\"))\n\n# Check the data\nsummary(fittingdata)\n\n       ID             CMT             EVID              EVI2       \n Min.   :793.0   Min.   :1.000   Min.   :0.00000   Min.   :0.0000  \n 1st Qu.:832.0   1st Qu.:2.000   1st Qu.:0.00000   1st Qu.:0.0000  \n Median :860.0   Median :2.000   Median :0.00000   Median :0.0000  \n Mean   :858.8   Mean   :1.926   Mean   :0.07394   Mean   :0.1613  \n 3rd Qu.:888.0   3rd Qu.:2.000   3rd Qu.:0.00000   3rd Qu.:0.0000  \n Max.   :915.0   Max.   :2.000   Max.   :1.00000   Max.   :4.0000  \n      MDV                DV               LNDV            AMT        \n Min.   :0.00000   Min.   :   0.00   Min.   :0.000   Min.   : 0.000  \n 1st Qu.:0.00000   1st Qu.:  23.52   1st Qu.:3.158   1st Qu.: 0.000  \n Median :0.00000   Median :  74.20   Median :4.306   Median : 0.000  \n Mean   :0.09373   Mean   : 179.93   Mean   :4.085   Mean   : 2.763  \n 3rd Qu.:0.00000   3rd Qu.: 283.00   3rd Qu.:5.645   3rd Qu.: 0.000  \n Max.   :1.00000   Max.   :1730.00   Max.   :7.456   Max.   :50.000  \n      TIME             DOSE            OCC             RATE       \n Min.   : 0.000   Min.   :25.00   Min.   :1.000   Min.   :  0.00  \n 1st Qu.: 0.583   1st Qu.:25.00   1st Qu.:1.000   1st Qu.:  0.00  \n Median : 2.250   Median :37.50   Median :1.000   Median :  0.00  \n Mean   : 5.851   Mean   :37.37   Mean   :1.378   Mean   : 16.55  \n 3rd Qu.: 6.363   3rd Qu.:50.00   3rd Qu.:2.000   3rd Qu.:  0.00  \n Max.   :48.217   Max.   :50.00   Max.   :2.000   Max.   :300.00  \n      AGE            SEX             RACE              WT        \n Min.   :18.0   Min.   :1.000   Min.   : 1.000   Min.   : 56.60  \n 1st Qu.:26.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.: 73.30  \n Median :31.0   Median :1.000   Median : 1.000   Median : 82.60  \n Mean   :32.9   Mean   :1.128   Mean   : 7.415   Mean   : 83.16  \n 3rd Qu.:40.0   3rd Qu.:1.000   3rd Qu.: 2.000   3rd Qu.: 90.60  \n Max.   :50.0   Max.   :2.000   Max.   :88.000   Max.   :115.30  \n       HT       \n Min.   :1.520  \n 1st Qu.:1.710  \n Median :1.780  \n Mean   :1.762  \n 3rd Qu.:1.820  \n Max.   :1.930  \n\n# Create plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose \nggplot(fittingdata, aes(x = TIME, y = DV, color = DOSE, group = ID)) +\n  geom_line() +\n  labs(x = \"Time\", y = \"DV\", color = \"Dose\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can see that the data records a time series measuring concentrations of DV (which stands for Dependent Variable, which is Mavoglurant). Looking at the summary, we can see that OCC has values greater than 1. We don’t know what these mean so we probably shouldn’t use them. We’ll remove all observations with values other than 1.\n\n# Load necessary library\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Filter the data\nfittingdata2 &lt;- fittingdata %&gt;% filter(OCC == 1)\n\nWe now want to compute the sum of DV for each individual to determine the full amount of drug for each individual. I understand that according to the exercise details this is not the best approach, but this is mainly for practice anyways. I consulted Microsoft Copilot for help with this using this prompt (which is also the same as the instructions laid out in the exercise explanation): “Write code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18.”\n\n# Exclude observations with TIME = 0 and compute the sum of DV for each individual\nfittingdata_sum &lt;- fittingdata2 %&gt;%\n  filter(TIME != 0) %&gt;%\n  group_by(ID) %&gt;%\n  summarize(Y = sum(DV))\n\n# Create a data frame that contains only the observations where TIME == 0\nfittingdata_time0 &lt;- fittingdata2 %&gt;%\n  filter(TIME == 0)\n\n# Use the appropriate join function to combine those two data frames\nfittingdata_combined &lt;- left_join(fittingdata_time0, fittingdata_sum, by = \"ID\")\n\nWe’ve created a new data frame that contains columns that are much easier to analyze now. We’ll do some final cleaning steps by converting RACE and SEX to factors and removing some columns that we no longer need.\n\n# Convert RACE and SEX to factor variables and keep only variables specified in the exercise instructions\nfittingdata_final &lt;- fittingdata_combined %&gt;%\n  mutate(RACE = as.factor(RACE),\n         SEX = as.factor(SEX)) %&gt;%\n  select(Y, DOSE, AGE, SEX, RACE, WT, HT)\n# Check data to make sure everything is good\nsummary(fittingdata_final)\n\n       Y               DOSE            AGE        SEX     RACE   \n Min.   : 826.4   Min.   :25.00   Min.   :18.00   1:104   1 :74  \n 1st Qu.:1700.5   1st Qu.:25.00   1st Qu.:26.00   2: 16   2 :36  \n Median :2349.1   Median :37.50   Median :31.00           7 : 2  \n Mean   :2445.4   Mean   :36.46   Mean   :33.00           88: 8  \n 3rd Qu.:3050.2   3rd Qu.:50.00   3rd Qu.:40.25                  \n Max.   :5606.6   Max.   :50.00   Max.   :50.00                  \n       WT               HT       \n Min.   : 56.60   Min.   :1.520  \n 1st Qu.: 73.17   1st Qu.:1.700  \n Median : 82.10   Median :1.770  \n Mean   : 82.55   Mean   :1.759  \n 3rd Qu.: 90.10   3rd Qu.:1.813  \n Max.   :115.30   Max.   :1.930  \n\nclass(fittingdata_final$RACE)\n\n[1] \"factor\"\n\n\nWe’ll begin a formal EDA now. We’re interested in how each of the variables influences our outcome variable that we created, “Y”. Again, this is the sum per individual of all of our original “DV” values. Before we begin, we should note that the documentation for this dataset is not very good. We don’t know what the values in RACE or SEX indicate, so interpreting results from the EDA will be challenging for these. According to the study this is based off of, 86% of participants were male, so we can assume that a value of 1 is male and 2 is female (based off of the frequency of these values in the dataset). We’ll generate plots for them regardless. First up is our AGE variable.\n\n# Load required package\nlibrary(ggplot2)\n\n# Histogram for Age\nggplot(fittingdata_final, aes(x = AGE)) +\n  geom_histogram(binwidth = 10) +\n  labs(title = \"Histogram of Age\", x = \"Age (years)\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Age\nggplot(fittingdata_final, aes(x = AGE, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Age\", x = \"Age (years)\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOur age values seem to have a relatively normal distribution with a minimum of 18 and maximum of 50. Our scatterplot shows that mavoglurant concentrations seem to remain the same on average between individuals of different ages. The plot shows no clear correlation one way or the other. Next, we’ll investigate SEX.\n\n# Boxplot for mavoglurant concentration by sex\nggplot(fittingdata_final, aes(x = SEX, y = Y)) +\n  geom_boxplot() +\n  labs(title = \"Concentration by Sex\", x = \"Sex\", y = \"Mavoglurant Concentration\")\n\n\n\n\n\n\n\n\nIf we knew what our dataset’s values meant or had clear documentation somewhere, we could interpret these results with certainty. Instead, all we can say is that if I’m right about 1 being male, they had higher concentrations on average than females. Given greatly unequal sample sizes and unclear documentation, we can’t draw many conclusions from this.\n\n# Bar plot for Race\nggplot(fittingdata_final, aes(x = RACE)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Race\", x = \"Race\", y = \"Count\")\n\n\n\n\n\n\n\n# Boxplot for mavoglurant concentration by race\nggplot(fittingdata_final, aes(x = RACE, y = Y)) +\n  geom_boxplot() +\n  labs(title = \"Concentration by Race\", x = \"Race\", y = \"Mavoglurant Concentration\")\n\n\n\n\n\n\n\n\nWe have no idea what this means since we don’t have good documentation on the variables. Next, we’ll look at our WT variable, which stands for weight (kg).\n\n# Histogram for Weight\nggplot(fittingdata_final, aes(x = WT)) +\n  geom_histogram(binwidth = 10) +\n  labs(title = \"Histogram of Weight\", x = \"Weight (kg)\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Weight\nggplot(fittingdata_final, aes(x = WT, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Weight\", x = \"Weight(kg)\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can see that there are more observations of low-mid weight than high weight individuals from our histogram. We can see from our scatterplot that there isn’t a strong correlation between weight and concentration, but it seems like higher weights have lower concentrations on average. Now we can explore HT, which is apparently our height variable. No units were given, so this will be difficult to interpret at best.\n\n# Histogram for Height\nggplot(fittingdata_final, aes(x = HT)) +\n  geom_histogram(binwidth = 0.1) +\n  labs(title = \"Histogram of Height\", x = \"Height\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Height\nggplot(fittingdata_final, aes(x = HT, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Height\", x = \"Height\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can’t tell much from the histogram since we don’t know what unit height is in, but the data seems relatively normally distributed. It is slightly skewed to the right, but not by much. The scatterplot doesn’t show a strong or clear correlation, but on average, it looks like concentration decreased as height increased. Finally, we’ll look at our dose variable, which only has values of 25, 37.5, and 50.\n\n# Bar plot for Dose\nggplot(fittingdata_final, aes(x = DOSE)) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Race\", x = \"Dose\", y = \"Count\")\n\n\n\n\n\n\n\n# Scatterplot for Y by Dose\nggplot(fittingdata_final, aes(x = DOSE, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\") +\n  labs(title = \"Mavoglurant Concentration by Dose\", x = \"Dose\", y = \"Mavoglurant Concentration\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: pseudoinverse used at 24.875\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: neighborhood radius 25.125\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: reciprocal condition number 2.903e-16\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n: There are other near singularities as well. 631.27\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : pseudoinverse used at\n24.875\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : neighborhood radius\n25.125\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : reciprocal condition\nnumber 2.903e-16\n\n\nWarning in predLoess(object$y, object$x, newx = if (is.null(newdata)) object$x\nelse if (is.data.frame(newdata))\nas.matrix(model.frame(delete.response(terms(object)), : There are other near\nsingularities as well. 631.27\n\n\n\n\n\n\n\n\n\nWe see that there were far fewer 37.5 doses than the others, but according to the scatterplot, there’s a clear trend of increased concentration as the dosage increases. This concludes our EDA; now, we can move into our model fitting.\nWe will now fit a linear model to Y using the main predictor of interest, DOSE. Then, we’ll fit a linear model to Y using all predictors and compare their RMSE and R-squared values. We’ll be using Microsoft Copilot in Precise mode for help with the base code again.\n\n# Load necessary libraries\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ infer        1.0.5     ✔ tidyr        1.3.1\n✔ modeldata    1.2.0     ✔ tune         1.1.2\n✔ parsnip      1.2.0     ✔ workflows    1.1.3\n✔ purrr        1.0.2     ✔ workflowsets 1.0.1\n✔ recipes      1.0.9     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\n# Split the data into training and testing sets\nfittingdata_split &lt;- initial_split(fittingdata_final, prop = 0.75)\ntrain_data &lt;- training(fittingdata_split)\ntest_data &lt;- testing(fittingdata_split)\n\n# Fit a linear model to the continuous outcome \"Y\" using the main predictor of interest, DOSE\nmodel1_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nmodel1_fit &lt;- model1_spec %&gt;% \n  fit(Y ~ DOSE, data = train_data)\n\n# Fit a linear model to the continuous outcome \"Y\" using all predictors\nmodel2_spec &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nmodel2_fit &lt;- model2_spec %&gt;% \n  fit(Y ~ ., data = train_data)\n\n# Compute RMSE and R-squared for model1\nmodel1_metrics &lt;- model1_fit %&gt;% \n  predict(test_data) %&gt;% \n  bind_cols(test_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\ncat(\"Model 1:\\n\")\n\nModel 1:\n\ncat(\"RMSE: \", model1_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate), \"\\n\")\n\nRMSE:  720.7356 \n\ncat(\"R-squared: \", model1_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(.estimate), \"\\n\\n\")\n\nR-squared:  0.4940505 \n\n# Compute RMSE and R-squared for model2\nmodel2_metrics &lt;- model2_fit %&gt;% \n  predict(test_data) %&gt;% \n  bind_cols(test_data) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\ncat(\"Model 2:\\n\")\n\nModel 2:\n\ncat(\"RMSE: \", model2_metrics %&gt;% filter(.metric == \"rmse\") %&gt;% pull(.estimate), \"\\n\")\n\nRMSE:  771.4353 \n\ncat(\"R-squared: \", model2_metrics %&gt;% filter(.metric == \"rsq\") %&gt;% pull(.estimate), \"\\n\")\n\nR-squared:  0.4507647 \n\nprint(model1_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n     359.28        57.76  \n\nprint(model2_fit)\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = Y ~ ., data = data)\n\nCoefficients:\n(Intercept)         DOSE          AGE         SEX2        RACE2        RACE7  \n   3085.189       61.001        6.431     -306.471      347.215     -450.254  \n     RACE88           WT           HT  \n   -158.188      -28.782     -404.702  \n\n\nFrom our linear model that only uses DOSE as a predictor, we can see that DOSE is positively correlated with total mavoglurate concentration, which matches up with our EDA plot data. We can tell by looking at the coefficients produced by our models; positive coefficients indicate positive correlation whereas negative indicates negative.\nOur second model shows that dose is positively correlated again. Furthermore, age and race2/88 are both positively correlated too, but the size of the coefficients indicates that age may be a weaker correlation. Sex 2, our females, are strongly negatively correlated with mavoglurate concentration. Race7 and height seem to be very strongly negatively correlated. Finally, weight is negatively correlated, but due to the coefficient size, this doesn’t seem to be a strong relationship.\nOur first model, which only uses DOSE as a predictor, seems to explain a bit more of the variation in the data. The R-squared value is slightly higher (by 0.003). However, the RMSE is also higher, which means that the error of Model 1 is slightly higher than that of model 2.\nNow, we’ll look at how to do a logistic regression model on our data. We’ll use SEX as the outcome since it’s a categorical variable, even though this doesn’t make sense from a science standpoint (it’s just practice). We’ll do the same thing: 1 model for just DOSE, and another for every predictor. Then we’ll produce an ROC-AUC, which just measures performance for the classification problems at various threshold settings. We’ll use Microsoft Copilot in Precise mode for the base code again.\n\n# Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, DOSE\nmodel3_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\nmodel3_fit &lt;- model3_spec %&gt;% \n  fit(SEX ~ DOSE, data = train_data)\n\n# Fit a logistic model to SEX using all predictors\nmodel4_spec &lt;- logistic_reg() %&gt;% \n  set_engine(\"glm\") %&gt;% \n  set_mode(\"classification\")\n\nmodel4_fit &lt;- model4_spec %&gt;% \n  fit(SEX ~ ., data = train_data)\n\n# Compute ROC-AUC for \"female\" class for model3\nmodel3_roc_auc_female &lt;- model3_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_2)\n\n# Compute ROC-AUC for \"male\" class for model3\nmodel3_roc_auc_male &lt;- model3_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\n\ncat(\"Model 3:\\n\")\n\nModel 3:\n\ncat(\"ROC-AUC for '2': \", model3_roc_auc_female$.estimate, \"\\n\")\n\nROC-AUC for '2':  0.4107143 \n\ncat(\"ROC-AUC for '1': \", model3_roc_auc_male$.estimate, \"\\n\\n\")\n\nROC-AUC for '1':  0.5892857 \n\n# Compute ROC-AUC for \"female\" class for model4\nmodel4_roc_auc_female &lt;- model4_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_2)\n\n# Compute ROC-AUC for \"male\" class for model4\nmodel4_roc_auc_male &lt;- model4_fit %&gt;%\n  predict(test_data, type = \"prob\") %&gt;%\n  bind_cols(test_data) %&gt;%\n  roc_auc(truth = SEX, .pred_1)\n\ncat(\"Model 4:\\n\")\n\nModel 4:\n\ncat(\"ROC-AUC for '2': \", model4_roc_auc_female$.estimate, \"\\n\")\n\nROC-AUC for '2':  0 \n\ncat(\"ROC-AUC for '1': \", model4_roc_auc_male$.estimate, \"\\n\")\n\nROC-AUC for '1':  1 \n\nprint(model3_fit)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ DOSE, family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)         DOSE  \n   -0.54717     -0.03258  \n\nDegrees of Freedom: 89 Total (i.e. Null);  88 Residual\nNull Deviance:      77.8 \nResidual Deviance: 76.18    AIC: 80.18\n\nprint(model4_fit)\n\nparsnip model object\n\n\nCall:  stats::glm(formula = SEX ~ ., family = stats::binomial, data = data)\n\nCoefficients:\n(Intercept)            Y         DOSE          AGE        RACE2        RACE7  \n   58.75086     -0.00105     -0.02966      0.05492     -2.22057      0.08128  \n     RACE88           WT           HT  \n   -1.99691     -0.07814    -30.78496  \n\nDegrees of Freedom: 89 Total (i.e. Null);  81 Residual\nNull Deviance:      77.8 \nResidual Deviance: 29.16    AIC: 47.16\n\n\nThe coefficients for both models are very different than they were for our linear regression model that had Y as the outcome. We can see that DOSE appears to be negatively correlated with SEX, which in our case would indicate that higher doses mean more males. DOSE is again negatively correlated in our model using every variable as a predictor. Age, weight, Race88 and Race7 are all positively correlated, which means that as these increase we’re more likely to see females. Y, Race2, and height are negatively correlated.\nWe can see that the ROC-AUC value for Model 3 (just dose as a predictor) shows similar performance of the model when predicting both male and female values. Remember that “1” is our males and “2” is our females. Model 4, on the other hand, shows a far stronger ability to accurately predict males than females. This makes sense given that we had so many more observations of males in our data.\nThis set of models isn’t as useful in making any sort of inferences about our data, as the question we asked before creating our model doesn’t make much sense. It’s good practice regardless.\nNow, we’ll prep our data for the next exercise. We’ll set a random seed and begin splitting our data into test/train sets to fit some more models.\n\n#The Race variable is weird; we'll remove it and continue with the exercise\n\nfittingdata_ultimate &lt;- fittingdata_final\n\nfittingdata_ultimate$RACE &lt;- NULL\n\n#set a random seed\n\nset.seed(1234)\n\n# Put 3/4 of the data into the training set \ndata_split &lt;- initial_split(fittingdata_ultimate, prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data2 &lt;- training(data_split)\ntest_data2  &lt;- testing(data_split)\n\nNow that we’ve split the dataframe with the RACE variable removed, we can fit two new models. One uses only DOSE as a predictor for Y and one uses all variables. I gave Microsoft Copilot in Precise Mode the following prompt to generate this code and modified it to the specifics of our frame:\n“In R, I have a data frame composed of 6 variables. I’ve split the observations with 75% in a training set and 25% in a testing set. Can you write code that uses the tidymodels framework to fit two linear models to our continuous outcome of interest (Y). The first model should only use DOSE as predictor, the second model should use all predictors. For both models, the metric to optimize should be RMSE. You should only use the training data set for fitting.”\n\n# Specify the model using only DOSE as predictor\nmodel_spec_dose &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# Create a workflow\nworkflow_dose &lt;- workflow() %&gt;% \n  add_model(model_spec_dose) %&gt;% \n  add_formula(Y ~ DOSE)\n\n# Specify the model using all predictors\nmodel_spec_all &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# Create a workflow\nworkflow_all &lt;- workflow() %&gt;% \n  add_model(model_spec_all) %&gt;% \n  add_formula(Y ~ .)\n\n#fit the DOSE model\ndose_fit &lt;- workflow_dose %&gt;% \n  fit(data = train_data2)\n\n#augment to evaluate performance metric\naug_dose&lt;- augment(dose_fit, train_data2)\naug_dose %&gt;% select(Y, .pred)\n\n# A tibble: 90 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 3004. 3207.\n 2 1347. 1871.\n 3 2772. 2539.\n 4 2028. 1871.\n 5 2353. 3207.\n 6  826. 1871.\n 7 3866. 1871.\n 8 3126. 1871.\n 9 1108. 1871.\n10 2815. 2539.\n# ℹ 80 more rows\n\n#get RMSE of DOSE model\nrmsedose&lt;- aug_dose %&gt;% rmse(truth = Y, .pred)\n\n#fit the all predictors model\nall_fit &lt;- workflow_all %&gt;% \n  fit(data = train_data2)\n\n#augment to evaluate performance metric\naug_all&lt;- augment(all_fit, train_data2)\naug_all %&gt;% select(Y, .pred)\n\n# A tibble: 90 × 2\n       Y .pred\n   &lt;dbl&gt; &lt;dbl&gt;\n 1 3004. 3303.\n 2 1347. 1953.\n 3 2772. 2745.\n 4 2028. 2081.\n 5 2353. 2894.\n 6  826. 1265.\n 7 3866. 2429.\n 8 3126. 1976.\n 9 1108. 1561.\n10 2815. 2549.\n# ℹ 80 more rows\n\n#get RMSE of ALL model\nrmseall&lt;- aug_all %&gt;% rmse(truth = Y, .pred)\n\n\n# Print the results\nrmsedose\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        703.\n\nrmseall\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        627.\n\n\nOur second model using every predictor has a lower RMSE. We’ll now compute the RMSE of a null-model (one that would just predict the mean outcome for each observation, without using any predictor information).\n\n# Compute the mean outcome\nmean_outcome &lt;- mean(train_data2$Y, na.rm = TRUE)\n\n# Create a data frame with the predicted values for the null model\npredictions_null &lt;- data.frame(.pred = rep(mean_outcome, nrow(train_data2)))\n\n# Compute RMSE for the null model\nrmse_null &lt;- predictions_null %&gt;% \n  bind_cols(train_data2 %&gt;% select(Y)) %&gt;% \n  yardstick::rmse(Y, .pred)\n\n# Print the RMSE for the null model\nprint(paste(\"RMSE for the null model: \", rmse_null$.estimate))\n\n[1] \"RMSE for the null model:  948.352631392634\"\n\n\nAs the results show us (RMSE of 627), the best fitting model appears to be the one using all variables as predictors for Y. As to be expected, the null model that doesn’t use any predictors has the highest RMSE, indicating that it’s a poor fit. The model usng only DOSE as a predictor has a far lower RMSE (702 compared to 948) but it is higher than our all predictor model. However, we can’t be sure that this isn’t due to overfitting since we’re only using RMSE as our metric. We’ll use cross-validation (CV) as a way to see if these results could be achieved on unseen data. We’ll follow the tidymodels framework again for this code.\n\n#reset the seed\nset.seed(1234)\n\n# Create 10-fold cross-validation splits\ncv_splits &lt;- vfold_cv(train_data2, v = 10)\n\n# Perform cross-validation for the DOSE model\ncv_results_dose &lt;- workflow_dose %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Perform cross-validation for the all predictors model\ncv_results_all &lt;- workflow_all %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Print the RMSE for each model\ncv_results_dose %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    691.    10    67.5 Preprocessor1_Model1\n\ncv_results_all %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    646.    10    64.8 Preprocessor1_Model1\n\n# Run the code again with a different seed\nset.seed(456)\n\n# Create 10-fold cross-validation splits\ncv_splits &lt;- vfold_cv(train_data2, v = 10)\n\n# Perform cross-validation for the DOSE model\ncv_results_dose &lt;- workflow_dose %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Perform cross-validation for the all predictors model\ncv_results_all &lt;- workflow_all %&gt;% \n  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))\n\n# Print the RMSE for each model\ncv_results_dose %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    689.    10    66.6 Preprocessor1_Model1\n\ncv_results_all %&gt;% collect_metrics()\n\n# A tibble: 1 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    630.    10    62.2 Preprocessor1_Model1\n\n\nWe can see that our RMSE value is 690 for our DOSE model and 645 for our all predictors model. The gap between these two is smaller than it was for our previous model evaluation without CV but the all predictors model still appears to fit better. Our second random number seed CV run gives 689 for DOSE and 630 for all predictors; this is very similar to the previous run but our all predictor model gives a slighlty stronger value.\nLooking at the standard error values, we can see that the standard error is lower in both RNG seeds for our all predictor models. These models seem more robust and than the DOSE models overall. We didn’t bother to run CV on the null model again because it doesn’t give much more information. Overall, our patterns seem the same as our initial evaluations indicated."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Kosewick’s website and data analysis portfolio",
    "section": "",
    "text": "Hello!\n\nAnd welcome to my website and data analysis portfolio.\nHere I’ll be compiling my projects for my MADA class in the Spring 2024 semester.\n\nFeel free to use the Menu Bar above to learn more about me and my work up to this point. Thank you for looking!\nLast updated 1/26/2024"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise, we will generate a synthetic data set that simulates foraging trends of the Northern Mockingbird somewhere in the United States. We will then generate some figures and statistical analyses to explore our new data set. I’ll be using Microsoft Edge AI tools to help generate code. Let’s begin by loading helpful packages for creating synthetic data sets.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ tibble  3.2.1\n✔ readr   2.1.4     ✔ tidyr   1.3.1\n✔ stringr 1.5.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nI’ve entered this prompt into Microsoft Edge copilot: “Write R code that creates data for N=100 Northern Mockingbirds. Individuals ages are randomly distributed between 1 and 14 years. Assume that individuals forage for two types of food: insects and plants. Individuals can consume a combination of either food type at an average of half an ounce daily. Food consumption is measured in ounces every other day. Assume that insects are more plentiful in warmer seasons; this should be reflected in the amount of each food type consumed in each season. The data should be collected over the course of a year.\nCreate a data frame that contains Food Type, Age, Daily Food Consumption (in ounces), Season, and Sex.\nFor function calls, specify explicitly the package from which the function comes from.\nAdd thorough documentation to the code so it is clear what each line of code does.”\nThis generated the following code:\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years\nage &lt;- sample(1:14, N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\"] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\"] * \n  rep(insect_proportion, each = N/2)[data$Season[data$Food_Type == \"insects\"]]\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                     NA spring female\n 2 insects      14                     NA spring male  \n 3 insects       3                     NA spring female\n 4 insects      10                     NA spring male  \n 5 insects       2                     NA spring female\n 6 insects       6                     NA spring male  \n 7 insects      11                     NA spring male  \n 8 insects       5                     NA spring male  \n 9 insects       4                     NA spring female\n10 insects      14                     NA spring female\n# ℹ 90 more rows\n\n\nThis is a good start, but there are some problems. First, the AI only gave values of NA to food consumption data collected in the spring and summer. Second, there are an abnormally high number of old birds in the population; in a normal wild population, we’d expect to see more mockingbirds in the 2-8 year range, as 14 is on the old end for a mockingbird. We’ll have the AI correct the data frame with the following prompt:\n“Northern Mockingbirds have an average lifespan of 8 years in the wild. Young and elderly individuals have a higher mortality rate. Given this information, please adjust the age variable to more accurately reflect age distribution in a wild population of mockingbirds. Furthermore, mockingbirds consume more food in the spring and summer due to increased energy demands that come with mating and nesting. Please replace the NA food consumption values you gave with randomized values that have an average of 0.5.”\nThe AI then adjusted our previous code string as follows:\n\n# Set the seed for reproducibility\n#set.seed(123)\n\n# Define the number of birds\n#N &lt;- 100\n\n# Generate random ages between 1 and 14 years\n#age &lt;- sample(1:14, N, replace = TRUE, prob = c(0.05, 0.05, 0.1, 0.15, 0.2, 0.15, 0.1, 0.05, 0.025, 0.025))\n\n# Define the two types of food\n#food_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\n#food_consumption &lt;- 0.5\n\n# Define the seasons\n#seasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\n#insect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\n#sex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\n#data &lt;- tibble(\n # Food_Type = rep(food_type, each = N/2),\n# Age = age,\n # Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n # Season = rep(seasons, each = N/4),\n # Sex = sex\n# )\n\n# Adjust the food consumption based on the season\n#data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] &lt;- \n # data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] * \n # rep(insect_proportion[1:2], each = sum(data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")))\n\n# Replace the NA values with randomized values that have an average of 0.5\n#data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), 0.5, 0.1)))\n\n# Print the data\n#data\n\nThis generates a new error in the way the AI created values for the age variable. I gave the AI this prompt to correct the broken lines:\n“The issue with NA values has been solved. I now need you to correct the line where you generate random ages between 1 and 14 years to resolve this error: Error in sample.int(length(x), size, replace, prob) : incorrect number of probabilities”\nThe following code was generated:\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years with a probability distribution that reflects the higher mortality rate of young and elderly individuals\nage &lt;- sample(c(1:4, 5:8, 9:12, 13:14), N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")] * \n  rep(insect_proportion[1:2], each = sum(data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\")))\n\nWarning in data$Daily_Food_Consumption[data$Food_Type == \"insects\" &\n(data$Season == : number of items to replace is not a multiple of replacement\nlength\n\n# Replace the NA values with randomized values that have an average of 0.5\nif (any(is.na(data$Daily_Food_Consumption))) {\n  data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), mean(Daily_Food_Consumption), 0.1)))\n}\n\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.286 spring female\n 2 insects      14                  0.384 spring male  \n 3 insects       3                  0.406 spring female\n 4 insects      10                  0.329 spring male  \n 5 insects       2                  0.284 spring female\n 6 insects       6                  0.309 spring male  \n 7 insects      11                  0.383 spring male  \n 8 insects       5                  0.289 spring male  \n 9 insects       4                  0.206 spring female\n10 insects      14                  0.284 spring female\n# ℹ 90 more rows\n\n\nWe seem to have finally generated a data set that should reflect associations we expect to see in the population. However, I’d like to make one more adjustment that reflects increased food consumption by females in the spring and summer due to reproductive demands. I’ll use a 1.75 times increase in food consumption, as data is limited on the exact amounts/averages mockingbirds consume.\n\n# Set the seed for reproducibility\nset.seed(123)\n\n# Define the number of birds\nN &lt;- 100\n\n# Generate random ages between 1 and 14 years with a probability distribution that reflects the higher mortality rate of young and elderly individuals\nage &lt;- sample(c(1:4, 5:8, 9:12, 13:14), N, replace = TRUE)\n\n# Define the two types of food\nfood_type &lt;- c(\"insects\", \"plants\")\n\n# Define the average daily food consumption in ounces\nfood_consumption &lt;- 0.5\n\n# Define the seasons\nseasons &lt;- c(\"spring\", \"summer\", \"fall\", \"winter\")\n\n# Define the proportion of insects consumed in each season\ninsect_proportion &lt;- c(0.6, 0.8, 0.4, 0.2)\n\n# Define the sex\nsex &lt;- sample(c(\"male\", \"female\"), N, replace = TRUE)\n\n# Generate the data\ndata &lt;- tibble(\n  Food_Type = rep(food_type, each = N/2),\n  Age = age,\n  Daily_Food_Consumption = rnorm(N, food_consumption, 0.1),\n  Season = rep(seasons, each = N/4),\n  Sex = sex\n)\n\n# Adjust the food consumption based on the season and sex\ndata$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\") & data$Sex == \"female\"] &lt;- \n  data$Daily_Food_Consumption[data$Food_Type == \"insects\" & (data$Season == \"spring\" | data$Season == \"summer\") & data$Sex == \"female\"] * \n  1.75\n\n# Replace the NA values with randomized values that have an average of 0.5\nif (any(is.na(data$Daily_Food_Consumption))) {\n  data &lt;- data %&gt;% mutate(Daily_Food_Consumption = replace_na(Daily_Food_Consumption, rnorm(sum(is.na(Daily_Food_Consumption)), mean(Daily_Food_Consumption), 0.1)))\n}\n\n\n# Print the data\ndata\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.835 spring female\n 2 insects      14                  0.640 spring male  \n 3 insects       3                  1.18  spring female\n 4 insects      10                  0.549 spring male  \n 5 insects       2                  0.828 spring female\n 6 insects       6                  0.515 spring male  \n 7 insects      11                  0.638 spring male  \n 8 insects       5                  0.482 spring male  \n 9 insects       4                  0.601 spring female\n10 insects      14                  0.829 spring female\n# ℹ 90 more rows\n\n\nThis dataset looks a lot better. We’ll now check the structure and summary to get a better idea of what we created.\n\n#check the structure and summary\nsummary(data)\n\n  Food_Type              Age        Daily_Food_Consumption    Season         \n Length:100         Min.   : 1.00   Min.   :0.2652         Length:100        \n Class :character   1st Qu.: 5.00   1st Qu.:0.4490         Class :character  \n Mode  :character   Median : 8.50   Median :0.5660         Mode  :character  \n                    Mean   : 7.92   Mean   :0.6026                           \n                    3rd Qu.:11.00   3rd Qu.:0.6929                           \n                    Max.   :14.00   Max.   :1.2025                           \n     Sex           \n Length:100        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\nstructure(data)\n\n# A tibble: 100 × 5\n   Food_Type   Age Daily_Food_Consumption Season Sex   \n   &lt;chr&gt;     &lt;int&gt;                  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1 insects       3                  0.835 spring female\n 2 insects      14                  0.640 spring male  \n 3 insects       3                  1.18  spring female\n 4 insects      10                  0.549 spring male  \n 5 insects       2                  0.828 spring female\n 6 insects       6                  0.515 spring male  \n 7 insects      11                  0.638 spring male  \n 8 insects       5                  0.482 spring male  \n 9 insects       4                  0.601 spring female\n10 insects      14                  0.829 spring female\n# ℹ 90 more rows\n\n\nOur data looks good and reflects the averages we had the AI incorporate when creating our values. We’ll now create a few plots looking at some relationships in the data.\n\n#create a plot with food consumption as a function of season for all individuals (stratified by sex)\nggplot(data, aes(x = Season, y = Daily_Food_Consumption, color = Sex)) +\n  geom_boxplot() +\n  labs(title = \"Food Consumption as a Function of Season for All Individuals, Stratified by Sex\",\n       x = \"Season\",\n       y = \"Daily Food Consumption (oz)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe boxplot shows that our assumptions are reflected in the data set; females consume more in the spring and summer and our average food consumption is 0.5 ounces a day. Now we’ll see if the data accurately shows changes in the primary type of food consumed over the seasons.\n\n#create a histogram comparing food type consumed in different seasons stratified by sex\nggplot(data, aes(x = Season, fill = Food_Type)) +\n  geom_bar(position = \"dodge\", stat = \"count\") +\n  facet_grid(. ~ Sex) +\n  labs(title = \"Food Type Comparison Across Seasons, Stratified by Sex\",\n       x = \"Season\",\n       y = \"Count\",\n       fill = \"Food Type\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThe plots show that insects are the predominant food source in the spring and summer while plants dominate the winter and fall. This accurately reflects booms in the insect population in spring and summer; Northern Mockingbirds consume over 85% insects then, dropping to around 15% in the colder seasons. Now that we see our data is tidy, reflects our assumptions and follows the trends we identified, we can fit the data to some linear models.\n\n#create a linear model with season and sex as predictors for food consumption\nmodel1&lt;- lm(Daily_Food_Consumption~ Season + Sex, data = data)\nsummary(model1)\n\n\nCall:\nlm(formula = Daily_Food_Consumption ~ Season + Sex, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.27214 -0.11026 -0.01652  0.08739  0.35195 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.58657    0.03355  17.483  &lt; 2e-16 ***\nSeasonspring  0.26398    0.04267   6.187 1.54e-08 ***\nSeasonsummer  0.11562    0.04261   2.713  0.00791 ** \nSeasonwinter -0.03203    0.04288  -0.747  0.45690    \nSexmale      -0.16488    0.03079  -5.354 5.95e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1506 on 95 degrees of freedom\nMultiple R-squared:  0.4768,    Adjusted R-squared:  0.4548 \nF-statistic: 21.65 on 4 and 95 DF,  p-value: 1.023e-12\n\n\nIt seems like winter doesn’t have a significant impact on food consumption, but spring and summer do in the created dataset. Now we’ll move on to make a couple more models.\n\n#create a linear model with season as a predictor for food type\n# model2&lt;- lm(Food_Type ~ Season, data = data)\n# summary(model2)\n\nA linear regression didn’t work for this type of data.The above line generated an error message. After consulting with AI, a multinomial logistic regression model would work better. We’ll also include sex as a predictor in this one.\n\n#load the nnet package for this model\nlibrary(nnet)\n# Create a multinomial logistic regression model with season and sex as predictors for food type\nmodel2 &lt;- multinom(Food_Type ~ Season + Sex, data = data)\n\n# weights:  6 (5 variable)\ninitial  value 69.314718 \niter  10 value 0.021604\niter  20 value 0.012091\niter  30 value 0.000938\niter  40 value 0.000662\niter  50 value 0.000442\niter  60 value 0.000273\niter  70 value 0.000224\niter  80 value 0.000159\niter  90 value 0.000153\niter 100 value 0.000122\nfinal  value 0.000122 \nstopped after 100 iterations\n\nsummary(model2)\n\nCall:\nmultinom(formula = Food_Type ~ Season + Sex, data = data)\n\nCoefficients:\n                  Values Std. Err.\n(Intercept)   13.3268743  186.7045\nSeasonspring -26.8167895  232.8969\nSeasonsummer -26.8610564  236.0821\nSeasonwinter  16.4198276    0.0000\nSexmale        0.5251443  186.4204\n\nResidual Deviance: 0.0002439321 \nAIC: 10.00024 \n\n\nIt seems that the model agrees with our assumptions. The odds of mockingbirds choosing plants over insects are lower in the spring and summer and the opposite in winter. The residual deviance is low, indicating a good fit. We’ve created a pretty good dataset that has the associations and trends we wanted to see."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Load the dslabs package. Install if you haven’t. Then, inspect the gapminder dataset.\n\n#load dslabs package and tidyverse\nlibrary(dslabs)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n#look at help file for gapminder data\nhelp(gapminder)\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data \nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nCreate a new object that contains only the African countries. Then, check the structure and summary of the new object.\n\n#create the object with only African countries\nafrican_countries &lt;- gapminder[gapminder$continent == \"Africa\", ]\n#check the structure and summary\nstr(african_countries)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(african_countries)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nNow, using the new African countries object, create two new objects. One should only contain “infant_mortality” and “life_expectancy” and the other should only hold “population” and “life_expectancy”.\n\n#create the object with only population and life expectancy data\nafrican_countries_pop_life &lt;- african_countries[, c(\"population\", \"life_expectancy\")]\n#create the object with only infant mortality and life expectancy data\nafrican_countries_infant_life &lt;- african_countries[, c(\"infant_mortality\", \"life_expectancy\")]\n\nNow that we’ve created two new objects that look at these specific variables, we can inspect them and get a better idea of the data.\n\n#look at the structure and summary of the first object\nstr(african_countries_pop_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(african_countries_pop_life)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n#do the same for the second object\nstr(african_countries_infant_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(african_countries_infant_life)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\nUsing the two new objects we can now create plots to characterize the relationship between life expectancy, population, and infant mortality. We’ll create two plots; one that analyzes life expectancy vs infant mortality and one that analyzes life expectancy vs population size. The latter will have a log scale to make the data easier to visualize.\n\n#load ggplot2 to create better plots\nlibrary(ggplot2)\n\n# Plot 1: Life expectancy vs. Infant mortality. lab() creates titles for the graph.\nggplot(african_countries_infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality\")\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n# Plot 2: Life expectancy vs. Population size.\n#scale_x_log10 puts the x axis (population) on a log scale.\nggplot(african_countries_pop_life, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size (log scale)\")\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nIn Plot 1, we can see a negative correlation. As life expectancy decreases, the number of infants dying increases; this makes sense since more developed countries with better healthcare have higher life expectancies and lower infant mortality rates. In Plot 2, population size and life expectancy are positively correlated. This is logical, as longer lives allow for greater population growth and more infrequent deaths. The “streaks” in the data can be attributed to the presence of different years for individual countries in the dataset.\nKnowing this, we can begin to narrow in on certain years and see which ones would be easiest to analyze given our dataset. We’ll figure out which years have missing data for infant mortality.\n\n#find which years have missing data for infant mortality. \n#is.na() identifies which rows have na as their value\n#select() shows us the years that are associated with these rows.\nafrican_countries %&gt;%\n  filter(is.na(infant_mortality)) %&gt;%\n  select(year)\n\n    year\n1   1960\n2   1960\n3   1960\n4   1960\n5   1960\n6   1960\n7   1960\n8   1960\n9   1960\n10  1960\n11  1961\n12  1961\n13  1961\n14  1961\n15  1961\n16  1961\n17  1961\n18  1961\n19  1961\n20  1961\n21  1961\n22  1961\n23  1961\n24  1961\n25  1961\n26  1961\n27  1961\n28  1962\n29  1962\n30  1962\n31  1962\n32  1962\n33  1962\n34  1962\n35  1962\n36  1962\n37  1962\n38  1962\n39  1962\n40  1962\n41  1962\n42  1962\n43  1962\n44  1963\n45  1963\n46  1963\n47  1963\n48  1963\n49  1963\n50  1963\n51  1963\n52  1963\n53  1963\n54  1963\n55  1963\n56  1963\n57  1963\n58  1963\n59  1963\n60  1964\n61  1964\n62  1964\n63  1964\n64  1964\n65  1964\n66  1964\n67  1964\n68  1964\n69  1964\n70  1964\n71  1964\n72  1964\n73  1964\n74  1964\n75  1965\n76  1965\n77  1965\n78  1965\n79  1965\n80  1965\n81  1965\n82  1965\n83  1965\n84  1965\n85  1965\n86  1965\n87  1965\n88  1965\n89  1966\n90  1966\n91  1966\n92  1966\n93  1966\n94  1966\n95  1966\n96  1966\n97  1966\n98  1966\n99  1966\n100 1966\n101 1966\n102 1967\n103 1967\n104 1967\n105 1967\n106 1967\n107 1967\n108 1967\n109 1967\n110 1967\n111 1967\n112 1967\n113 1968\n114 1968\n115 1968\n116 1968\n117 1968\n118 1968\n119 1968\n120 1968\n121 1968\n122 1968\n123 1968\n124 1969\n125 1969\n126 1969\n127 1969\n128 1969\n129 1969\n130 1969\n131 1970\n132 1970\n133 1970\n134 1970\n135 1970\n136 1971\n137 1971\n138 1971\n139 1971\n140 1971\n141 1971\n142 1972\n143 1972\n144 1972\n145 1972\n146 1972\n147 1972\n148 1973\n149 1973\n150 1973\n151 1973\n152 1973\n153 1973\n154 1974\n155 1974\n156 1974\n157 1974\n158 1974\n159 1975\n160 1975\n161 1975\n162 1975\n163 1975\n164 1976\n165 1976\n166 1976\n167 1977\n168 1977\n169 1977\n170 1978\n171 1978\n172 1979\n173 1979\n174 1980\n175 1981\n176 2016\n177 2016\n178 2016\n179 2016\n180 2016\n181 2016\n182 2016\n183 2016\n184 2016\n185 2016\n186 2016\n187 2016\n188 2016\n189 2016\n190 2016\n191 2016\n192 2016\n193 2016\n194 2016\n195 2016\n196 2016\n197 2016\n198 2016\n199 2016\n200 2016\n201 2016\n202 2016\n203 2016\n204 2016\n205 2016\n206 2016\n207 2016\n208 2016\n209 2016\n210 2016\n211 2016\n212 2016\n213 2016\n214 2016\n215 2016\n216 2016\n217 2016\n218 2016\n219 2016\n220 2016\n221 2016\n222 2016\n223 2016\n224 2016\n225 2016\n226 2016\n\n\nThere is data missing up to 1981 and then again for 2016, so we’ll select 2000. We’ll create a new object now with only observations from 2000.\n\n#create an object with only data from 2000\nafrican_countries_2000 &lt;- african_countries[african_countries$year == 2000, ]\n\nNow, we’ll make the same plots as before using only the data from 2000.\n\n# Plot 3: Life expectancy vs. Infant mortality.\nggplot(african_countries_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point() +\n  labs(title = \"Life Expectancy vs. Infant Mortality\")\n\n\n\n\n\n\n\n# Plot 4: Life expectancy vs. Population size. \nggplot(african_countries_2000, aes(x = population, y = life_expectancy)) +\n  geom_point() +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy vs. Population Size (log scale)\")\n\n\n\n\n\n\n\n\nThere still seems to be a negative correlation in plot 3, but plot 4 shows no noticeable correlation. We can now create some linear models with this data and draw more conclusions from the year 2000.\n\n#Table 1: fit life expectancy as a function of infant mortality. \n#lm() creates a linear model for the specified variables from a given dataset.\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, african_countries_2000)\n#print results to screen\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = african_countries_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n#Table 2: fit life expectancy as a function of population size\nfit2 &lt;- lm(life_expectancy ~ population, african_countries_2000)\n#print results to screen\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = african_countries_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nBased off of the results from the fit, we can see that infant mortality is a statistically significant predictor of life expectancy for African countries in the year 2000. On the other hand, population does not seem to be a statistically significant predictor for life expectancy in 2000. These are logical conclusions given our prior knowledge of demography.\nThis section contributed by Cory Cribb\nLoading dslabs dataset “murders”. Probably a more morbid data set but interesting to observe nonetheless.\n\nlibrary(dslabs)\nhelp(murders)\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ...\n\nsummary(murders)\n\n    state               abb                      region     population      \n Length:51          Length:51          Northeast    : 9   Min.   :  563626  \n Class :character   Class :character   South        :17   1st Qu.: 1696962  \n Mode  :character   Mode  :character   North Central:12   Median : 4339367  \n                                       West         :13   Mean   : 6075769  \n                                                          3rd Qu.: 6636084  \n                                                          Max.   :37253956  \n     total       \n Min.   :   2.0  \n 1st Qu.:  24.5  \n Median :  97.0  \n Mean   : 184.4  \n 3rd Qu.: 268.0  \n Max.   :1257.0  \n\n\nSince I am originally from the Southern region of the US; lets explore murders in that region.\n\nSouth_Murders &lt;- murders[murders$region== \"South\", ]\nstr(South_Murders)\n\n'data.frame':   17 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Arkansas\" \"Delaware\" \"District of Columbia\" ...\n $ abb       : chr  \"AL\" \"AR\" \"DE\" \"DC\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ population: num  4779736 2915918 897934 601723 19687653 ...\n $ total     : num  135 93 38 99 669 376 116 351 293 120 ...\n\nsummary(South_Murders)\n\n    state               abb                      region     population      \n Length:17          Length:17          Northeast    : 0   Min.   :  601723  \n Class :character   Class :character   South        :17   1st Qu.: 2967297  \n Mode  :character   Mode  :character   North Central: 0   Median : 4625364  \n                                       West         : 0   Mean   : 6804378  \n                                                          3rd Qu.: 8001024  \n                                                          Max.   :25145561  \n     total      \n Min.   : 27.0  \n 1st Qu.:111.0  \n Median :207.0  \n Mean   :246.8  \n 3rd Qu.:293.0  \n Max.   :805.0  \n\n\nFrom this data set, it would appear the researchers classified 17 states as being in the Southern region. Let’s explore if population size had any relationship to gun murders.\n\nPop_and_murder &lt;- South_Murders[, c(\"total\", \"population\")]\nstr(Pop_and_murder)\n\n'data.frame':   17 obs. of  2 variables:\n $ total     : num  135 93 38 99 669 376 116 351 293 120 ...\n $ population: num  4779736 2915918 897934 601723 19687653 ...\n\nsummary(Pop_and_murder)\n\n     total         population      \n Min.   : 27.0   Min.   :  601723  \n 1st Qu.:111.0   1st Qu.: 2967297  \n Median :207.0   Median : 4625364  \n Mean   :246.8   Mean   : 6804378  \n 3rd Qu.:293.0   3rd Qu.: 8001024  \n Max.   :805.0   Max.   :25145561  \n\n\nCreate a scatter plot viewing total gun murders on the x-axis and state population on the y-axis to observe a trend. Add a best fit line to the plot to see if there is a trend.\n\nattach(South_Murders)\n\nThe following object is masked from package:tidyr:\n\n    population\n\nplot(total,population, main= \"Total gun murders vs. population size\", xlab=\"Total gun murders\", ylab=\"population\")\nabline(lm(population~total))\n\n\n\n\n\n\n\n\nFrom a quick view of the plot, we see that there appears to be a positive correlation that there are more gun murders in states with higher populations. Let’s run a linear model to see if the data gives a statistically significant observation.\n\nfit3 &lt;- lm(population~total, South_Murders)\nsummary(fit3)\n\n\nCall:\nlm(formula = population ~ total, data = South_Murders)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-5332407  -680032   482183  1257898  1945758 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -443125     717119  -0.618    0.546    \ntotal          29370       2229  13.178 1.19e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1898000 on 15 degrees of freedom\nMultiple R-squared:  0.9205,    Adjusted R-squared:  0.9152 \nF-statistic: 173.7 on 1 and 15 DF,  p-value: 1.189e-09\n\n\nFrom the simple linear regression, we see that the slope is statistically significant. The adjusted R-squared is 0.9152 which would indicated a Strong, Positive correlation in total gun murders per Gross Population size in the Southern Region of the United States of America."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Me holding a corn snake at the Savannah River Site Ecology Lab. They’re friendly but a bit freaky!"
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "Background",
    "text": "Background\n\nEducation\nI completed my Bachelors in Wildlife Sciences at Mississippi State University in May of 2023. I entered the ILS program at UGA the following August and have since matriculated into the EHS program as a first year in Erin Lipp’s lab.\n\n\nHometown\nI was born in Orange, CA and raised in Collierville, TN outside of Memphis. My parents and 5 pets (3 cats and 2 dogs) still live there!\n\n\nFun Fact\nDuring field research on pollinators at my undergraduate university, I was stung about 5 times by a swarm of yellowjackets that I apparently offended! I couldn’t move one of my fingers for three days."
  },
  {
    "objectID": "aboutme.html#research-interestsexperience",
    "href": "aboutme.html#research-interestsexperience",
    "title": "About me",
    "section": "Research Interests/Experience",
    "text": "Research Interests/Experience\n\nCurrent Research\nI don’t have my own project yet, but I’m interested in pursuing research on bacterial pathogens and environmental epidemiology. I would love to include a wildlife aspect!\n\n\nExperience\nI have taken some basic statistics courses and an overview of data analysis using R. I don’t know any coding languages, but I can navigate RStudio and make some basic plots and analyses.\n\n\nHopes for the class\nI hope to become proficient in making my data usable for analysis and to learn more about creating a workflow that allows for reproducibility.\n\n\nInteresting Data Analysis Facts\nThis website has some fun facts about data analysis. I found it interesting that 80% of a data scientist’s time is spent cleaning the data!"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise, I’ll be using a dataset I got from the CDC website about smoking attributable expenses in the U.S. The data includes expenses per state for different categories such as hospital bills, ambulances, and prescriptions as well as overall expenses for the U.S. from 2005-2009. We’ll do an exploratory data analysis and some data processing for this exercise.\n\n#load the dataset\nlibrary(here)\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\nsmoke_expense&lt;- read.csv(here(\"cdcdata-exercise\", \"SAE.csv\"))\n#make sure data fully loaded by checking number of rows and columns. We should have 19 variables and 1560 observations\nnrow(smoke_expense)\n\n[1] 1560\n\nncol(smoke_expense)\n\n[1] 19\n\n#check the structure and summary of the data. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #\nsummary(smoke_expense)\n\n      Year      LocationAbbr       LocationDesc        DataSource       \n Min.   :2005   Length:1560        Length:1560        Length:1560       \n 1st Qu.:2006   Class :character   Class :character   Class :character  \n Median :2007   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2007                                                           \n 3rd Qu.:2008                                                           \n Max.   :2009                                                           \n  TopicType          TopicDesc         MeasureDesc          Variable        \n Length:1560        Length:1560        Length:1560        Length:1560       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Data_Value_Unit    Data_Value_Type      Data_Value      \n Length:1560        Length:1560        Min.   :     8.4  \n Class :character   Class :character   1st Qu.:   105.7  \n Mode  :character   Mode  :character   Median :   306.1  \n                                       Mean   :  1545.2  \n                                       3rd Qu.:   842.3  \n                                       Max.   :132459.8  \n Data_Value_Footnote_Symbol Data_Value_Footnote GeoLocation       \n Length:1560                Length:1560         Length:1560       \n Class :character           Class :character    Class :character  \n Mode  :character           Mode  :character    Mode  :character  \n                                                                  \n                                                                  \n                                                                  \n Topic.Type.ID        Topic.ID          Measure.ID        SubMeasureID      \n Length:1560        Length:1560        Length:1560        Length:1560       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  DisplayOrder\n Min.   :1.0  \n 1st Qu.:2.0  \n Median :3.5  \n Mean   :3.5  \n 3rd Qu.:5.0  \n Max.   :6.0  \n\n#structure(smoke_expense)\n\nWe can see that there’s one observation for each of our 6 expense types per year. We can also see from the structure and summary that there are many columns that we don’t need for an analysis. We can go ahead and pick out the ones that are of interest to us: year, location, variable (which refers to the type of expense), and value (which is the cost in millions of dollars for each expense type). All of the other columns seem to be for record keeping purposes.\n\n#make a new data frame containing only our four columns of interest: Year, LocationAbbr, Variable, and Data_Value.\nsmoke_expense_2 &lt;- smoke_expense[, c(\"Year\", \"LocationAbbr\", \"Variable\", \"Data_Value\")]\n#check the new object to make sure it has everything we want. For the sake of the webpage, I've made the structure line an annotation, but if you're viewing this in R go ahead and delete the #\n#structure(smoke_expense_2)\nsummary(smoke_expense_2)\n\n      Year      LocationAbbr         Variable           Data_Value      \n Min.   :2005   Length:1560        Length:1560        Min.   :     8.4  \n 1st Qu.:2006   Class :character   Class :character   1st Qu.:   105.7  \n Median :2007   Mode  :character   Mode  :character   Median :   306.1  \n Mean   :2007                                         Mean   :  1545.2  \n 3rd Qu.:2008                                         3rd Qu.:   842.3  \n Max.   :2009                                         Max.   :132459.8  \n\n\nThis new object is much more condensed and easier to work with. We’ll check now to make sure there are no NA values and then proceed with some EDA.\n\n#check for NA values\nna_check&lt;- is.na(smoke_expense_2)\nprint(sum(na_check))\n\n[1] 0\n\n\nSince there aren’t any NA values and looking at the structure indicates no missing values, we can begin to check the mean and standard deviation of each expense type across the 4 years in the data set. Rather than looking at all 50 states, let’s focus on 5 to make this a bit easier. We’ll use GA, TN, MS, CA, and FL.\nUsing Microsoft Copilot with GPT-4 in “Precise Mode”, I entered the following prompt to get the code I’m about to use: “For my exploratory analysis I want to summarize each variable in a way that can be described by a distribution. For instance, I want to be able to determine the mean and standard deviation of each expense type for 5 different states over the 4 year period recorded in the dataset. What is the best approach for this and could you provide some example code?”\n\n# Load necessary libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Filter for the 5 states you are interested in\nstates &lt;- c(\"TN\", \"MS\", \"GA\", \"FL\", \"CA\")\nsmoke_filtered &lt;- smoke_expense_2 %&gt;% filter(LocationAbbr %in% states)\n\n# Calculate mean and standard deviation\nsmoke_summary &lt;- smoke_filtered %&gt;%\n  group_by(LocationAbbr, Year, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr', 'Year'. You can override\nusing the `.groups` argument.\n\n# Print the summary statistics\nprint(smoke_summary)\n\n# A tibble: 150 × 5\n# Groups:   LocationAbbr, Year [25]\n   LocationAbbr  Year Variable             Mean    SD\n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt;\n 1 CA            2005 Ambulatory          2057.    NA\n 2 CA            2005 Hospital            5309.    NA\n 3 CA            2005 Nursing Home         592.    NA\n 4 CA            2005 Other                620.    NA\n 5 CA            2005 Prescription Drugs  1883.    NA\n 6 CA            2005 Total              10460.    NA\n 7 CA            2006 Ambulatory          2122.    NA\n 8 CA            2006 Hospital            5652.    NA\n 9 CA            2006 Nursing Home         641.    NA\n10 CA            2006 Other                677.    NA\n# ℹ 140 more rows\n\n\nThis is good information, but since there’s only one observation for each expense category per year, we don’t learn much from a mean or standard deviation (SD) calculation. We can group each year together to get the mean and SD to allow for easier creation of synthetic data for the second part of this exercise.\n\n# Calculate mean and standard deviation for all 5 states combined\nsmoke_summary &lt;- smoke_filtered %&gt;%\n  group_by(LocationAbbr, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr'. You can override using the\n`.groups` argument.\n\n# Print the summary statistics\nprint(smoke_summary)\n\n# A tibble: 30 × 4\n# Groups:   LocationAbbr [5]\n   LocationAbbr Variable             Mean     SD\n   &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n 1 CA           Ambulatory          2294.  208. \n 2 CA           Hospital            6006.  571. \n 3 CA           Nursing Home         699.   87.9\n 4 CA           Other                725.   76.8\n 5 CA           Prescription Drugs  2172.  197. \n 6 CA           Total              11896. 1131. \n 7 FL           Ambulatory          1419.  105. \n 8 FL           Hospital            3532.  270. \n 9 FL           Nursing Home         595.   58.9\n10 FL           Other                514.   53.3\n# ℹ 20 more rows\n\n\nNow we can create some visualizations of this information for easier interpretation of the data. I entered the following prompt into Microsoft Copilot with GPT-4 in “Precise mode”: “I want to make plots now to determine if the distribution of this data is relatively normal. How would I go about that?”\nI received this code string from it, with the only modifications I made being the proper column names and adjusting the binwidth to the square root of all of the observations:\n\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a histogram\nggplot(smoke_filtered, aes(x = Data_Value)) + \n  geom_histogram(binwidth = 12.25, fill = \"blue\", color = \"black\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Histogram of Expenses\", x = \"Expense\", y = \"Frequency\")\n\n\n\n\n\n\n\n# Create a density plot\nggplot(smoke_filtered, aes(x = Data_Value)) + \n  geom_density(fill = \"blue\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Density Plot of Expenses\", x = \"Expense\", y = \"Density\") \n\n\n\n\n\n\n\n# Create a Q-Q plot\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nqqPlot(smoke_filtered$Data_Value, distribution = \"norm\", main = \"Q-Q Plot of Expenses\")\n\n\n\n\n\n\n\n\n[1] 131  45\n\n\nThe histogram isn’t very useful for much besides visualizing the numbers, but the Q-Q plot shows that the distribution for the data as a whole seems normal but contains some outliers to the left and the right. The density plots point out that “Other”, “Nursing Home”, and “Prescription Drugs”, and “Ambulatory” expenses are pretty skewed, but the other two are pretty normally distributed. Hopefully this is enough information to create a good synthetic data set that mimics the trends seen in this one!\n\nThis section was contributed by Emma Hardin-Parker\n\n# Load required packages\nlibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(gtsummary)\n\n\n# Set a seed for reproducibility\nset.seed(207)\n# Define the number of observations to generate\nn_obs &lt;- 150\n\nI am now going to get a feel for the data ussing skimr and gtsummary() functions.\n\nskimr::skim(smoke_filtered)\n\n\nData summary\n\n\nName\nsmoke_filtered\n\n\nNumber of rows\n150\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nLocationAbbr\n0\n1\n2\n2\n0\n5\n0\n\n\nVariable\n0\n1\n5\n18\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nYear\n0\n1\n2007.0\n1.42\n2005.0\n2006.00\n2007.00\n2008.00\n2009.0\n▇▇▇▇▇\n\n\nData_Value\n0\n1\n1747.8\n2581.55\n50.6\n225.15\n660.15\n2073.02\n13292.4\n▇▁▁▁▁\n\n\n\n\n\n\ngtsummary::tbl_summary(smoke_filtered, statistic = list(\n  all_continuous() ~ \"{mean}/{median}/{min}/{max}/{sd}\",\n  all_categorical() ~ \"{n} / {N} ({p}%)\"\n),)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1501\n\n\n\n\nYear\n\n\n\n\n    2005\n30 / 150 (20%)\n\n\n    2006\n30 / 150 (20%)\n\n\n    2007\n30 / 150 (20%)\n\n\n    2008\n30 / 150 (20%)\n\n\n    2009\n30 / 150 (20%)\n\n\nLocationAbbr\n\n\n\n\n    CA\n30 / 150 (20%)\n\n\n    FL\n30 / 150 (20%)\n\n\n    GA\n30 / 150 (20%)\n\n\n    MS\n30 / 150 (20%)\n\n\n    TN\n30 / 150 (20%)\n\n\nVariable\n\n\n\n\n    Ambulatory\n25 / 150 (17%)\n\n\n    Hospital\n25 / 150 (17%)\n\n\n    Nursing Home\n25 / 150 (17%)\n\n\n    Other\n25 / 150 (17%)\n\n\n    Prescription Drugs\n25 / 150 (17%)\n\n\n    Total\n25 / 150 (17%)\n\n\nData_Value\n1,748/660/51/13,292/2,582\n\n\n\n1 n / N (%); Mean/Median/Minimum/Maximum/SD\n\n\n\n\n\n\n\n\nNow I am going to create a synthetic data set based off the actual data.\n\nsyn_smoke &lt;- data.frame(\n  Year = integer(n_obs),\n  LocationAbbr = character(n_obs),\n  Variable = character(n_obs),\n  Data_Value = numeric(n_obs)\n)\n\n\n#Variable1\nsyn_smoke$Year &lt;- sample(c(\"2005\", \"2006\", \"2007\", \"2008\", \"2009\"),\n                         n_obs, replace = TRUE,\n                         prob = as.integer(table(smoke_filtered$Year)))\n                   \n#Variable2 \nsyn_smoke$LocationAbbr &lt;- sample(c(\"GA\", \"MS\", \"TN\", \"FL\", \"CA\"),\n                                 n_obs, replace = TRUE,\n                                 prob =  as.numeric(table(smoke_filtered$LocationAbbr)/100))\n\n#Variable3\nsyn_smoke$Variable &lt;- sample(c(\"Ambulatory\", \"Hospital\", \"Nursing Home\", \"Other\", \"Prescription Drugs\", \"Total\"),\n                             n_obs,\n                             replace = TRUE,\n                             prob = as.numeric(table(smoke_filtered$Variable)/100))\n#Variable4\nsyn_smoke$Data_Value &lt;- round(runif(n_obs,\n                                    min = min(smoke_filtered$Data_Value),\n                                    max = max(smoke_filtered$Data_Value)), 1)\n\nTo make sure the synthetic data set was created properly, I am going to use the head(), glimpse(), and summary() functions to see if the first five rows look okay for further analyses.\n\nhead(syn_smoke)\n\n  Year LocationAbbr           Variable Data_Value\n1 2006           TN Prescription Drugs     3505.4\n2 2008           TN              Total      890.3\n3 2005           CA              Total     5273.0\n4 2006           CA           Hospital     7084.6\n5 2007           GA Prescription Drugs    12709.5\n6 2006           GA              Total     6280.9\n\nsummary(syn_smoke)\n\n     Year           LocationAbbr         Variable           Data_Value     \n Length:150         Length:150         Length:150         Min.   :   76.5  \n Class :character   Class :character   Class :character   1st Qu.: 2745.9  \n Mode  :character   Mode  :character   Mode  :character   Median : 6888.2  \n                                                          Mean   : 6425.2  \n                                                          3rd Qu.: 9791.8  \n                                                          Max.   :13236.8  \n\nglimpse(syn_smoke)\n\nRows: 150\nColumns: 4\n$ Year         &lt;chr&gt; \"2006\", \"2008\", \"2005\", \"2006\", \"2007\", \"2006\", \"2008\", \"…\n$ LocationAbbr &lt;chr&gt; \"TN\", \"TN\", \"CA\", \"CA\", \"GA\", \"GA\", \"CA\", \"FL\", \"CA\", \"MS…\n$ Variable     &lt;chr&gt; \"Prescription Drugs\", \"Total\", \"Total\", \"Hospital\", \"Pres…\n$ Data_Value   &lt;dbl&gt; 3505.4, 890.3, 5273.0, 7084.6, 12709.5, 6280.9, 8885.6, 1…\n\n\nEverything looks as it should, so it’s time to make some exploratory figures and tables.\n\n# Calculate mean and standard deviation for all 5 states combined\nsyn_smoke_summary &lt;- syn_smoke %&gt;%\n  group_by(LocationAbbr, Variable) %&gt;%\n  summarise(\n    Mean = mean(Data_Value, na.rm = TRUE),\n    SD = sd(Data_Value, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'LocationAbbr'. You can override using the\n`.groups` argument.\n\n\n\n# Create a histogram\nsyn_hist &lt;- ggplot(syn_smoke, aes(x = Data_Value)) + \n  geom_histogram(binwidth = 12.25, fill = \"blue\", color = \"black\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Histogram of Expenses\", x = \"Expense\", y = \"Frequency\")\nprint(syn_hist)\n\n\n\n\n\n\n\n# Create a density plot\nlibrary(scales)\nsyn_dens &lt;- ggplot(syn_smoke, aes(x = Data_Value)) + \n  geom_density(fill = \"blue\") +\n  facet_wrap(~ Variable) +\n  theme_minimal() +\n  labs(title = \"Density Plot of Expenses\", x = \"Expense\", y = \"Density\") +\n  scale_x_continuous(labels = label_number())\nprint(syn_dens)\n\n\n\n\n\n\n\n# Create a Q-Q plot\nlibrary(car)\nsyn_qq &lt;- qqPlot(syn_smoke$Data_Value, distribution = \"norm\", main = \"Q-Q Plot of Expenses\")\n\n\n\n\n\n\n\n\nThe histograms produced between the original data and the synthetic data were quite different. Most of the individual histograms per Variable created with the original data were skewed to the right, while the histograms produced with the synthetic data were evenly distributed throughout.\nThe density plots also look different between the original data and the synthetic data, however, I struggled to code this plot. For some reason, the density value on the y-axis used scientific notation and I had to add an additional line of code to remove it. Even with that transformation, the densities on average are higher in the synthetic plot than the original plot.\nThe q-q plot was undoubtedly the most interesting plot to compare to the original data. The synthetic q-q plot is significantly more normally distributed that the original plot. Though the histograms differed drastically, it is much easier to compare normality between the data sets using a q-q plot, so I am happy that I was able to successfully create this one."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise, we will be recreating a figure found on the FiveThirtyEight website about data detailing how Congress members are older on average than ever before. We’ll use the ggplot package and some help from Microsoft Copilot’s Precise Mode to generate the base code.\nI entered this prompt to get this first output that I modified to load the actual dataset:\n“I would like to use R to recreate the figure titled”The House and Senate are older than ever before Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023” found at this link: https://fivethirtyeight.com/features/aging-congress-boomers/. Can you give me code to reproduce that exact figure? The data is open access and available here: https://data.fivethirtyeight.com/. It is under the section “Congress Today Is Older Than It’s Ever Been” from April 2, 2023.”\nI also entered this prompt for the line that adds the column “Year” to the set: “The dataset doesn’t have a column for”Year”, but instead records the time period in a column named “congress” which is defined as follows: The number of the Congress that this member’s row refers to. For example, 118 indicates the member served in the 118th Congress (2023-2025). How do I go from this raw format to the one in their figure?” As well as this prompt: “Is there a way I can modify the plot to just show the average ages for each year? The variable”age_years” in the dataset doesn’t do this; it only lists the age for each member in the set.”\nHere is the AI output (made into annotations to prevent loading/error messages) after all of these prompts:\n\n# Load necessary libraries\n# library(dplyr)\n# library(ggplot2)\n\n# Calculate average age for each year\n#avg_age &lt;- congress %&gt;%\n#  group_by(Year, chamber) %&gt;%\n#  summarise(avg_age_years = mean(age_years, na.rm = TRUE))\n\n# Plot the data\n#ggplot(avg_age, aes(x = Year, y = avg_age_years, color = chamber)) +\n # geom_line() +\n # labs(title = \"The House and Senate are older than ever before\",\n  #     subtitle = \"Average age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n  #     x = \"Year\",\n  #     y = \"Average Age\",\n  #     color = \"Chamber\") +\n # theme_bw()\n\nAnd everything past here is the manually modified code:\n\n#load packages\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Replace 'path_to_file' with the path to your downloaded file\ncongress &lt;- read.csv(here(\"presentation-exercise\", \"congress-demographics\", \"data_aging_congress.csv\"))\n#Check the data\nhead(congress)\n\n  congress start_date chamber state_abbrev party_code                 bioname\n1       82 1951-01-03   House           ND        200    AANDAHL, Fred George\n2       80 1947-01-03   House           VA        100 ABBITT, Watkins Moorman\n3       81 1949-01-03   House           VA        100 ABBITT, Watkins Moorman\n4       82 1951-01-03   House           VA        100 ABBITT, Watkins Moorman\n5       83 1953-01-03   House           VA        100 ABBITT, Watkins Moorman\n6       84 1955-01-03   House           VA        100 ABBITT, Watkins Moorman\n  bioguide_id   birthday cmltv_cong cmltv_chamber age_days age_years generation\n1     A000001 1897-04-09          1             1    19626  53.73306       Lost\n2     A000002 1908-05-21          1             1    14106  38.62012   Greatest\n3     A000002 1908-05-21          2             2    14837  40.62149   Greatest\n4     A000002 1908-05-21          3             3    15567  42.62012   Greatest\n5     A000002 1908-05-21          4             4    16298  44.62149   Greatest\n6     A000002 1908-05-21          5             5    17028  46.62012   Greatest\n\n# Add a Year column to the data frame. 1787 is the first period of Congress with new ones every 2 years, so this calculation makes the \"congress\" column easier to visualize.\ncongress$Year &lt;- 1787 + 2 * congress$congress\n\n#Calculate the average age for each year in the dataset\navg_age &lt;- congress %&gt;%\n  group_by(Year, chamber) %&gt;%\n  summarise(avg_age_years = mean(age_years, na.rm = TRUE))\n\n`summarise()` has grouped output by 'Year'. You can override using the\n`.groups` argument.\n\n\nNow we can create the plot:\n\n# Plot the data\nggplot(avg_age, aes(x = Year, y = avg_age_years, color = chamber)) +\n  geom_line() +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Average age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       x = \"Year\",\n       y = \"Average Age\",\n       color = \"Chamber\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis is very close to the original. We’ll now just modify the x and y axes to have the same increments in time as the original and change the colors of the lines. We’ll also change the plot to show stepwise increments instead of lines and remove the gridlines. We’ll also remove the axis labels since the original didn’t have any and bold our title. Then, we’ll adjust the legend position.\n\n#modify the axis increments and the colors of the lines. Remove gridlines, make step plot instead of line plot, and thicken lines. Also adjust the legend position.\nggplot(avg_age, aes(x = Year, y = avg_age_years, color = chamber)) +\n  geom_step(linewidth = 1.5) +\n  labs(title = \"The House and Senate are older than ever before\",\n       subtitle = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n       x = \"\",\n       y = \"\",\n       color = \"\") +\n  scale_x_continuous(breaks = seq(1920, 2020, by = 20), limits = c(1920, 2020)) +\n  scale_y_continuous(breaks = seq(45, 65, by = 5), limits = c(45, 65)) +\n  scale_color_manual(values = c(\"Senate\" = \"purple\", \"House\" = \"green\")) +\n  theme_bw() +\n  theme(legend.position = \"top\", legend.justification = c(0,1))+\n  theme(panel.grid.major.x = element_blank(),\n        panel.grid.minor.x = element_blank(),\n        panel.border = element_blank(),\n        plot.title = element_text(face=\"bold\"))\n\nWarning: Removed 6 rows containing missing values (`geom_step()`).\n\n\n\n\n\n\n\n\n\nThis looks extremely close to the original. R is a very useful tool for creating and reproducing visualizations. Here’s the original figure for comparison: \nFor the next part of this exercise, we’ll create table with the same data shown in the plot. To begin, I gave Microsoft Copilot Precise Mode this prompt: “Now I would like to make a table that displays the information shown in the plot in a visually pleasing way. You can pick the R package used. What would the code look like for this?”\n\n# Load the required package\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n# Create the table\nkable(avg_age, caption = \"Average age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\") %&gt;%\n  kable_styling(\"striped\", full_width = F)\n\n\nAverage age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\n\n\nYear\nchamber\navg_age_years\n\n\n\n\n1919\nHouse\n50.63485\n\n\n1919\nSenate\n56.71305\n\n\n1921\nHouse\n51.51950\n\n\n1921\nSenate\n57.19329\n\n\n1923\nHouse\n51.50112\n\n\n1923\nSenate\n57.26109\n\n\n1925\nHouse\n52.35196\n\n\n1925\nSenate\n56.95630\n\n\n1927\nHouse\n53.28969\n\n\n1927\nSenate\n56.88502\n\n\n1929\nHouse\n53.80848\n\n\n1929\nSenate\n58.03901\n\n\n1931\nHouse\n53.71394\n\n\n1931\nSenate\n57.30885\n\n\n1933\nHouse\n52.54248\n\n\n1933\nSenate\n57.46966\n\n\n1935\nHouse\n51.56923\n\n\n1935\nSenate\n56.98198\n\n\n1937\nHouse\n51.33543\n\n\n1937\nSenate\n57.57079\n\n\n1939\nHouse\n51.24558\n\n\n1939\nSenate\n57.21297\n\n\n1941\nHouse\n51.17905\n\n\n1941\nSenate\n57.60996\n\n\n1943\nHouse\n52.07287\n\n\n1943\nSenate\n57.91862\n\n\n1945\nHouse\n52.45802\n\n\n1945\nSenate\n57.70944\n\n\n1947\nHouse\n51.73979\n\n\n1947\nSenate\n56.40930\n\n\n1949\nHouse\n51.65482\n\n\n1949\nSenate\n56.99059\n\n\n1951\nHouse\n52.45117\n\n\n1951\nSenate\n56.68717\n\n\n1953\nHouse\n52.16560\n\n\n1953\nSenate\n57.30762\n\n\n1955\nHouse\n52.48055\n\n\n1955\nSenate\n57.37286\n\n\n1957\nHouse\n53.36413\n\n\n1957\nSenate\n57.79469\n\n\n1959\nHouse\n52.10761\n\n\n1959\nSenate\n57.68167\n\n\n1961\nHouse\n52.73533\n\n\n1961\nSenate\n57.16725\n\n\n1963\nHouse\n52.09492\n\n\n1963\nSenate\n56.61896\n\n\n1965\nHouse\n50.88059\n\n\n1965\nSenate\n57.85668\n\n\n1967\nHouse\n51.22730\n\n\n1967\nSenate\n57.61036\n\n\n1969\nHouse\n51.78546\n\n\n1969\nSenate\n56.83123\n\n\n1971\nHouse\n52.32744\n\n\n1971\nSenate\n56.28458\n\n\n1973\nHouse\n51.64166\n\n\n1973\nSenate\n55.60350\n\n\n1975\nHouse\n50.24058\n\n\n1975\nSenate\n55.64103\n\n\n1977\nHouse\n49.67102\n\n\n1977\nSenate\n54.33374\n\n\n1979\nHouse\n49.30329\n\n\n1979\nSenate\n52.97910\n\n\n1981\nHouse\n48.75481\n\n\n1981\nSenate\n52.82680\n\n\n1983\nHouse\n49.15492\n\n\n1983\nSenate\n53.80838\n\n\n1985\nHouse\n50.19013\n\n\n1985\nSenate\n54.78347\n\n\n1987\nHouse\n51.14722\n\n\n1987\nSenate\n54.90860\n\n\n1989\nHouse\n51.93884\n\n\n1989\nSenate\n55.91800\n\n\n1991\nHouse\n52.93039\n\n\n1991\nSenate\n57.56222\n\n\n1993\nHouse\n51.73735\n\n\n1993\nSenate\n58.04549\n\n\n1995\nHouse\n51.35202\n\n\n1995\nSenate\n58.52094\n\n\n1997\nHouse\n51.97229\n\n\n1997\nSenate\n57.96277\n\n\n1999\nHouse\n53.07509\n\n\n1999\nSenate\n58.70057\n\n\n2001\nHouse\n53.75171\n\n\n2001\nSenate\n58.81935\n\n\n2003\nHouse\n54.40583\n\n\n2003\nSenate\n59.92630\n\n\n2005\nHouse\n55.53191\n\n\n2005\nSenate\n60.85637\n\n\n2007\nHouse\n56.19800\n\n\n2007\nSenate\n62.16361\n\n\n2009\nHouse\n56.70651\n\n\n2009\nSenate\n62.20867\n\n\n2011\nHouse\n56.65312\n\n\n2011\nSenate\n61.88345\n\n\n2013\nHouse\n56.68415\n\n\n2013\nSenate\n61.09670\n\n\n2015\nHouse\n57.15462\n\n\n2015\nSenate\n61.15650\n\n\n2017\nHouse\n57.93973\n\n\n2017\nSenate\n62.33103\n\n\n2019\nHouse\n57.44181\n\n\n2019\nSenate\n62.75417\n\n\n2021\nHouse\n57.76279\n\n\n2021\nSenate\n63.53330\n\n\n2023\nHouse\n57.30675\n\n\n2023\nSenate\n63.93180\n\n\n\n\n\n\n\nThis has too many rows and isn’t very visually appealing. I’ll try a different prompt: “Is there a different type of table you suggest to display a large dataset?”\n\n# Load the required package\nlibrary(dplyr)\nlibrary(knitr)\n\n# Create a new variable for the decade\navg_age$Decade &lt;- 10 * floor(avg_age$Year / 10)\n\n# Summarize the data\nsummary_data &lt;- avg_age %&gt;%\n  group_by(Decade, chamber) %&gt;%\n  summarise(\n    Mean = mean(avg_age_years),\n    Min = min(avg_age_years),\n    Max = max(avg_age_years)\n  )\n\n`summarise()` has grouped output by 'Decade'. You can override using the\n`.groups` argument.\n\n# Create the table\nknitr::kable(summary_data, caption = \"Summary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\")\n\n\nSummary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\n\n\nDecade\nchamber\nMean\nMin\nMax\n\n\n\n\n1910\nHouse\n50.63485\n50.63485\n50.63485\n\n\n1910\nSenate\n56.71305\n56.71305\n56.71305\n\n\n1920\nHouse\n52.49415\n51.50112\n53.80848\n\n\n1920\nSenate\n57.26694\n56.88502\n58.03901\n\n\n1930\nHouse\n52.08133\n51.24558\n53.71394\n\n\n1930\nSenate\n57.30885\n56.98198\n57.57079\n\n\n1940\nHouse\n51.82091\n51.17905\n52.45802\n\n\n1940\nSenate\n57.32758\n56.40930\n57.91862\n\n\n1950\nHouse\n52.51381\n52.10761\n53.36413\n\n\n1950\nSenate\n57.36880\n56.68717\n57.79469\n\n\n1960\nHouse\n51.74472\n50.88059\n52.73533\n\n\n1960\nSenate\n57.21690\n56.61896\n57.85668\n\n\n1970\nHouse\n50.63680\n49.30329\n52.32744\n\n\n1970\nSenate\n54.96839\n52.97910\n56.28458\n\n\n1980\nHouse\n50.23718\n48.75481\n51.93884\n\n\n1980\nSenate\n54.44905\n52.82680\n55.91800\n\n\n1990\nHouse\n52.21343\n51.35202\n53.07509\n\n\n1990\nSenate\n58.15840\n57.56222\n58.70057\n\n\n2000\nHouse\n55.31879\n53.75171\n56.70651\n\n\n2000\nSenate\n60.79486\n58.81935\n62.20867\n\n\n2010\nHouse\n57.17469\n56.65312\n57.93973\n\n\n2010\nSenate\n61.84437\n61.09670\n62.75417\n\n\n2020\nHouse\n57.53477\n57.30675\n57.76279\n\n\n2020\nSenate\n63.73255\n63.53330\n63.93180\n\n\n\n\n\nMuch better, but it isn’t very stylistically pleasing. I’ll ask the AI one more prompt: “Can I stylise it to look more appealing?”\n\n# Load the required package\nlibrary(gt)\n\n# Create the table\nsummary_data %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Summary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\"\n  ) %&gt;%\n  cols_label(\n    Decade = \"Decade\",\n    chamber = \"Chamber\",\n    Mean = \"Mean Age\",\n    Min = \"Minimum Age\",\n    Max = \"Maximum Age\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Mean, Min, Max),\n    decimals = 2\n  ) %&gt;%\n  tab_options(\n    table.width = px(500),\n    table.font.size = px(12)\n  )\n\n\n\n\n\n\n\nSummary of average age of the U.S. Senate and U.S. House by decade, 1919 to 2023\n\n\nChamber\nMean Age\nMinimum Age\nMaximum Age\n\n\n\n\n1910\n\n\nHouse\n50.63\n50.63\n50.63\n\n\nSenate\n56.71\n56.71\n56.71\n\n\n1920\n\n\nHouse\n52.49\n51.50\n53.81\n\n\nSenate\n57.27\n56.89\n58.04\n\n\n1930\n\n\nHouse\n52.08\n51.25\n53.71\n\n\nSenate\n57.31\n56.98\n57.57\n\n\n1940\n\n\nHouse\n51.82\n51.18\n52.46\n\n\nSenate\n57.33\n56.41\n57.92\n\n\n1950\n\n\nHouse\n52.51\n52.11\n53.36\n\n\nSenate\n57.37\n56.69\n57.79\n\n\n1960\n\n\nHouse\n51.74\n50.88\n52.74\n\n\nSenate\n57.22\n56.62\n57.86\n\n\n1970\n\n\nHouse\n50.64\n49.30\n52.33\n\n\nSenate\n54.97\n52.98\n56.28\n\n\n1980\n\n\nHouse\n50.24\n48.75\n51.94\n\n\nSenate\n54.45\n52.83\n55.92\n\n\n1990\n\n\nHouse\n52.21\n51.35\n53.08\n\n\nSenate\n58.16\n57.56\n58.70\n\n\n2000\n\n\nHouse\n55.32\n53.75\n56.71\n\n\nSenate\n60.79\n58.82\n62.21\n\n\n2010\n\n\nHouse\n57.17\n56.65\n57.94\n\n\nSenate\n61.84\n61.10\n62.75\n\n\n2020\n\n\nHouse\n57.53\n57.31\n57.76\n\n\nSenate\n63.73\n63.53\n63.93\n\n\n\n\n\n\n\nThis is a pretty good table overall and much easier to digest than the first one the AI spit out. Tables don’t seem to have as many options as figures when it comes to customization, but this is a good and quick way to visualize data and it gives more information than the plot did."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "EMMA HARDIN-PARKER contributed to this exercise.\nThe structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nHair Color\n0\n1\n1\n2\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nStrands of Hair\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5592.11111\n3533.51456\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\nFigure 2 shows a box plot giving the distributions of “Height” and “Hair Color” made from R. Black haired individuals tend to average about 173 cm. with a max of 180 cm. and a low of 162. Brown haired individuals average about 166 cm. with a max of 176 and low of 154. Red haired individuals average about 166 cm. with a max of 171 and low of 151. There is no distribution for purple and white due to there being only one individual for each.\n\n\n\n\n\n\n\n\nFigure 2: A box plot of Height (cm) and Hair Color.\n\n\n\n\n\nFigure 3 shows a scatter plot showing the correlation between “Weight” and “Strands of Hair” made from R. It turns out that there is no correlation between these two variables.\n\n\n\n\n\n\n\n\nFigure 3: A scatter plot showing Weight (lbs) and Strands of Hair."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nExample Table 3 shows a silly linear model fit of “Hair Color” and “Strands of Hair” as predictors for “Height”. The small sample size makes drawing any accurate conclusions difficult.\n\n\n\n\nTable 3: Linear model fit table of Hair Color and Strands of Hair as predictors for Height.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n176.7015914\n11.9719368\n14.7596495\n0.0006747\n\n\nHair ColorBR\n-3.4262889\n14.0614015\n-0.2436662\n0.8232017\n\n\nHair ColorP\n13.5369154\n17.6933112\n0.7650866\n0.4999000\n\n\nHair ColorR\n-8.9391056\n13.5302875\n-0.6606737\n0.5560240\n\n\nHair ColorW\n-39.1558092\n16.2731172\n-2.4061653\n0.0953447\n\n\nStrands of Hair\n-0.0009048\n0.0018297\n-0.4945259\n0.6548825"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Hair Color            0             1   3   6     0        6          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable   n_missing complete_rate   mean     sd  p0  p25  p50  p75 p100\n1 Height                  0             1  166.    16.0 133  156  166  178  183\n2 Weight                  0             1   70.1   21.2  45   55   70   80  110\n3 Strands of Hair         0             1 5592.  3534.    1 5000 6000 8178 9122\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▃▁▃▂▇\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height            &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"16…\n$ Weight            &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 5…\n$ Gender            &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"…\n$ `Strands of Hair` &lt;dbl&gt; 4, 5000, 5005, 6000, 7000, 8000, 1, 9000, 7500, 7550…\n$ `Hair Color`      &lt;chr&gt; \"Blonde\", \"Red\", \"Black\", \"Brown\", \"White\", \"Purple\"…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Strands of Hair\n Length:14          Min.   :  45.0   Length:14          Min.   :   1   \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:5010   \n Mode  :character   Median :  70.0   Mode  :character   Median :6525   \n                    Mean   : 602.7                      Mean   :5960   \n                    3rd Qu.:  90.0                      3rd Qu.:7888   \n                    Max.   :7000.0                      Max.   :9122   \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Strands of Hair` `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M                      4 Blonde      \n2 175        70 O                   5000 Red         \n3 sixty      60 F                   5005 Black       \n4 178        76 F                   6000 Brown       \n5 192        90 NA                  7000 White       \n6 6          55 F                   8000 Purple      \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n5959.57\n2882.38\n1\n5009.75\n6525\n7887.5\n9122\n▂▁▃▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n3\n6\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nStrands of Hair\n0\n1\n5592.11\n3533.51\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/XylemHu/Desktop/EPID8060E/GitHub/kevinkosewick-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                       `Allowed Values`  \n  &lt;chr&gt;           &lt;chr&gt;                                       &lt;chr&gt;             \n1 Height          height in centimeters                       numeric value &gt;0 …\n2 Weight          weight in kilograms                         numeric value &gt;0 …\n3 Gender          identified gender (male/female/other)       M/F/O/NA          \n4 Strands of Hair How many strands of hair the individual has numeric value &gt;0 …\n5 Hair Color      What color the invididual's hair is         BR/BL/R/W/P; BR=b…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height            &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"16…\n$ Weight            &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 5…\n$ Gender            &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"…\n$ `Strands of Hair` &lt;dbl&gt; 4, 5000, 5005, 6000, 7000, 8000, 1, 9000, 7500, 7550…\n$ `Hair Color`      &lt;chr&gt; \"BL\", \"R\", \"BL\", \"BR\", \"W\", \"P\", \"R\", \"BR\", \"BR\", \"B…\n\nsummary(rawdata)\n\n    Height              Weight          Gender          Strands of Hair\n Length:14          Min.   :  45.0   Length:14          Min.   :   1   \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:5010   \n Mode  :character   Median :  70.0   Mode  :character   Median :6525   \n                    Mean   : 602.7                      Mean   :5960   \n                    3rd Qu.:  90.0                      3rd Qu.:7888   \n                    Max.   :7000.0                      Max.   :9122   \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender `Strands of Hair` `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M                      4 BL          \n2 175        70 O                   5000 R           \n3 sixty      60 F                   5005 BL          \n4 178        76 F                   6000 BR          \n5 192        90 NA                  7000 W           \n6 6          55 F                   8000 P           \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n5959.57\n2882.38\n1\n5009.75\n6525\n7887.5\n9122\n▂▁▃▃▇\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nStrands of Hair\n0\n1.00\n6033.00\n2986.41\n1\n5024.00\n7000\n8000\n9122\n▂▁▂▃▇\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nStrands of Hair\n0\n1\n5893.55\n3232.78\n1\n5012.0\n7000\n8089\n9122\n▃▁▃▃▇\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nStrands of Hair\n0\n1\n5592.11\n3533.51\n1\n5000\n6000\n8178\n9122\n▃▁▃▂▇\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing informatio"
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Kevin Kosewick's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]