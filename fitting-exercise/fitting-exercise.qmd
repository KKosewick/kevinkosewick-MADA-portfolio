---
title: "Fitting Exercise"
author: "Kevin Kosewick"
editor: visual
---

We will be using the dataset found [here](https://github.com/metrumresearchgroup/BayesPBPK-tutorial) and made by the nlmixr team for this exercise.  The dataset contains pharmacokinetic observations from 120 subjects who were administered IV infusions of mavoglurant. We'll begin by loading the necessary packages and the dataset.

```{r}
# Load necessary libraries
library(ggplot2)
library(readr)
library(here)

# Load the data
fittingdata <- read.csv(here("fitting-exercise","fittingdata.csv"))

# Check the data
summary(fittingdata)

# Create plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose 
ggplot(fittingdata, aes(x = TIME, y = DV, color = DOSE, group = ID)) +
  geom_line() +
  labs(x = "Time", y = "DV", color = "Dose") +
  theme_minimal()
```
We can see that the data records a time series measuring concentrations of DV (which stands for Dependent Variable, which is Mavoglurant). Looking at the summary, we can see that OCC has values greater than 1. We don't know what these mean so we probably shouldn't use them. We'll remove all observations with values other than 1.

```{r}
# Load necessary library
library(dplyr)

# Filter the data
fittingdata2 <- fittingdata %>% filter(OCC == 1)
```
We now want to compute the sum of DV for each individual to determine the full amount of drug for each individual. I understand that according to the exercise details this is not the best approach, but this is mainly for practice anyways. I consulted Microsoft Copilot for help with this using this prompt (which is also the same as the instructions laid out in the exercise explanation): "Write code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18."

```{r}
# Exclude observations with TIME = 0 and compute the sum of DV for each individual
fittingdata_sum <- fittingdata2 %>%
  filter(TIME != 0) %>%
  group_by(ID) %>%
  summarize(Y = sum(DV))

# Create a data frame that contains only the observations where TIME == 0
fittingdata_time0 <- fittingdata2 %>%
  filter(TIME == 0)

# Use the appropriate join function to combine those two data frames
fittingdata_combined <- left_join(fittingdata_time0, fittingdata_sum, by = "ID")
```

We've created a new data frame that contains columns that are much easier to analyze now. We'll do some final cleaning steps by converting RACE and SEX to factors and removing some columns that we no longer need.

```{r}
# Convert RACE and SEX to factor variables and keep only variables specified in the exercise instructions
fittingdata_final <- fittingdata_combined %>%
  mutate(RACE = as.factor(RACE),
         SEX = as.factor(SEX)) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)
# Check data to make sure everything is good
summary(fittingdata_final)
class(fittingdata_final$RACE)
```
We'll begin a formal EDA now. We're interested in how each of the variables influences our outcome variable that we created, "Y". Again, this is the sum per individual of all of our original "DV" values. Before we begin, we should note that the documentation for this dataset is not very good. We don't know what the values in RACE or SEX indicate, so interpreting results from the EDA will be challenging for these. According to the study this is based off of, 86% of participants were male, so we can assume that a value of 1 is male and 2 is female (based off of the frequency of these values in the dataset). We'll generate plots for them regardless. First up is our AGE variable.

```{r}
# Load required package
library(ggplot2)

# Histogram for Age
ggplot(fittingdata_final, aes(x = AGE)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Age", x = "Age (years)", y = "Count")

# Scatterplot for Y by Age
ggplot(fittingdata_final, aes(x = AGE, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Age", x = "Age (years)", y = "Mavoglurant Concentration")
```
Our age values seem to have a relatively normal distribution with a minimum of 18 and maximum of 50. Our scatterplot shows that mavoglurant concentrations seem to remain the same on average between individuals of different ages. The plot shows no clear correlation one way or the other. Next, we'll investigate SEX.

```{r}
# Boxplot for mavoglurant concentration by sex
ggplot(fittingdata_final, aes(x = SEX, y = Y)) +
  geom_boxplot() +
  labs(title = "Concentration by Sex", x = "Sex", y = "Mavoglurant Concentration")
```
If we knew what our dataset's values meant or had clear documentation somewhere, we could interpret these results with certainty. Instead, all we can say is that if I'm right about 1 being male, they had higher concentrations on average than females. Given greatly unequal sample sizes and unclear documentation, we can't draw many conclusions from this.

```{r}
# Bar plot for Race
ggplot(fittingdata_final, aes(x = RACE)) +
  geom_bar() +
  labs(title = "Bar Plot of Race", x = "Race", y = "Count")

# Boxplot for mavoglurant concentration by race
ggplot(fittingdata_final, aes(x = RACE, y = Y)) +
  geom_boxplot() +
  labs(title = "Concentration by Race", x = "Race", y = "Mavoglurant Concentration")
```
We have no idea what this means since we don't have good documentation on the variables. Next, we'll look at our WT variable, which stands for weight (kg).

```{r}
# Histogram for Weight
ggplot(fittingdata_final, aes(x = WT)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Weight", x = "Weight (kg)", y = "Count")

# Scatterplot for Y by Weight
ggplot(fittingdata_final, aes(x = WT, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Weight", x = "Weight(kg)", y = "Mavoglurant Concentration")
```
We can see that there are more observations of low-mid weight than high weight individuals from our histogram. We can see from our scatterplot that there isn't a strong correlation between weight and concentration, but it seems like higher weights have lower concentrations on average. Now we can explore HT, which is apparently our height variable. No units were given, so this will be difficult to interpret at best.

```{r}
# Histogram for Height
ggplot(fittingdata_final, aes(x = HT)) +
  geom_histogram(binwidth = 0.1) +
  labs(title = "Histogram of Height", x = "Height", y = "Count")

# Scatterplot for Y by Height
ggplot(fittingdata_final, aes(x = HT, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Height", x = "Height", y = "Mavoglurant Concentration")
```
We can't tell much from the histogram since we don't know what unit height is in, but the data seems relatively normally distributed. It is slightly skewed to the right, but not by much. The scatterplot doesn't show a strong or clear correlation, but on average, it looks like concentration decreased as height increased. Finally, we'll look at our dose variable, which only has values of 25, 37.5, and 50.

```{r}
# Bar plot for Dose
ggplot(fittingdata_final, aes(x = DOSE)) +
  geom_bar() +
  labs(title = "Bar Plot of Race", x = "Dose", y = "Count")

# Scatterplot for Y by Dose
ggplot(fittingdata_final, aes(x = DOSE, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Dose", x = "Dose", y = "Mavoglurant Concentration")
```
We see that there were far fewer 37.5 doses than the others, but according to the scatterplot, there's a clear trend of increased concentration as the dosage increases. This concludes our EDA; now, we can move into our model fitting.

We will now fit a linear model to Y using the main predictor of interest, DOSE. Then, we'll fit a linear model to Y using all predictors and compare their RMSE and R-squared values. We'll be using Microsoft Copilot in Precise mode for help with the base code again.

```{r}
# Load necessary libraries
library(tidymodels)

# Split the data into training and testing sets
fittingdata_split <- initial_split(fittingdata_final, prop = 0.75)
train_data <- training(fittingdata_split)
test_data <- testing(fittingdata_split)

# Fit a linear model to the continuous outcome "Y" using the main predictor of interest, DOSE
model1_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

model1_fit <- model1_spec %>% 
  fit(Y ~ DOSE, data = train_data)

# Fit a linear model to the continuous outcome "Y" using all predictors
model2_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

model2_fit <- model2_spec %>% 
  fit(Y ~ ., data = train_data)

# Compute RMSE and R-squared for model1
model1_metrics <- model1_fit %>% 
  predict(test_data) %>% 
  bind_cols(test_data) %>% 
  metrics(truth = Y, estimate = .pred)

cat("Model 1:\n")
cat("RMSE: ", model1_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), "\n")
cat("R-squared: ", model1_metrics %>% filter(.metric == "rsq") %>% pull(.estimate), "\n\n")

# Compute RMSE and R-squared for model2
model2_metrics <- model2_fit %>% 
  predict(test_data) %>% 
  bind_cols(test_data) %>% 
  metrics(truth = Y, estimate = .pred)

cat("Model 2:\n")
cat("RMSE: ", model2_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), "\n")
cat("R-squared: ", model2_metrics %>% filter(.metric == "rsq") %>% pull(.estimate), "\n")

print(model1_fit)
print(model2_fit)
```
From our linear model that only uses DOSE as a predictor, we can see that DOSE is positively correlated with total mavoglurate concentration, which matches up with our EDA plot data. We can tell by looking at the coefficients produced by our models; positive coefficients indicate positive correlation whereas negative indicates negative.

Our second model shows that dose is positively correlated again. Furthermore, age and race2/88 are both positively correlated too, but the size of the coefficients indicates that age may be a weaker correlation. Sex 2, our females, are strongly negatively correlated with mavoglurate concentration. Race7 and height seem to be very strongly negatively correlated. Finally, weight is negatively correlated, but due to the coefficient size, this doesn't seem to be a strong relationship.

Our first model, which only uses DOSE as a predictor, seems to explain a bit more of the variation in the data. The R-squared value is slightly higher (by 0.003). However, the RMSE is also higher, which means that the error of Model 1 is slightly higher than that of model 2.  

Now, we'll look at how to do a logistic regression model on our data. We'll use SEX as the outcome since it's a categorical variable, even though this doesn't make sense from a science standpoint (it's just practice). We'll do the same thing: 1 model for just DOSE, and another for every predictor. Then we'll produce an ROC-AUC, which just measures performance for the classification problems at various threshold settings. We'll use Microsoft Copilot in Precise mode for the base code again.

```{r}
# Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, DOSE
model3_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

model3_fit <- model3_spec %>% 
  fit(SEX ~ DOSE, data = train_data)

# Fit a logistic model to SEX using all predictors
model4_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

model4_fit <- model4_spec %>% 
  fit(SEX ~ ., data = train_data)

# Compute ROC-AUC for "female" class for model3
model3_roc_auc_female <- model3_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_2)

# Compute ROC-AUC for "male" class for model3
model3_roc_auc_male <- model3_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_1)

cat("Model 3:\n")
cat("ROC-AUC for '2': ", model3_roc_auc_female$.estimate, "\n")
cat("ROC-AUC for '1': ", model3_roc_auc_male$.estimate, "\n\n")

# Compute ROC-AUC for "female" class for model4
model4_roc_auc_female <- model4_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_2)

# Compute ROC-AUC for "male" class for model4
model4_roc_auc_male <- model4_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_1)

cat("Model 4:\n")
cat("ROC-AUC for '2': ", model4_roc_auc_female$.estimate, "\n")
cat("ROC-AUC for '1': ", model4_roc_auc_male$.estimate, "\n")

print(model3_fit)
print(model4_fit)

```
The coefficients for both models are very different than they were for our linear regression model that had Y as the outcome. We can see that DOSE appears to be negatively correlated with SEX, which in our case would indicate that higher doses mean more males. DOSE is again negatively correlated in our model using every variable as a predictor. Age, weight, Race88 and Race7 are all positively correlated, which means that as these increase we're more likely to see females. Y, Race2, and height are negatively correlated.

We can see that the ROC-AUC value for Model 3 (just dose as a predictor) shows similar performance of the model when predicting both male and female values. Remember that "1" is our males and "2" is our females. Model 4, on the other hand, shows a far stronger ability to accurately predict males than females. This makes sense given that we had so many more observations of males in our data.

This set of models isn't as useful in making any sort of inferences about our data, as the question we asked before creating our model doesn't make much sense. It's good practice regardless.

Now, we'll prep our data for the next exercise. We'll set a random seed and begin splitting our data into test/train sets to fit some more models.

```{r}
#The Race variable is weird; we'll remove it and continue with the exercise

fittingdata_ultimate <- fittingdata_final

fittingdata_ultimate$RACE <- NULL

#set a random seed

set.seed(1234)

# Put 3/4 of the data into the training set 
data_split <- initial_split(fittingdata_ultimate, prop = 3/4)

# Create data frames for the two sets:
train_data2 <- training(data_split)
test_data2  <- testing(data_split)

```

Now that we've split the dataframe with the RACE variable removed, we can fit two new models. One uses only DOSE as a predictor for Y and one uses all variables. I gave Microsoft Copilot in Precise Mode the following prompt to generate this code and modified it to the specifics of our frame:

"In R, I have a data frame composed of 6 variables. I've split the observations with 75% in a training set and 25% in a testing set. Can you write code that uses the tidymodels framework to fit two linear models to our continuous outcome of interest (Y). The first model should only use DOSE as predictor, the second model should use all predictors. For both models, the metric to optimize should be RMSE. You should only use the training data set for fitting."

```{r}
# Specify the model using only DOSE as predictor
model_spec_dose <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Create a workflow
workflow_dose <- workflow() %>% 
  add_model(model_spec_dose) %>% 
  add_formula(Y ~ DOSE)

# Specify the model using all predictors
model_spec_all <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

# Create a workflow
workflow_all <- workflow() %>% 
  add_model(model_spec_all) %>% 
  add_formula(Y ~ .)

#fit the DOSE model
dose_fit <- workflow_dose %>% 
  fit(data = train_data2)

#augment to evaluate performance metric
aug_dose<- augment(dose_fit, train_data2)
aug_dose %>% select(Y, .pred)

#get RMSE of DOSE model
rmsedose<- aug_dose %>% rmse(truth = Y, .pred)

#fit the all predictors model
all_fit <- workflow_all %>% 
  fit(data = train_data2)

#augment to evaluate performance metric
aug_all<- augment(all_fit, train_data2)
aug_all %>% select(Y, .pred)

#get RMSE of ALL model
rmseall<- aug_all %>% rmse(truth = Y, .pred)


# Print the results
rmsedose
rmseall

```
Our second model using every predictor has a lower RMSE. We'll now compute the RMSE of a null-model (one that would just predict the mean outcome for each observation, without using any predictor information).

```{r}
# Compute the mean outcome
mean_outcome <- mean(train_data2$Y, na.rm = TRUE)

# Create a data frame with the predicted values for the null model
predictions_null <- data.frame(.pred = rep(mean_outcome, nrow(train_data2)))

# Compute RMSE for the null model
rmse_null <- predictions_null %>% 
  bind_cols(train_data2 %>% select(Y)) %>% 
  yardstick::rmse(Y, .pred)

# Print the RMSE for the null model
print(paste("RMSE for the null model: ", rmse_null$.estimate))
```

As the results show us (RMSE of 627), the best fitting model appears to be the one using all variables as predictors for Y. As to be expected, the null model that doesn't use any predictors has the highest RMSE, indicating that it's a poor fit. The model usng only DOSE as a predictor has a far lower RMSE (702 compared to 948) but it is higher than our all predictor model. 
However, we can't be sure that this isn't due to overfitting since we're only using RMSE as our metric. We'll use cross-validation (CV) as a way to see if these results could be achieved on unseen data. We'll follow the tidymodels framework again for this code.

```{r}
#reset the seed
set.seed(1234)

# Create 10-fold cross-validation splits
cv_splits <- vfold_cv(train_data2, v = 10)

# Perform cross-validation for the DOSE model
cv_results_dose <- workflow_dose %>% 
  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))

# Perform cross-validation for the all predictors model
cv_results_all <- workflow_all %>% 
  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))

# Print the RMSE for each model
cv_results_dose %>% collect_metrics()
cv_results_all %>% collect_metrics()

# Run the code again with a different seed
set.seed(456)

# Create 10-fold cross-validation splits
cv_splits <- vfold_cv(train_data2, v = 10)

# Perform cross-validation for the DOSE model
cv_results_dose <- workflow_dose %>% 
  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))

# Perform cross-validation for the all predictors model
cv_results_all <- workflow_all %>% 
  fit_resamples(resamples = cv_splits, metrics = metric_set(rmse))

# Print the RMSE for each model
cv_results_dose %>% collect_metrics()
cv_results_all %>% collect_metrics()

```
We can see that our RMSE value is 690 for our DOSE model and 645 for our all predictors model. The gap between these two is smaller than it was for our previous model evaluation without CV but the all predictors model still appears to fit better. Our second random number seed CV run gives 689 for DOSE and 630 for all predictors; this is very similar to the previous run but our all predictor model gives a slighlty stronger value.

Looking at the standard error values, we can see that the standard error is lower in both RNG seeds for our all predictor models. These models seem more robust and than the DOSE models overall. We didn't bother to run CV on the null model again because it doesn't give much more information. Overall, our patterns seem the same as our initial evaluations indicated.
