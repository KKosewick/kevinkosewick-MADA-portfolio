---
title: "Fitting Exercise"
author: "Kevin Kosewick"
editor: visual
---

We will be using the dataset found [here](https://github.com/metrumresearchgroup/BayesPBPK-tutorial) and made by the nlmixr team for this exercise.  The dataset contains pharmacokinetic observations from 120 subjects who were administered IV infusions of mavoglurant. We'll begin by loading the necessary packages and the dataset.

```{r}
# Load necessary libraries
library(ggplot2)
library(readr)
library(here)

# Load the data
fittingdata <- read.csv(here("fitting-exercise","fittingdata.csv"))

# Check the data
summary(fittingdata)

# Create plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose 
ggplot(fittingdata, aes(x = TIME, y = DV, color = DOSE, group = ID)) +
  geom_line() +
  labs(x = "Time", y = "DV", color = "Dose") +
  theme_minimal()
```
We can see that the data records a time series measuring concentrations of DV (which stands for Dependent Variable, which is Mavoglurant). Looking at the summary, we can see that OCC has values greater than 1. We don't know what these mean so we probably shouldn't use them. We'll remove all observations with values other than 1.

```{r}
# Load necessary library
library(dplyr)

# Filter the data
fittingdata2 <- fittingdata %>% filter(OCC == 1)
```
We now want to compute the sum of DV for each individual to determine the full amount of drug for each individual. I understand that according to the exercise details this is not the best approach, but this is mainly for practice anyways. I consulted Microsoft Copilot for help with this using this prompt (which is also the same as the instructions laid out in the exercise explanation): "Write code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18."

```{r}
# Exclude observations with TIME = 0 and compute the sum of DV for each individual
fittingdata_sum <- fittingdata2 %>%
  filter(TIME != 0) %>%
  group_by(ID) %>%
  summarize(Y = sum(DV))

# Create a data frame that contains only the observations where TIME == 0
fittingdata_time0 <- fittingdata2 %>%
  filter(TIME == 0)

# Use the appropriate join function to combine those two data frames
fittingdata_combined <- left_join(fittingdata_time0, fittingdata_sum, by = "ID")
```

We've created a new data frame that contains columns that are much easier to analyze now. We'll do some final cleaning steps by converting RACE and SEX to factors and removing some columns that we no longer need.

```{r}
# Convert RACE and SEX to factor variables and keep only variables specified in the exercise instructions
fittingdata_final <- fittingdata_combined %>%
  mutate(RACE = as.factor(RACE),
         SEX = as.factor(SEX)) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)
# Check data to make sure everything is good
summary(fittingdata_final)
class(fittingdata_final$RACE)
```
We'll begin a formal EDA now. We're interested in how each of the variables influences our outcome variable that we created, "Y". Again, this is the sum per individual of all of our original "DV" values. Before we begin, we should note that the documentation for this dataset is not very good. We don't know what the values in RACE or SEX indicate, so interpreting results from the EDA will be challenging for these. According to the study this is based off of, 86% of participants were male, so we can assume that a value of 1 is male and 2 is female (based off of the frequency of these values in the dataset). We'll generate plots for them regardless. First up is our AGE variable.

```{r}
# Load required package
library(ggplot2)

# Histogram for Age
ggplot(fittingdata_final, aes(x = AGE)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Age", x = "Age (years)", y = "Count")

# Scatterplot for Y by Age
ggplot(fittingdata_final, aes(x = AGE, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Age", x = "Age (years)", y = "Mavoglurant Concentration")
```
Our age values seem to have a relatively normal distribution with a minimum of 18 and maximum of 50. Our scatterplot shows that mavoglurant concentrations seem to remain the same on average between individuals of different ages. The plot shows no clear correlation one way or the other. Next, we'll investigate SEX.

```{r}
# Boxplot for mavoglurant concentration by sex
ggplot(fittingdata_final, aes(x = SEX, y = Y)) +
  geom_boxplot() +
  labs(title = "Concentration by Sex", x = "Sex", y = "Mavoglurant Concentration")
```
If we knew what our dataset's values meant or had clear documentation somewhere, we could interpret these results with certainty. Instead, all we can say is that if I'm right about 1 being male, they had higher concentrations on average than females. Given greatly unequal sample sizes and unclear documentation, we can't draw many conclusions from this.

```{r}
# Bar plot for Race
ggplot(fittingdata_final, aes(x = RACE)) +
  geom_bar() +
  labs(title = "Bar Plot of Race", x = "Race", y = "Count")

# Boxplot for mavoglurant concentration by race
ggplot(fittingdata_final, aes(x = RACE, y = Y)) +
  geom_boxplot() +
  labs(title = "Concentration by Race", x = "Race", y = "Mavoglurant Concentration")
```
We have no idea what this means since we don't have good documentation on the variables. Next, we'll look at our WT variable, which stands for weight (kg).

```{r}
# Histogram for Weight
ggplot(fittingdata_final, aes(x = WT)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Histogram of Weight", x = "Weight (kg)", y = "Count")

# Scatterplot for Y by Weight
ggplot(fittingdata_final, aes(x = WT, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Weight", x = "Weight(kg)", y = "Mavoglurant Concentration")
```
We can see that there are more observations of low-mid weight than high weight individuals from our histogram. We can see from our scatterplot that there isn't a strong correlation between weight and concentration, but it seems like higher weights have lower concentrations on average. Now we can explore HT, which is apparently our height variable. No units were given, so this will be difficult to interpret at best.

```{r}
# Histogram for Height
ggplot(fittingdata_final, aes(x = HT)) +
  geom_histogram(binwidth = 0.1) +
  labs(title = "Histogram of Height", x = "Height", y = "Count")

# Scatterplot for Y by Height
ggplot(fittingdata_final, aes(x = HT, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Height", x = "Height", y = "Mavoglurant Concentration")
```
We can't tell much from the histogram since we don't know what unit height is in, but the data seems relatively normally distributed. It is slightly skewed to the right, but not by much. The scatterplot doesn't show a strong or clear correlation, but on average, it looks like concentration decreased as height increased. Finally, we'll look at our dose variable, which only has values of 25, 37.5, and 50.

```{r}
# Bar plot for Dose
ggplot(fittingdata_final, aes(x = DOSE)) +
  geom_bar() +
  labs(title = "Bar Plot of Race", x = "Dose", y = "Count")

# Scatterplot for Y by Dose
ggplot(fittingdata_final, aes(x = DOSE, y = Y)) +
  geom_point() +
  geom_smooth(method = "loess") +
  labs(title = "Mavoglurant Concentration by Dose", x = "Dose", y = "Mavoglurant Concentration")
```
We see that there were far fewer 37.5 doses than the others, but according to the scatterplot, there's a clear trend of increased concentration as the dosage increases. This concludes our EDA; now, we can move into our model fitting.

We will now fit a linear model to Y using the main predictor of interest, DOSE. Then, we'll fit a linear model to Y using all predictors and compare their RMSE and R-squared values. We'll be using Microsoft Copilot in Precise mode for help with the base code again.

```{r}
# Load necessary libraries
library(tidymodels)

# Split the data into training and testing sets
fittingdata_split <- initial_split(fittingdata_final, prop = 0.75)
train_data <- training(fittingdata_split)
test_data <- testing(fittingdata_split)

# Fit a linear model to the continuous outcome "Y" using the main predictor of interest, DOSE
model1_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

model1_fit <- model1_spec %>% 
  fit(Y ~ DOSE, data = train_data)

# Fit a linear model to the continuous outcome "Y" using all predictors
model2_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

model2_fit <- model2_spec %>% 
  fit(Y ~ ., data = train_data)

# Compute RMSE and R-squared for model1
model1_metrics <- model1_fit %>% 
  predict(test_data) %>% 
  bind_cols(test_data) %>% 
  metrics(truth = Y, estimate = .pred)

cat("Model 1:\n")
cat("RMSE: ", model1_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), "\n")
cat("R-squared: ", model1_metrics %>% filter(.metric == "rsq") %>% pull(.estimate), "\n\n")

# Compute RMSE and R-squared for model2
model2_metrics <- model2_fit %>% 
  predict(test_data) %>% 
  bind_cols(test_data) %>% 
  metrics(truth = Y, estimate = .pred)

cat("Model 2:\n")
cat("RMSE: ", model2_metrics %>% filter(.metric == "rmse") %>% pull(.estimate), "\n")
cat("R-squared: ", model2_metrics %>% filter(.metric == "rsq") %>% pull(.estimate), "\n")

print(model1_fit)
print(model2_fit)
```
From our linear model that only uses DOSE as a predictor, we can see that DOSE is positively correlated with total mavoglurate concentration, which matches up with our EDA plot data. We can tell by looking at the coefficients produced by our models; positive coefficients indicate positive correlation whereas negative indicates negative.

Our second model shows that dose is positively correlated again. Furthermore, age and race2/88 are both positively correlated too, but the size of the coefficients indicates that age may be a weaker correlation. Sex 2, our females, are strongly negatively correlated with mavoglurate concentration. Race7 and height seem to be very strongly negatively correlated. Finally, weight is negatively correlated, but due to the coefficient size, this doesn't seem to be a strong relationship.

Our first model, which only uses DOSE as a predictor, seems to explain a bit more of the variation in the data. The R-squared value is slightly higher (by 0.003). However, the RMSE is also higher, which means that the error of Model 1 is slightly higher than that of model 2.  

Now, we'll look at how to do a logistic regression model on our data. We'll use SEX as the outcome since it's a categorical variable, even though this doesn't make sense from a science standpoint (it's just practice). We'll do the same thing: 1 model for just DOSE, and another for every predictor. Then we'll produce an ROC-AUC, which just measures performance for the classification problems at various threshold settings. We'll use Microsoft Copilot in Precise mode for the base code again.

```{r}
# Fit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, DOSE
model3_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

model3_fit <- model3_spec %>% 
  fit(SEX ~ DOSE, data = train_data)

# Fit a logistic model to SEX using all predictors
model4_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

model4_fit <- model4_spec %>% 
  fit(SEX ~ ., data = train_data)

# Compute ROC-AUC for "female" class for model3
model3_roc_auc_female <- model3_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_2)

# Compute ROC-AUC for "male" class for model3
model3_roc_auc_male <- model3_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_1)

cat("Model 3:\n")
cat("ROC-AUC for '2': ", model3_roc_auc_female$.estimate, "\n")
cat("ROC-AUC for '1': ", model3_roc_auc_male$.estimate, "\n\n")

# Compute ROC-AUC for "female" class for model4
model4_roc_auc_female <- model4_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_2)

# Compute ROC-AUC for "male" class for model4
model4_roc_auc_male <- model4_fit %>%
  predict(test_data, type = "prob") %>%
  bind_cols(test_data) %>%
  roc_auc(truth = SEX, .pred_1)

cat("Model 4:\n")
cat("ROC-AUC for '2': ", model4_roc_auc_female$.estimate, "\n")
cat("ROC-AUC for '1': ", model4_roc_auc_male$.estimate, "\n")

print(model3_fit)
print(model4_fit)

```
The coefficients for both models are very different than they were for our linear regression model that had Y as the outcome. We can see that DOSE appears to be negatively correlated with SEX, which in our case would indicate that higher doses mean more males. DOSE is again negatively correlated in our model using every variable as a predictor. Age, weight, Race88 and Race7 are all positively correlated, which means that as these increase we're more likely to see females. Y, Race2, and height are negatively correlated.

We can see that the ROC-AUC value for Model 3 (just dose as a predictor) shows similar performance of the model when predicting both male and female values. Remember that "1" is our males and "2" is our females. Model 4, on the other hand, shows a far stronger ability to accurately predict males than females. This makes sense given that we had so many more observations of males in our data.

This set of models isn't as useful in making any sort of inferences about our data, as the question we asked before creating our model doesn't make much sense. It's good practice regardless.